{"id":"71","paper":{"title":{"text":"Evaluating Boolean polynomials on spherical layers"},"authors":[{"name":"Dumer, Ilya"},{"name":"Kapralova, Olga"}],"abstr":{"text":"The goal of this talk is to extend the existing families of recursive coding constructions, such as Reed-Muller codes or polar codes.  We consider m-variate Boolean polynomials of degree r or less and evaluate them on the binary m-tuples of a given Hamming weight b.  From the coding perspective, this setting defines a punctured binary Reed-Muller code RM(r,m) whose positions form a Hamming sphere of weight b in the m-dimensional  binary space.  In this talk,  we specify some recursive properties of this single-layer spherical construction and define its code parameters for arbitrary values  of the input  parameters m,  r,  and b."},"body":{"text":"Shannon introduced mutual information and demonstrated how useful this quantity is to analyze a communication setup [1]. For instance the capasity of a simple information channel equals the maximum mutual information between input and output where the maximum is taken over all input distribu- tions. This result is remarkabel in that mutual information is symmetric in the two arguments but an information channel is highly unsymmetric in the sense that the sender has (partly or full) control over the input, but the sender has only indirect control over the distribution of output. Thus, the concept of mutual information does not capture that information ows from the input to the output.\nIn channels with feedback there is both a ow of informa- tion from input to output and a ow from output to input. The concept of directed information has been introduced to analyse channels with feedback, see [2], [3], [4], [5] and . Here we shall go one step further in order to understand what it means that information ows. This question is both of philosophical, theoretical and practical interest. There are many situations where we need more precise concepts in order to understand to what extend information is owing. We just mention quantum entanglement and secrecy sharing as concepts for which it is not so obvious whether these concepts represent quantities that can ow.\nIn this paper we shall be more modest and restrict our attention to Bayesian networks, because these networks are important in modelling causality. Thus, for Bayesian networks there is a natural direction of causality and time.\nBy a variable we shall understand any measurable mapping from a probability space into a nite space. The niteness condition is assumed for simplicity and most of our results can easily be extended to variables that can take in nitely many values. Variables will be denoted with capital letters. If X and Y are variables then X [ Y shall denote the vector valued variable (X; Y ) : If (X i ) is a ( nite or in nite)\nsequence of variables then X n m shall denote the sequence (X m ; X m+1 ; :::; X n ) where m n: Bold face capital letters will indicate in nite sequences.\nIf the variables X and Y are independent for each value z of the variable Z we say that X and Y are conditionally independent given Z and write (X?Y j Z) : The relation conditional independence is a semi graphoid relation, which means that\n(2) We introduce conditional mutual information I (X; Y j Z)\nand note that I (X; Y j Z) 0 with equality if and only if (X?Y j Z) : The relations (1) and (2) for conditional independence have counterparts for conditional independence:\nI (X; Y j Z) = I (Y ; X j Z) \t (3a) I (X; Y [ Z j W ) = I (X; Y j W ) + I (X; Z j Y [ W ) :\n(3b) Note that these equations implies the semi graphoid relations. We also get the important relation\nthat can be used to de ne conditional mutual information from mutual information. Now (3a) and (4) together implies (3b).\nSemi graphoid relations have proved important in under- standing causality [7], but they are dif cult to classify. In mod- elling causality one therefore prefer semi graphoid relations described by Bayesian networks or related graphical models. Here we shall focus on Bayesian networks. A Bayesian network is a directed acyclic graph with a variable associated to each node in the graph. If there is a directed path from the variable X to the variable Y in the network we shall write X \t Y and note that is a partial ordering of the nodes and the associated variables. The ancestor set a (Y ) is de ned as the set of variables X such that there exists a path to a variable in Y:\nWe shall restrict to syncronious networks consisting of a number of layers on top of each other such that any arrow in\nthe graph points from one layer to the next. For such networks the layers shall form a Markov chain where any variables in one layer is independent of all other variables in the same alyer given its parents. We shall write X Y if X is in a lower layer than Y; and X Y if X and Y are in the same layer. For each variable there is a Markov kernel from the parents of the variables to the variable and the joint distribution on all variables is determined by these Markov kernels. There exists a syncronization of a network if and only all directed paths from one variable to another are of equal length. By introduction of dummy variables it is easy to syncronize any network and therefore the restriction to syncronious networks gives no loss of generality.\nThe previous description of a Bayesian network is static in the sense that it relates a graph to some variables with a probabilistic structure but with no time evolution. We shall now introduce dynamic Bayesian networks. For each variable X in a static Bayesian network we associate an double in nite sequence X of variables. The joint distribution on all the sequences should be a stationary Markov chain with the Markov kernel being the product of the Markov kernels from the parents of a variable to the variable. The dynamic Bayesian network can be considered as a Bayesian network with in - nitely many nodes/variables. The static Baysian network can then be considered as the stationary distribution of the dynamic Bayesian network.\nFor a stationay sequence X we de ne the entropy rate in the usual way, i.e.\nSimilarly we de ne the conditional mutual information rate by\nFor sequences of variables X; Y and Z in a dynamic Bayesian network we have\nwhere X; Y and Z are the associated variables in the static Bayesian network. The relation can be used to translate con- cepts for sequences into concepts for static Bayesian networks. Note that in general\nLet X denote a sequence. Then we let T (X) denote the delayed sequence such that T (X) n = X n 1 :\nConsider two sequences of random variables X N and Y N . Then the directed information from X N to Y N is de ned by\n1 N\nIf X = (X n ) n 2Z and Y = (Y n ) n 2Z are stationary processes then the directed information rate from X to Y is de ned as the limit\nThe mutual information rate between two sequences of random variables X N and Y N is given by\n0 B B B B\n1 A\n1 C C C C A\nWe see that a mutual information rate is a sum of two directed information terms plus a term that we shall call the residual information. Thus\n~ I res X N ; Y N = 1 N\nThe residual information measures how much the sequences deviates from being a Bayesian network of the form depicted in Figure 1. For stationary sequences we de ne the residual information as the limit\nI (X; Y) = \t (5) ~ I (X ! Y) + ~ I (Y ! X) + ~ I res (X; Y) :\nExample 1: Let X and Y be sequences of variables in a dynamical Bayesian network corresponding to variables X and Y associated with single noden in the graph. Then\n~ I (X ! Y) if X Y ~ I (Y ! X) if Y X\nWith this de ntion of directed information we have that there may be an information ow from X to Y allthough :X Y: In this sense information may not ow along the direction of causation. We shall discuss this problem later and suggest a solution.\nBoth the de nition of directed information and residual in- formation depends on the syncronization of the sequences. We shall now see how the de nitions rely on the syncronization. We have\n= H Y 1 j Y 0 1 H Y 1 j X 1 1 [ Y 0 1 H Y 1 j Y 0 1 H Y 1 j X 0 1 [ Y 0 1\nis obvious. Thus ~ I (T n (X) ! Y) is a decreasing function of n and ~ I (X !T n (Y)) is an increasing function of n: Therefore the numbers for which both ~ I (T n (X) ! Y) and ~ I (X !T n (Y)) are positive is an interval. Only in this interval communication is possible in both directions and in practice this idea is often use to check syncronization. In the litterature directed information is de ned by\nsee [5] and [8]. With this de nition mutual information will not split up into two ows and a residual term. Instead one has to involve the operator T to get formulas relating mutual information and directed information.\nLet k be a positive integer and let X be a stationary sequence. Then kX shall denote \"block sequence\" where the n'th variable is block X (n 1)k+1 ; X (n 1)k+2 ; :::; X nk : We get\nConditional versions of directed information have been studied in [4], [5], [8] and [6]. Note that the de ntion we shall use is a little different from what is found in the litterature. We de ne the conditional directed information by\n(6) so that the information owing from X N to Y N given Z N is the information owing from both variables minus the amount of information owing Z N : Thus\n1 N\n= H Z 1 j Z 0 1 [ W 0 1 H Z 1 j X 0 1 [ Z 0 1 [ W 0 1 + H Z 1 j X 0 1 [ Z 0 1 [ W 0 1\nWe are now able to write a conditional mutual information in terms of conditional ows and residual information. We have\nI Z N ; Y N = I (X [ Z; Y) I (Z; Y) :\nEach of these terms can be written as a sum of two ow and residual information. Thus,\nThe formula is closely related to [8, Prop. 3], but by introduing residual information we have formulas that do not involve the delay operator. It seems tempting to de ne ~ I res (X [ Z; Y)\n~ I res (Z; Y) as the residual mutual information of X and Y given Z; but it may lead to a negative quantity.\nThe basic equations for conditional mutual information can now be stated in terms of directed and residual information. Equation 3a states that\n~ I (X ! YkZ) + ~ I (Y ! X [ Z) ~ I (Y ! Z) + ~ I res (X [ Z; Y) ~ I res (Z; Y)\n= ~ I (Y ! XkZ) + ~ I (X ! Y [ Z) ~ I (X ! Z) + ~ I res (Y [ Z; X) ~ I res (Z; X)\n~ I (X ! Y [ Z) ~ I (X ! YkZ) ~ I (X ! Z) \t (7) + ~ I res (X; Y [ Z) ~ I res (Z; X)\n= ~ I (Y ! X [ Z) ~ I (Y ! XkZ) ~ I (Y ! Z) + ~ I res (Y; X [ Z) ~ I res (Z; Y) :\nWe see that relation 7 together with Equation 5 and the de nitions 6 and 4 implies the identities 3a and 3b.\nAs we have noticed that directed information is relative to the syncronization of the sequences. Nevertheless it is sometimes possible to provide lower bound to the directed information, which are independent of the syncronization. Here we shall just provide a simple example that will illustrate the idea.\nConsider (sets of) variables X; Y; Z and W in a Bayesian network satisfying that statistical independence holds if and only the d-separation criteria is ful lled. Assume that X and Y are independent given Z [ W: Then we have the lower bound\nTo se this we put W 0 = W \\ a (X [ Y [ Z) and note X and Y are independent of Z [ W 0 [9]. Now,\n+ ~ I {W ! X [ Y [ Z j W 0 ~ I (W 0 ! X [ Y [ Z) I (W ; X [ Y [ Z)\nI (W ; X [ Y j Z) I (X; Y j Z) :\nThe last inequality is easily proved by using the Venn diagram method [10].\nUnder weak conditions this lower bound holds even when X and Y are not independent given Z [ W: More re ned lower bounds are an area for future investigations."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"H. Marko"}],"title":{"text":"The bidirectional communication theory - a generalization of information theory"}},{"authors":[{"name":"J. L. Massey"}],"title":{"text":"Causality, feedback and directed information"}},{"authors":[{"name":"G. Krame"}],"title":{"text":"Directed Information for Channels with Feedback, vol"}},{"authors":[{"name":"G. Kramer"}],"title":{"text":"Capacity results for the discrete memoryless network"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. S. Pradhan"}],"title":{"text":"Directed information for communication problems with common side information and delayed feedback/feedforward"}},{"authors":[{"name":"J. Pear"}],"title":{"text":"Probabilistic Reasoning in Intelligent Systems"}},{"authors":[{"name":"J. L. Massey"},{"name":"P. C. Massey"}],"title":{"text":"Conservation of mutual and directed information"}},{"authors":[{"name":"P. HarremoÃ«"}],"title":{"text":"Time and Conditional Independence, vol"}},{"authors":[{"name":"R. W. Yeun"}],"title":{"text":"A First Course in Information Theory"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/ita2013/71"},"links":[{"id":"205","weight":9},{"id":"2760","weight":21},{"id":"2741","weight":21},{"id":"3180","weight":3},{"id":"67","weight":29},{"id":"2206","weight":5},{"id":"1606","weight":9},{"id":"153","weight":4},{"id":"3037","weight":6},{"id":"436","weight":14},{"id":"3191","weight":6},{"id":"12","weight":5},{"id":"223","weight":2},{"id":"2254","weight":4},{"id":"1278","weight":4},{"id":"2287","weight":6},{"id":"3200","weight":11},{"id":"2217","weight":5},{"id":"3222","weight":8},{"id":"981","weight":12},{"id":"3021","weight":12},{"id":"170","weight":6},{"id":"3199","weight":3},{"id":"813","weight":10},{"id":"3069","weight":7},{"id":"2634","weight":4},{"id":"2442","weight":2},{"id":"3183","weight":15},{"id":"2763","weight":10},{"id":"135","weight":2},{"id":"404","weight":24},{"id":"702","weight":2},{"id":"450","weight":4},{"id":"770","weight":2},{"id":"3182","weight":5},{"id":"1889","weight":4},{"id":"762","weight":3},{"id":"3154","weight":2},{"id":"2002","weight":2},{"id":"3065","weight":14},{"id":"3195","weight":5},{"id":"1060","weight":4},{"id":"3132","weight":6},{"id":"1160","weight":3},{"id":"3187","weight":31},{"id":"944","weight":5},{"id":"3059","weight":5},{"id":"2646","weight":2},{"id":"104","weight":4},{"id":"90","weight":3},{"id":"33","weight":3},{"id":"273","weight":17},{"id":"1442","weight":7},{"id":"2352","weight":5},{"id":"470","weight":8},{"id":"1643","weight":10},{"id":"250","weight":3},{"id":"163","weight":2},{"id":"26","weight":5},{"id":"2455","weight":2},{"id":"1298","weight":22},{"id":"1268","weight":2},{"id":"2830","weight":4},{"id":"679","weight":10},{"id":"123","weight":8},{"id":"3049","weight":6},{"id":"1223","weight":5},{"id":"107","weight":8},{"id":"217","weight":3},{"id":"3038","weight":2},{"id":"1996","weight":33},{"id":"973","weight":4},{"id":"1279","weight":2},{"id":"2119","weight":5},{"id":"888","weight":4},{"id":"459","weight":2},{"id":"155","weight":2},{"id":"1877","weight":4},{"id":"1559","weight":3},{"id":"83","weight":3},{"id":"3186","weight":6},{"id":"3072","weight":3},{"id":"306","weight":9},{"id":"1121","weight":3},{"id":"3068","weight":4},{"id":"3164","weight":2},{"id":"398","weight":3},{"id":"152","weight":4},{"id":"2750","weight":5},{"id":"1421","weight":10},{"id":"2178","weight":4},{"id":"1866","weight":8},{"id":"2488","weight":2},{"id":"900","weight":3},{"id":"59","weight":4},{"id":"353","weight":7},{"id":"1846","weight":12},{"id":"1074","weight":8},{"id":"3168","weight":4},{"id":"263","weight":5},{"id":"3157","weight":5},{"id":"2317","weight":22},{"id":"375","weight":2},{"id":"3127","weight":3},{"id":"572","weight":6},{"id":"419","weight":3},{"id":"372","weight":6},{"id":"138","weight":21},{"id":"3073","weight":3},{"id":"54","weight":8},{"id":"628","weight":2},{"id":"408","weight":3},{"id":"3005","weight":12},{"id":"1696","weight":13},{"id":"3016","weight":4},{"id":"1212","weight":4},{"id":"2324","weight":4},{"id":"487","weight":2},{"id":"3027","weight":6},{"id":"1096","weight":2},{"id":"3196","weight":2},{"id":"2033","weight":5},{"id":"1830","weight":2},{"id":"108","weight":7},{"id":"691","weight":12},{"id":"2740","weight":5},{"id":"2812","weight":3},{"id":"3243","weight":2},{"id":"2188","weight":2},{"id":"445","weight":2},{"id":"3185","weight":2},{"id":"1671","weight":4},{"id":"1844","weight":15},{"id":"219","weight":3},{"id":"2174","weight":4},{"id":"2759","weight":2},{"id":"2319","weight":13},{"id":"676","weight":2},{"id":"82","weight":4},{"id":"1448","weight":7},{"id":"3056","weight":10},{"id":"1","weight":5},{"id":"2996","weight":12},{"id":"2617","weight":2},{"id":"2435","weight":25},{"id":"3051","weight":5},{"id":"1499","weight":10},{"id":"1840","weight":10},{"id":"1988","weight":4},{"id":"1908","weight":3},{"id":"771","weight":8},{"id":"221","weight":5},{"id":"25","weight":3},{"id":"658","weight":4},{"id":"2743","weight":3},{"id":"687","weight":4},{"id":"230","weight":6},{"id":"3123","weight":5},{"id":"2773","weight":4},{"id":"31","weight":14},{"id":"3071","weight":9},{"id":"1439","weight":4},{"id":"278","weight":13},{"id":"3113","weight":4},{"id":"1444","weight":7},{"id":"151","weight":9},{"id":"887","weight":7},{"id":"95","weight":2},{"id":"1109","weight":17},{"id":"264","weight":3},{"id":"257","weight":3},{"id":"3030","weight":6},{"id":"694","weight":5},{"id":"1573","weight":12},{"id":"203","weight":4},{"id":"525","weight":3},{"id":"1847","weight":4},{"id":"3205","weight":15},{"id":"1509","weight":10},{"id":"1836","weight":4},{"id":"3167","weight":4},{"id":"115","weight":3},{"id":"1822","weight":4},{"id":"275","weight":5},{"id":"376","weight":2},{"id":"3197","weight":5},{"id":"293","weight":6},{"id":"630","weight":6},{"id":"767","weight":5},{"id":"326","weight":6},{"id":"2939","weight":8},{"id":"1915","weight":8},{"id":"1233","weight":4},{"id":"641","weight":4},{"id":"1905","weight":8},{"id":"3189","weight":3},{"id":"831","weight":8},{"id":"2862","weight":10},{"id":"402","weight":5},{"id":"371","weight":3},{"id":"2443","weight":2},{"id":"1084","weight":5},{"id":"2729","weight":5},{"id":"3156","weight":2},{"id":"1883","weight":2},{"id":"3074","weight":5},{"id":"1055","weight":6},{"id":"1102","weight":9},{"id":"2173","weight":4},{"id":"1517","weight":11},{"id":"198","weight":2},{"id":"506","weight":2},{"id":"187","weight":16},{"id":"165","weight":9},{"id":"1627","weight":2},{"id":"92","weight":3},{"id":"621","weight":13},{"id":"3070","weight":11},{"id":"1503","weight":2},{"id":"3181","weight":2},{"id":"1546","weight":8},{"id":"3198","weight":2},{"id":"2778","weight":6},{"id":"1098","weight":2},{"id":"183","weight":13},{"id":"688","weight":3},{"id":"3169","weight":6},{"id":"982","weight":8},{"id":"359","weight":2},{"id":"1930","weight":7},{"id":"3173","weight":5},{"id":"288","weight":2},{"id":"1553","weight":5},{"id":"3184","weight":4},{"id":"3044","weight":2},{"id":"3170","weight":5},{"id":"485","weight":12},{"id":"3193","weight":2},{"id":"2738","weight":3},{"id":"240","weight":10},{"id":"1166","weight":8},{"id":"1550","weight":4},{"id":"782","weight":3},{"id":"3125","weight":2},{"id":"2495","weight":16},{"id":"420","weight":2},{"id":"3136","weight":3},{"id":"74","weight":4},{"id":"1456","weight":3},{"id":"243","weight":3},{"id":"1214","weight":3},{"id":"1083","weight":8},{"id":"3114","weight":6},{"id":"1434","weight":4},{"id":"1871","weight":3},{"id":"3188","weight":4},{"id":"3177","weight":6},{"id":"3166","weight":3},{"id":"1187","weight":11},{"id":"449","weight":3},{"id":"377","weight":4},{"id":"424","weight":3},{"id":"3029","weight":5},{"id":"3151","weight":10},{"id":"2146","weight":5},{"id":"2638","weight":4},{"id":"2315","weight":4},{"id":"1423","weight":2},{"id":"3208","weight":8},{"id":"370","weight":2},{"id":"1402","weight":5},{"id":"2299","weight":7},{"id":"2309","weight":5},{"id":"1258","weight":2},{"id":"2288","weight":14},{"id":"2340","weight":3},{"id":"2286","weight":8},{"id":"2038","weight":8},{"id":"752","weight":4},{"id":"1393","weight":2},{"id":"1714","weight":4},{"id":"3162","weight":6}],"meta":{"jsonClass":"Map$Map3","room":"The Tookan","date":"1360512900000","session":"3"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
