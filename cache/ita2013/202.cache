{"id":"202","paper":{"title":{"text":"Model construction for human motion classification using inertial sensors "},"authors":[{"name":"Chieh Chien"},{"name":"James Xu"},{"name":"Hua-I Chang"},{"name":"Xiaoxu Wu"},{"name":"Greg Pottie"}],"abstr":{"text":"Many commercial products have appeared that claim to classify human motions using a single inertial sensor. While certain motions can sometimes be accurately classified in this manner, for many medically significant activities, performance is completely inadequate. In particular, there are challenges in customizing to the activities of interest for particular individuals, dealing with motions of the upper limbs, assessing quality of motions, and coping with members of the general public not following usage instructions precisely. We present a general model that takes these factors into account, and present our progress in building tracking systems and classifiers that can scale to large populations."},"body":{"text":"The theory of quantization derives largely from Lloyd\u2019s work [9], which formalized the optimal performance and found asymptotic approximations to the optimal performance for high-rate scalar quantization. The ideas were initially extended to vector quantizers by Zador [10]. Zador considered two separate cases of code constraints: total number of codewords (ﬁxed-rate codes) and quantizer output entropy (variable-rate or entropy-constrained codes). The ﬁxed-rate results of Zador were generalized and simpliﬁed by Bucklew and Wise [1] and Graf and Luschgy [5]. The entropy-constrained results of Zador were generalized in [6] using the Lagrangian formulation of [3]. Extensions to combined con- straints on entropy and codebook size are presented without proof, but with heuristic arguments based on Gersho\u2019s conjecture. Details are contained in a paper in progress. See also [7], [8].\nA quantizer or vector quantizer q on k can be described by the following mappings and sets: an encoder α : k → I, where I = {0, 1, 2, . . .} is an index set, an associated partition S = {S i : i ∈ I}, S i ⊂ k , such that α(x) = i if x ∈ S i , a decoder β : I → k , an associated reproduction codebook C = {β(i) : i ∈ I} of size N (q) = |C|, and a length function { (i) : i ∈ I} which is admissible in the sense that i∈I e − (i) ≤ 1. Let q denote both the collection of mappings and the overall mapping q(x) = β(α(x)).\nThe distortion d(x, ˆ x) between an input x and a quantized version ˆ x = β(α(x)) is assumed here to be the squared error distortion ||x − ˆ x|| 2 . If X is a random vector with density f (assumed to be ab- solutely continuous with respect to Lebesgue mea- sure), the average distortion is deﬁned as D f (q) = E f d(X, β(α(X))), where E f denotes expectation with respect to f . The average rate is deﬁned by R f (q) = (1 − η)E f (α(X)) + η ln N (q). Deﬁne the average Lagrangian distortion\n+ λ[(1 − η) (α(X)) + η ln N (q)] , (1) where λ > 0 and η ∈ [0, 1]. The corresponding optimization is to characterize\nIn this context the high-rate results for the traditional cases of variable-rate coding (η = 0) and ﬁxed-rate coding (η = 1) can be considered as special cases of the limiting behavior as λ → 0 of\nThe Lagrangian form of the variable-rate result [6] is that under suitable conditions\nwhere h(f ) is the differential entropy of the pdf f and θ k ∆ = inf λ>0 θ(u, λ, 0), and u is the uniform pdf on a unit cube. The Lagrangian form of the ﬁxed- rate result [8] is\nwhere ψ k ∆ = inf λ>0 θ(u, λ, 1). The proofs of these results tend to be tedious, but they largely follow Zador\u2019s original approach:\nStep 1 Prove the result for a uniform density on a cube.\nStep 2 Prove the result for a pdf that is piecewise constant on disjoint cubes of equal volume.\nStep 3 Prove the result for a general pdf on a cube by approximating it by a piecewise constant pdf on small cubes.\nStep 4 Extend the result for a general pdf on the cube to general pdfs on k by limiting arguments.\nThe uniform density u on [0, 1) k plays a funda- mental role both as Step 1 and as a simple example sufﬁcient for developing the properties of the Zador constants related to θ k and ψ k .\nThe form of the Lagrangian approach suggests a conjecture for the asymptotic behavior consistent with the high-rate Lagrangian ﬁxed- and variable- rate cases:\nwhere θ(f, η) is ﬁnite. Furthermore, the particular form of the traditional cases suggests a second conjecture\u2014that the asymptotically optimal perfor- mance θ(f, η) can be expressed as a sum of two terms, the ﬁrst involving an inﬁmum for the uniform density on a unit cube and the second depending on the speciﬁc pdf:\n2 ln λ . (8) From the traditional variable-rate and ﬁxed-rate cases, θ k (0) = θ k and h(f, 0) = h(f ), whereas θ k (1) = ψ 1 and h(f, 1) = ln ||f || k/2 k/(k+2) .\nFor λ > 0 and η ∈ (0, 1), the Lloyd optimality properties become\n\u2022 For a given decoder β and length func- tion , the optimal encoder satisﬁes α(x) = argmin i (d(x, β(i)) + λη (i)) .\n\u2022 For a given encoder α, the optimal decoder satisﬁes β(i) = argmin y E(d(X, y|α(X) = i) if the minimum exists.\n\u2022 For a given encoder α, the optimal code- length is (i) = − ln Pr(α(X) = i). Therefore E f (α(X)) = H f (q), the Shannon entropy of the quantizer output.\n\u2022 A necessary condition for optimality of a ﬁxed- rate quantizer q with codebook C is that there be no subcodebook C ⊂ C for which\nwhere H f (C) and H f (C ) denote the entropies of the partitions corresponding to the optimal encoding for codebook C and C .\nGiven a codebook C (or decoder β) or partition S (or encoder α), the Lloyd properties determine the remaining components, so optimizing over quantiz- ers is equivalent to optimizing over codebooks or partitions.\nIt is convenient to consider the quantity deﬁned by normalizing the average Lagrangian distortion by λ and adding a weighted ln λ term. Deﬁne\nand the related quantities θ(f, λ, η) \t = inf S θ(f, λ, η, S), θ(f, η) = lim sup λ→0 θ(f, λ, η), and θ(f, η) = lim inf λ→0 θ(f, λ, η). The main conjecture (6) is true if and only if θ(f, η) = θ(f, η), in which case the common value is denoted θ(f, η).\nLemma 1: θ(f, λ, η, S) and θ(f, λ, η) are mono- tonic nondecreasing, concave, and continuous func- tions of η.\nCorollary 1: Suppose that for a pdf f the con- jecture (6) holds for all η ∈ [0, 1] and hence\nexists for η ∈ [0, 1]. Then θ(f, η) is a monotone nondecreasing concave function of η and it is con- tinuous except possibly at the origin. Furthermore, θ (f, η) = dθ(f, η)/dη exists and is ﬁnite for all η ∈ (0, 1) except possibly on a set of Lebesgue measure 0.\nIf equation (6) holds for f and η, then for every sequence λ n → 0 there exists a sequence of partitions S n for which\nθ(f, λ n , η, S n ) = θ(f, η), \t (10) in which case we say that the sequence S n is (η, λ n )-asymptotically optimal or simply (η, λ n )- a.o. for the pdf f .\nIf S n is (η, λ n )-a.o. for a pdf f , then (12) can be used as in the ﬁxed-rate and variable-rate cases to show that\n= θ(f, η). (11) A sequence of partitions S n satisfying (11) will be called η-asymptotically optimal or brieﬂy η-a.o.\nfor the pdf f . If a sequence is (η, λ n )-a.o. for any sequence λ n , then it is eta-a.o. Conversely if it is η-a.o., then it is (η, λ n )-a.o. for some λ n → 0.\nIn the traditional cases the asymptotic behavior of distortion and rate can be teased apart from the linear combination of the two in (10) [7], [8]. These results can be extended to the combined constraint case using the following inequality based on the ln r ≤ r − 1 inequality:\nθ(f, λ, η, S) ≥ k 2\nln 2e k\nwith equality if and only if λ = 2D f (S)/k. If S n is η-a.o. for f , then\nIf, in addition, we make the assumption of Corollary 1 that (6) holds for all η ∈ [0, 1], then we can also separate out the behavior of H f (S n ) and ln |S n )| in terms of λ n :\nPerhaps surprisingly, the two growing terms ln |S n | and H f (S n )) differ only by a constant in the limit if θ (f, η) is ﬁnite! Combining the previous results yields the following corollary which separates out the asymptotic behavior of distortion, entropy, code- book size, and the difference between the entropy and codebook size.\nLemma 2: Suppose that for a pdf f the conjecture (6) holds for all η ∈ [0, 1], hence θ(f, η) = lim λ→0 θ(f, λ, η) exists for η ∈ [0, 1]. Then for almost all η ∈ (0, 1), if S n is (η, λ n )-a.o. then\nln |S n | + k 2\nGersho\u2019s conjecture and the associated approx- imations can be used to derive the basic results for joint entropy and codebook size constrained quantization. This approach, although not rigorous, provides insight into the results and a consistency check with the rigorous development. An obvious modiﬁcation suggests a solution to the general case.\nGersho\u2019s conjecture involves two assumptions re- garding asymptotically optimal sequences of ﬁxed- rate and variable-rate quantizers. First, it is assumed that there exists a quantizer point density function Λ(x) such that a sequence of optimal codes with N codewords, N = 1, 2, . . . will satisfy\n1 N\nwhere k Λ(x) dx = 1. Second, Gersho assumed that If f X (x) is smooth and R is large, then the minimum distortion quantizer has cells S i that are (approximately) scaled, rotated, and translated copies of S ∗ , the convex polytope that tesselates R k with minimum normalized moments of inertia\nwhere y(S) denotes the centroid of S. Speciﬁcally, deﬁne\nUnder these assumptions, it can be argued using Riemann approximations of integrals and sums that for large N\n= ln N (q) − H(f ||Λ), \t (18) where the relative entropy H(f ||λ) \t =\nf (x) ln(f (x)/Λ(x)) dx. Application of Holder\u2019s inequality to (17) yields the classic ﬁxed-rate result and the combination of (17) and (18) with Jensen\u2019s inequality yields the classic variable-rate result. Suppose that a quantizer q has a quantizer point density Λ and a total of N quantization levels for N large, then\n(1 − η)[ln N − H(f ||Λ)] + η ln N + k 2\nIf the quantizer point density Λ is ﬁxed, then the optimum choice of N is the that which minimizes\nc k E f (Λ(X)) −2/k λ\n(20) and the goal becomes the minimization of θ(f, λ, η, Λ) over all point density functions Λ. Ger- sho\u2019s conjecture and this heuristic approach imply that (1/2) ln(kec k /2) = θ k = ψ k , the Zador ﬁxed- rate and variable-rate constants are equal, but no proof of this result exists except for k = 1 and asymptotically as k → ∞. For our purposes we can identify (k/2) ln(kec k /2) as θ k (η) when comparing (20) with the rigorous results, which results in the conjecture\nConjecture (6) has been proved for uniform den- sities on cubes [8].\nThus η-a.o. quantizers exist for such densities and the previous results Lemma 2 concerning the behav- ior θ(f, η) apply to θ(u, η) = θ k (η). For example, from the remarks following conjecture 3 of [7], the lemma implies that for almost all η, the asymp- totic quantizer point density function exists and is uniform for the uniform distribution, extending the known result for ﬁxed-rate coding to the combined case.\nStep 2 assumes that the pdf f is nonzero on the union of a ﬁnite number M of disjoint cubes {C(m) : m = 1, 2, . . . , M } on which it is constant:\n(24) where m w m = 1 and and a k is the volume of each C(m). Design for each cube C(m) a nearly optimal code with partition S m for the conditionally uniform pdf f m using a Lagrange multiplier λ m and a common value of η for all m. For the moment we leave open the choice of the λ m except for the assumption that they are all small enough for Lemma 2 to apply. After some algebra, the overall Lagrangian distortion becomes\nθ(f, λ, η, S) ≈ θ k (η)− k 2 +(1−η)H(w)+k ln a+ k\nSo the goal is to minimize the function θ(f, η, µ) = θ k (η) + φ(w, η, {λ m }) over {λ m }. Unfortunately, φ is not a convex function of the Lagrangian mul- tipliers. However, transforming variables as ν m = ln(λ m /λ) results in a convex optimization problem\nF 1 (w, ν) = φ(w, 1, ν) = k\nSince φ(w, η, ν) is strictly convex in ν, there must be a unique minimum. Since φ(w, η, ν) is differen- tiable with respect to ν, the minimum must be at a point with 0 gradient, which implies\nStrict convexity of φ(w, η, ν) guarantees the exis- tence of a ν satisfying this equation and furthermore that ν minimizes φ(w, η, ν). Unfortunately there seems to be no nice closed form solution for ν in terms of w.\nCombining the above arguments with the careful limiting arguments proves the following.\nTheorem 2: If f is a piecewise constant pdf of the form given in (24), then\nThis proves the positive part of (6), but to prove the conjecture requires a converse to the effect that\nUnfortunately the converse has proved more dif- ﬁcult than in either of the traditional cases, but we conjecture that it holds based on the fact that a development based on Gersho\u2019s conjecture and approximations is consistent with our conjecture.\nThe next step is to generalize from piecewise constant pdfs on a cube to more general pdfs on the unit cube. The arguments for the piecewise continuous case extend to this case and also to unbounded support sets with a moment condition and show that\nTheorem 3: If f is a pdf satisfying the moment condition of E f ( X 2+δ ) ≤ ∞ for some δ > 0, then the result of Theorem 2 holds with\nφ(f, η, ν) = (1 − η)F 0 (f, ν) + ηF 1 (f, ν) where\nF 0 (f, ν) = φ(f, 0, ν) = k\nF 1 (f, ν) = φ(f, 1, ν) = k\nφ(f, η, ν) is a strictly convex function of ν. Un- fortunately, in this inﬁnite dimensional case convex- ity does not guarantee the existence of a minimizing ν and hence further assumptions are needed. It does, however, guarantee that if a minimizing ν exists, it is unique (at least up to a set of measure zero). In particular, if there is a local minimum of φ(f, η, ν) with respect to ν, then it is the unique global minimum. By adding more assumptions (in particular, that f is such that φ(f, η, ν) is twice continuously differentiable), a calculus of variations argument results in the conditions\nwhich is the continuous analog of the piecewise con- stant result. Note that as in the piecewise constant case, the minimizing ν must satisfy\nTransforming the variables to Λ using (31) yields the identical optimization to (22). This suggests that the Λ arising in the Lagrangian optimization is in fact Gersho\u2019s quantizer point density function.\nThe author gratefully acknowledges discus- sions with Stephen Boyd, Tamas Linder, John Gill,Michelle Effros and Deirdre O\u2019Brien. This work was partially supported by the National Science Foundation under NSF Grant CCR-0073050 and by gifts from the Hewlett-Packard coorporation."},"refs":[{"authors":[{"name":"J. A. Bucklew"},{"name":"G. L. Wise"}],"title":{"text":"Multidimensional asymptotic quantization theory with rth power distor- tion measures"}},{"authors":[{"name":"J. A. Bucklew"}],"title":{"text":"Two results on the asymptotic per- formance of quantizers"}},{"authors":[{"name":"P. A. Cho"},{"name":"T. Lookabaug"},{"name":"R. M. Gray"}],"title":{"text":"Entropy- constrained vector quantization"}},{"authors":[{"name":"A. Gersho"}],"title":{"text":"Asymptotically optimal block quantiza- tion"}},{"authors":[{"name":"S. Gra"},{"name":"H. Luschg"}],"title":{"text":"Foundations of Quantization for Probability Distributions, Springer, Lecture Notes in Mathematics, 1730, Berlin, 2000"}},{"authors":[{"name":"R. M. Gray"},{"name":"T. Linder"},{"name":"J. Li"}],"title":{"text":"A Lagrangian for- mulation of Zador\u2019s entropy-constrained quantization theorem"}},{"authors":[{"name":"R. M. Gray"},{"name":"T. Linder"}],"title":{"text":"Results and Conjectures on High Rate Quantization"}},{"authors":[{"name":"R. M. Gray"},{"name":"J. T. Gill"}],"title":{"text":"A Lagrangian formulation of ﬁxed rate and entropy/memory constrained quantization"}},{"authors":[{"name":"S. P. Lloyd"},{"name":"M. IEEE Transactions on Information Theor"}],"title":{"text":"Least squares quantization in PC 1957"}},{"authors":[{"name":"P. L. Zador"}],"title":{"text":"Topics in the asymptotic quantization of continuous random variables"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/ita2013/202"},"links":[{"id":"205","weight":3},{"id":"2760","weight":6},{"id":"1486","weight":4},{"id":"2741","weight":3},{"id":"67","weight":2},{"id":"2206","weight":4},{"id":"1606","weight":4},{"id":"153","weight":5},{"id":"3037","weight":2},{"id":"436","weight":2},{"id":"3191","weight":2},{"id":"12","weight":3},{"id":"223","weight":3},{"id":"1547","weight":9},{"id":"1278","weight":2},{"id":"2287","weight":5},{"id":"3200","weight":8},{"id":"3008","weight":5},{"id":"3083","weight":6},{"id":"2217","weight":5},{"id":"3222","weight":2},{"id":"1430","weight":5},{"id":"3021","weight":2},{"id":"170","weight":7},{"id":"3199","weight":2},{"id":"813","weight":5},{"id":"3069","weight":2},{"id":"2634","weight":3},{"id":"526","weight":3},{"id":"2442","weight":6},{"id":"3183","weight":4},{"id":"362","weight":5},{"id":"2763","weight":2},{"id":"2534","weight":5},{"id":"1235","weight":6},{"id":"702","weight":4},{"id":"450","weight":4},{"id":"770","weight":2},{"id":"3165","weight":2},{"id":"1082","weight":4},{"id":"3182","weight":2},{"id":"762","weight":7},{"id":"3154","weight":5},{"id":"3065","weight":5},{"id":"3195","weight":18},{"id":"284","weight":5},{"id":"3218","weight":4},{"id":"1060","weight":2},{"id":"3132","weight":3},{"id":"3137","weight":3},{"id":"3059","weight":6},{"id":"90","weight":9},{"id":"236","weight":4},{"id":"33","weight":8},{"id":"117","weight":3},{"id":"273","weight":3},{"id":"3111","weight":7},{"id":"1442","weight":2},{"id":"2276","weight":2},{"id":"2352","weight":15},{"id":"3176","weight":7},{"id":"3255","weight":2},{"id":"1643","weight":3},{"id":"250","weight":4},{"id":"3036","weight":7},{"id":"163","weight":3},{"id":"2455","weight":6},{"id":"3175","weight":21},{"id":"3201","weight":2},{"id":"2830","weight":3},{"id":"679","weight":4},{"id":"123","weight":5},{"id":"3049","weight":5},{"id":"1479","weight":2},{"id":"1215","weight":4},{"id":"107","weight":3},{"id":"1473","weight":5},{"id":"217","weight":3},{"id":"1462","weight":6},{"id":"13","weight":5},{"id":"3038","weight":4},{"id":"1996","weight":8},{"id":"325","weight":2},{"id":"2025","weight":10},{"id":"973","weight":5},{"id":"1279","weight":2},{"id":"459","weight":2},{"id":"1877","weight":5},{"id":"1559","weight":2},{"id":"83","weight":2},{"id":"3186","weight":3},{"id":"3072","weight":6},{"id":"306","weight":3},{"id":"1121","weight":2},{"id":"3164","weight":6},{"id":"1107","weight":4},{"id":"2697","weight":2},{"id":"398","weight":2},{"id":"152","weight":11},{"id":"2750","weight":5},{"id":"1421","weight":9},{"id":"2178","weight":3},{"id":"72","weight":2},{"id":"1866","weight":4},{"id":"3233","weight":4},{"id":"2488","weight":6},{"id":"900","weight":4},{"id":"59","weight":5},{"id":"353","weight":5},{"id":"1846","weight":2},{"id":"1074","weight":3},{"id":"1081","weight":20},{"id":"3168","weight":3},{"id":"1136","weight":2},{"id":"2751","weight":5},{"id":"1103","weight":8},{"id":"375","weight":5},{"id":"3127","weight":6},{"id":"572","weight":4},{"id":"372","weight":2},{"id":"138","weight":2},{"id":"3073","weight":5},{"id":"54","weight":3},{"id":"1660","weight":7},{"id":"628","weight":5},{"id":"408","weight":4},{"id":"3005","weight":2},{"id":"1696","weight":5},{"id":"3016","weight":3},{"id":"1212","weight":4},{"id":"508","weight":4},{"id":"65","weight":4},{"id":"1096","weight":2},{"id":"642","weight":12},{"id":"3196","weight":5},{"id":"2033","weight":4},{"id":"1830","weight":6},{"id":"108","weight":4},{"id":"691","weight":4},{"id":"2740","weight":4},{"id":"1443","weight":8},{"id":"2812","weight":3},{"id":"3243","weight":4},{"id":"2188","weight":2},{"id":"3134","weight":3},{"id":"1365","weight":2},{"id":"445","weight":3},{"id":"1671","weight":4},{"id":"1844","weight":5},{"id":"219","weight":3},{"id":"428","weight":2},{"id":"2174","weight":3},{"id":"2759","weight":4},{"id":"1128","weight":3},{"id":"431","weight":4},{"id":"676","weight":7},{"id":"82","weight":4},{"id":"1376","weight":3},{"id":"1448","weight":2},{"id":"3056","weight":3},{"id":"1","weight":3},{"id":"2435","weight":3},{"id":"3174","weight":5},{"id":"1840","weight":5},{"id":"1988","weight":5},{"id":"2884","weight":14},{"id":"771","weight":3},{"id":"3088","weight":5},{"id":"25","weight":3},{"id":"658","weight":2},{"id":"2743","weight":3},{"id":"687","weight":4},{"id":"230","weight":5},{"id":"3123","weight":4},{"id":"309","weight":4},{"id":"2177","weight":3},{"id":"2773","weight":4},{"id":"31","weight":4},{"id":"3071","weight":2},{"id":"503","weight":2},{"id":"1439","weight":2},{"id":"278","weight":3},{"id":"764","weight":2},{"id":"3113","weight":2},{"id":"1444","weight":3},{"id":"887","weight":3},{"id":"95","weight":4},{"id":"1109","weight":3},{"id":"1438","weight":3},{"id":"184","weight":2},{"id":"264","weight":2},{"id":"257","weight":3},{"id":"3030","weight":4},{"id":"694","weight":2},{"id":"1847","weight":3},{"id":"3205","weight":4},{"id":"75","weight":2},{"id":"1509","weight":3},{"id":"246","weight":6},{"id":"3167","weight":5},{"id":"115","weight":2},{"id":"1822","weight":2},{"id":"275","weight":3},{"id":"308","weight":4},{"id":"2838","weight":4},{"id":"3197","weight":3},{"id":"630","weight":3},{"id":"767","weight":3},{"id":"354","weight":5},{"id":"326","weight":5},{"id":"2939","weight":7},{"id":"1076","weight":5},{"id":"430","weight":3},{"id":"1915","weight":4},{"id":"1233","weight":4},{"id":"641","weight":7},{"id":"3189","weight":6},{"id":"575","weight":5},{"id":"831","weight":6},{"id":"2862","weight":7},{"id":"371","weight":4},{"id":"2443","weight":6},{"id":"1084","weight":7},{"id":"2729","weight":2},{"id":"423","weight":2},{"id":"3156","weight":2},{"id":"1883","weight":6},{"id":"1055","weight":2},{"id":"1102","weight":2},{"id":"2173","weight":3},{"id":"2753","weight":4},{"id":"1517","weight":6},{"id":"699","weight":3},{"id":"506","weight":7},{"id":"187","weight":2},{"id":"165","weight":3},{"id":"2255","weight":4},{"id":"1627","weight":12},{"id":"92","weight":2},{"id":"1528","weight":4},{"id":"705","weight":3},{"id":"1503","weight":9},{"id":"261","weight":3},{"id":"3181","weight":4},{"id":"3198","weight":5},{"id":"2778","weight":17},{"id":"1098","weight":2},{"id":"220","weight":3},{"id":"183","weight":4},{"id":"3129","weight":7},{"id":"3169","weight":2},{"id":"982","weight":3},{"id":"359","weight":5},{"id":"1930","weight":2},{"id":"288","weight":4},{"id":"3133","weight":3},{"id":"1553","weight":6},{"id":"1116","weight":3},{"id":"3184","weight":2},{"id":"3044","weight":7},{"id":"3170","weight":14},{"id":"485","weight":4},{"id":"3193","weight":18},{"id":"2738","weight":3},{"id":"240","weight":9},{"id":"1166","weight":3},{"id":"782","weight":2},{"id":"3125","weight":2},{"id":"2495","weight":8},{"id":"420","weight":3},{"id":"323","weight":5},{"id":"3136","weight":3},{"id":"74","weight":4},{"id":"1231","weight":5},{"id":"1214","weight":3},{"id":"3114","weight":2},{"id":"1434","weight":6},{"id":"1871","weight":5},{"id":"3188","weight":5},{"id":"3177","weight":6},{"id":"3166","weight":5},{"id":"1187","weight":2},{"id":"449","weight":3},{"id":"377","weight":4},{"id":"424","weight":10},{"id":"3029","weight":3},{"id":"3151","weight":2},{"id":"229","weight":19},{"id":"2146","weight":7},{"id":"2315","weight":7},{"id":"1423","weight":2},{"id":"1680","weight":2},{"id":"3208","weight":4},{"id":"370","weight":10},{"id":"1402","weight":6},{"id":"2299","weight":3},{"id":"2309","weight":6},{"id":"1258","weight":2},{"id":"2340","weight":2},{"id":"2286","weight":4},{"id":"2038","weight":3},{"id":"2448","weight":7},{"id":"752","weight":2},{"id":"1393","weight":3},{"id":"1714","weight":11},{"id":"3162","weight":2}],"meta":{"jsonClass":"Map$Map3","room":"The Cocatoo","date":"1360923900000","session":"1"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
