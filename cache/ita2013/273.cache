{"id":"273","paper":{"title":{"text":"An Efficient Exhaustive Low-Weight Codeword Search for Structured LDPC Codes "},"authors":[{"name":"Seyed Mehrdad Khatami"},{"name":"Ludovic Danjean"},{"name":"Dung Viet Nguyen"},{"name":"Bane Vasic"}],"abstr":{"text":"We present an algorithm to find all low-weight codewords in a given quasi-cyclic low-density parity-check code with a fixed column-weight and girth. The main idea is to view a low-weight codeword as an $(a,0)$ trapping sets, and then show that each topologically different $(a,0)$ trapping set can be dissected into smaller trapping sets.  The proposed search method relies on the knowledge of possible topologies of such smaller trapping sets present in a code ensemble, which enables an efficient search. Combined with the high-rate QC LDPC code construction which successively adds blocks of permutation matrices, the algorithm ensures that in the code construction procedure all codewords up to a certain weight are avoided, which leads to a code with the desired minimum distance."},"body":{"text":"The technique presented in [3], [4], [5] for accurately esti- mating achievable rates on binary-input partial-response chan- nels has led to interesting insights for constructing capacity- achieving codes. In particular, the mutual information rate for an independent and identically distributed (i.i.d) equiprobable input process may be appreciably lower than capacity at a moderate-to-low signal-to-noise ratio (SNR), suggesting that turbo codes and low-density parity-check (LDPC) codes alone may not be suitable at lower SNRs. However, through a simple example, Gallager showed [6, p. 208] how capacity could be achieved through the concatenation of an outer linear parity- check code (or coset thereof) with an inner transformer, or shaping encoder, that induces the capacity achieving distri- bution from an i.i.d. binary equiprobable process. Pursuing this idea with the knowledge gained in [3], [4], [5], Ma, Varnica, and Kavˇci´c [7], [8] presented the ﬁrst complete methodology for near-capacity concatenated coding systems on partial-response channels, including a novel design for an inner ﬁnite-state shaping encoder. Soriaga and Siegel [1], [2] later followed with similar concatenated coding systems that used less heuristic shaping code design methods, while Doan and Narayanan [9] provided an alternative and simpler inner code design suitable for low SNRs.\nIn this paper, we examine the problem of designing shaping encoders from the perspective of minimizing the Kullback- Leibler divergence rate between the encoder-induced process and the target process. Speciﬁcally, we consider only target processes which are ﬁnite-state Markov. By relating this problem to coding for channels with cost constraints, we are able to characterize the minimum achievable divergence rate.\nWe then revisit two effective shaping encoder construction methods presented earlier in [1], [2], [10], and evaluate their resulting divergence rates and compare them to the minimum achievable. Such example encoders include those derived from constrained graphs for typical sequences [1], as well as rate- 1 shaping encoders used in the near-capacity concatenated coding systems of [2]. For completeness, we begin with a brief review of the capacity for channels with cost constraints.\nA channel with cost constraints is a noiseless channel where each symbol x, chosen from some ﬁnite alphabet X, is assigned a nonnegative cost w(x), and the entire sequence is constrained to have an average (per-symbol) cost of no more than W . For a µth-order ﬁnite-memory cost function w(x t |x t −1 t −µ ), the cost of using a symbol x t at time t is dependent upon the last µ symbols. The average cost for a sequence x N 1 is then N −1 N t =1 w(x t |x t −1 t −µ ), where the initial cost for the ﬁrst µ terms is predeﬁned. Such cost functions can be described with a labeled directed graph G cost , in which each state corresponds to a sequence of µ symbols, and the transitions between states i = (x t −1 t −µ ) and j = (x t t −µ+1 ) are labeled with the appropriate cost w(i, j) = w(x t |x t −1 t −µ ).\nIf we deﬁne S N (W ) as the number of sequences of length N with average cost less than or equal to W , then the capacity for a given cost constraint is\nIn [11], Justesen and Høholdt give the maxentropic Markov chain for a given average cost W . Incidentally, as we shown in Appendix I, this Markov chain also achieves the capacity. Both of these results, summarized below, rely on the one- step cost-enumerator matrix A (s), with A i,j (s) = s w (i,j) , and A i,j (s) = 0 whenever w(i, j) is inﬁnite (i.e., whenever there is no transition allowed from i to j).\nTheorem 1: (Justesen and Høholdt [11, Theorem 1]) Con- sider a channel with cost constraints given by a ﬁnite-memory cost function w(i, j) and labeled graph G cost . For 0 ≤ s < 1, let A(s) be the corresponding one-step cost-enumerator matrix, with maximal eigenvalue λ(s) and left and right eigenvectors u (s) and v(s), respectively, where u(s)v(s) T =\nthe maxentropic Markov chain has state-transition probabilities p j |i = s w (i,j) v j (s)/λ(s)v i (s). \t (3)\nTheorem 2: For a channel with cost constraints given by a ﬁnite-memory cost function w(i, j) and labeled graph G cost , the capacity of the channel with an average cost constraint W (s) is C(W (s)), as given above in equations (2) and (4), respectively.\nRecall that the Kullback-Leibler (K-L) divergence rate be- tween two random processes with distributions Q and P is\n1 N\nNote that we do not necessarily have D(Q||P ) = D(P ||Q), and in some cases it has been shown that D(Q||P ) = 0 implies Q = P [12]. These properties are analogous those for the divergence between two random variables [15, p. 18]. For a given shaping encoder, we might measure D(Q||P ) between the induced encoder-output process Q and the target process P . This is a convenient criterion in that it has a closed-form expression when both processes are Markov, or in some cases Markov-driven, and share the same state space. Additionally, the near-capacity input processes we seek to induce are also ﬁnite-state Markov [5], [13].\nWhen designing a shaping encoder of a given rate, it is interesting to characterize the minimum K-L divergence rate that can be achieved. This can be derived once we consider the problem from the perspective of coding for channels with cost constraints. That is, consider a µth-order binary Markov process with distribution P , and deﬁne a corresponding ﬁnite- memory cost function,\nIf we let the initial cost of the ﬁrst µ symbols be w 0 (x µ 1 ) = − log 2 P (X µ 1 = x µ 1 ), we ﬁnd that the total cost of a sequence x N 1 is w(x N 1 ) = − log 2 P (X N 1 = x N 1 ).\nFor a random process with distribution Q and an entropy rate H(Q), the average cost is ¯ w(Q) = lim sup N →∞ N −1 E w(X N 1 ) , where the expectation is with respect to Q. This can be expressed in closed form whenever Q corresponds to a stationary Markov-driven process deﬁned on some graph G with output labels {x(e)} and transition\nprobabilities {q e }, and the labels of all µ-step paths into each state v equal x µ 1 (v); i.e.,\nMore importantly, the average cost is related to the K-L divergence rate by\nTheorem 3: For any ﬁnite-order Markov process with dis- tribution P , let\nbe the minimum divergence rate over all random process distributions Q that have an entropy rate H(Q) = R. D ∗ (R) can be represented parametrically (using Theorem 2) as\nwhere the ﬁnite-memory cost function w(x t |x t −1 t −µ ) = − log 2 P (X t = x t |X t t −µ = x t t −µ ).\nProof: One can express the capacity as C(W ) = max H(Q) over all Q such that ¯ w(Q) ≤ W . By duality, we also have that W = min ¯ w(Q) over all Q such that H(Q) ≥ C(W ). From the monotonicity and concavity properties of C(W ), this is equal to the minimum over all Q such that H(Q) = C(W ). Finally, using equation (6), we have that\n¯ w(Q) = \t min\nTherefore, we can solve for D ∗ (C(W )), and using the result in Theorem 2 we can represent this relationship parametrically as D ∗ (C(W (s))) = W (s) − C(W (s)).\nNotice that for invertible encoders with rate strictly less than the entropy rate of P , the minimum achievable divergence rate is nonzero. Interestingly, a non-invertible encoder might achieve an arbitrarily small divergence rate, but in this case the entropy rate of the encoder output may be actually less than the encoder rate. Examples for non-invertible and invertible encoders are given in the next section.\nWe now revisit two encoder construction methods developed earlier in [1], [2], and prove that there are cases where each method can attain arbitrarily low K-L divergence rates with respect to the target input process. (Full details for both of these methods can be found in [10].)\nOne simple approach to shaping encoder design which generalized Gallager\u2019s construction [6, p. 208] was presented in [2], and led to near-capacity coding systems. Brieﬂy, given a target ﬁnite-state Markov process with M states, the method constructs an M -state, rate k : n encoder by taking the n- th power of the graph that describes the target process, and\nthen approximating the n-step transition probabilities with a distribution in multiples of 2 −k (while ensuring that transi- tions which do not occur have zero probability). An encoder which induces such a distribution can then be produced by assigning each branch with input labels accordingly. We refer to this technique as Construction Method 1, and a step-by-step example given in Figure 1.\nTheorem 4: Consider any target ﬁnite-order binary aperi- odic and irreducible Markov process with distribution P and entropy rate H. For any ǫ > 0 and rate R ≥ H, there exists a rate ⌊nR⌋ : n encoder generated by Construction Method 1 which induces a distribution Q such that D(Q||P ) < ǫ, for sufﬁciently large n.\nProof: Let {p e } be the set of branch probabilities and G = (V, E, L) be the graph that together describe the target µ-th order binary Markov process with distribution P . From this, we can determine the n-step branch probabilities {p (n) e }, and the power graph G n = (V, E \u2032 , L \u2032 ). In this power graph, each edge e ∈ E \u2032 has the output label x n 1 (e), and for each each state v ∈ V , all of the µ-step paths into that state have the same label x 0 1−µ (v). This allows us to write\nFrom Construction Method 1, we know that the induced Markov distribution Q from the encoder output can be rep- resented by the set of probabilities {q e } also deﬁned on this graph G n . Therefore, noting the relationship between the\nwhere π v (Q) is the stationary state distribution of Q. (A similar expression can be found in [14], though we introduce a slight generalization for n symbols per edge.) Note that (8) is always non-negative and well-deﬁned for our situation, because q e = 0 whenever p (n) e = 0.\nNow for each state v ∈ V , let A (n) δ (v) be the δ-typical set with respect to P [15, p. 51] and conditioned on an initial state v. Following the Shannon-McMillan-Breiman theorem [15, p. 474], for any 0 < δ < 1, there exists an N 0 such that P (X n 1 ∈ A (n) δ (v)) > 1 − δ for all n > N 0 . Consequently, for any state v, we can bound the size of the typical set with A (n) δ (v) ≥ (1 − δ)2 n (H−δ) [15, Theorem 3.1.2], since we assumed the target Markov process was irreducible and aperiodic.\nWhen applying Construction Method 1, this asymptotic equipartition property also allows us to closely approximate p (n) e with a uniform distribution when n is large. That is, assuming n > N 0 , we ﬁnd some subset B (n) δ (v) ⊂ A (n) δ (v) for each state v ∈ V such that log 2 B (n) δ (v) = M def = ⌊n(H − δ) + log 2 (1 − δ)⌋ . Then we assign\nFinally, for us to derive an encoder that induces this uniform distribution, we need to make sure that all 2 ⌊nR⌋ encoder input sequences can be evenly distributed among the 2 M sequences in B (n) δ (v); i.e., we need ⌊nR⌋ ≥ M . But because R is chosen so that R ≥ H, we already have that ⌊nR⌋ ≥ M for any 1 > δ > 0. Thus, the approach above yields an encoder which approximates {p (n) e } and has q e = 0 whenever p (n) e = 0.\nGiven the encoder above, we can then calculate the diver- gence rate between the two process distributions Q and P . By substitution of q e into (8), we get\nfurther noting that M ≥ n(H − δ) + log 2 (1 − δ) − 1, we have D(Q||P ) ≤ 2δ + 1 n (1 − log 2 (1 − δ)) . Since this last bound holds for any 1 > δ > 0 and all sufﬁciently large n, D(Q||P ) can be made arbitrarily small.\nIn [1] (see also [10]), another method for encoder design was developed by exploiting the connection between typicality\nand graphical representations of sequences. More speciﬁcally, for some target process with distribution P and entropy H, it was shown that, for any ǫ > 0, a constraint graph G could always be constructed such that the capacity of the graph was greater than H − ǫ, and such that sequences generated from G were typical with respect to P , i.e., S N (G) ⊂ A (N ) ǫ for large enough N . An example of such a constraint graph is given in Figure 2. Moreover, one could then derive ﬁnite-state encoders for typical sequences by applying the state-splitting algorithm or other code design methodologies to these graphs [16], and ultimately produce an encoder for typical sequences that has a rate of at least H − δ, for any δ > 0 [1]. By further examining this overall technique, which we term Construction Method 2, we arrive at the next theorem.\nTheorem 5: Let P be the distribution of the target process with entropy rate H, and let Q be the distribution of the output process for a rate k : n encoder generated by Construction Method 2 driven by i.i.d. equiprobable binary inputs. Then D(Q||P ) = H − k/n, and thus, the divergence rate between the encoder output process and target process can be made arbitrarily small.\nProof: This can actually be proved without referring to the explicit details of the encoder graph. That is, let S nN (E, v) be the set of all length- nN sequences generated from state v of a rate k : n encoder E obtained by Construction Method 2. Noting that E is an invertible encoder, it follows that |S nN (E, v)| = 2 kN . All of these sequences are equiprobable when the encoder is driven with i.i.d. equiprobable binary inputs. This allows us to bound the K-L divergence for a sequence of N blocks, i.e.,\nBy similar arguments we can show that for any ǫ > 0 the limit-inﬁmum is bounded from below by H − k/n − ǫ, and so the theorem is proved.\nAlternatively, one might design the inner code such that mutual information rate on the channel for the encoder- induced input process, I(X ; Y), is close to that of the target input process, I(X ∗ ; Y). However, closed-form expressions for these quantities are not yet known, so in this paper we adopted the more convenient criterion of minimizing the divergence rate D(Q||P ). Although a vanishing D(Q||P ) may be sufﬁcient if one desires an arbitrarily small gap between I(X ; Y) and I(X ∗ ; Y), this condition is not actually necessary. The analysis of [17] examines the set of necessary conditions, and the construction methods of [8] are based on other criteria and still achieve rates very near capacity.\nFinally, since we related shaping encoder design to coding for channels with cost constraints, one might use another method to construct encoders which minimize the divergence rate, such as that of Khayrallah and Neuhoff [18].\nTo our knowledge, the relationship between Theorem 2 and Theorem 1 (from [11]) has not appeared in the literature, but the analysis we employ below arises in other problems such as the computation of asymptotic weight spectra for convolutional codes (e.g., see Pﬁster [19, Theorem 3.3.6], and references therein). Moreover, it is generally accepted that the result in [11] is a lower bound to capacity even though an explicit code construction is not given. Details for the straightforward code construction from typical sequences can be found in [10].\nProof: We need only show that the capacity of a channel with cost constraints can be upper bounded by C(W (s)) ≤ log 2 λ(s) − W (s) log 2 s, for 0 ≤ s < 1, where\n(9) The lower bound follows from Theorem 1. (See [10].)\nLet us begin by considering some ﬁxed cost constraint W , and for the set S N (W ) let us deﬁne a cost-enumerating func- tion B(N, W, s) = h ≤W N b h s h , where b h is the number of sequences with total cost h. This enumerator can also be written as\nwhere α h is the fraction of sequences in S N (W ) with total cost h. For any 0 ≤ s < 1, as a consequence of Jensen\u2019s\ninequality (e.g., [15, p. 25]), we have |S N (W )|\n≥ |S N (W )| s W N P h ≤W N α h = |S N (W )| s W N .\nbecause B(N, W, s) only enumerates a subset of all sequences. If we assume that the enumerator matrix is irreducible, i.e., for each (i, j) there exists an ℓ such that A(s) ℓ i,j > 0, then its largest (nonnegative) eigenvalue λ(s) must have left and right eigenvectors with strictly positive components; and this can be used to show that\nas in [16, p. 1667]. Therefore, we can conclude that |S N (W )| s W N ≤ B(N, W, s) ≤ Kλ(s) N ,\nMoreover, this upper bound can be tightened by maximizing the right-hand side with respect to s, i.e., by taking derivatives and determining the extremal points. This results in a depen- dence of W on s corresponding to the middle expression in equation (9). Therefore, to complete the proof of the upper bound it remains to be shown that\nRecall that the eigenvectors u (s) and v(s) are assumed to be normalized so that u (s)v(s) T = 1. Then, λ(s) = u (s)A(s)v(s) T = i,j u i (s)s w (i,j) v j (s). Taking the deriva- tive, we ﬁnd\nUsing this argument, the second expression on the right side of (10) becomes\nwhich equals λ(s) ∂ ∂s u (s)v(s) T . But from the assumption that u (s)v(s) T = 1, this derivative is zero. Therefore, we conclude that equation (10) simpliﬁes to\nand the result in (9) readily follows. Thus the upper bound is proved."},"refs":[{"authors":[{"name":"J. B. Soriaga"},{"name":"P. H. Siegel"}],"title":{"text":"On distribution shaping codes for partial- response channels"}},{"authors":[{"name":"J. B. Soriaga"},{"name":"P. H. Siegel"}],"title":{"text":"Near-capacity coding systems for partial- response channels"}},{"authors":[{"name":"D. Arnold"},{"name":"H. Loeliger"}],"title":{"text":"On the information rate of binary-input channels with memory"}},{"authors":[{"name":"V. Sharma"},{"name":"S. K. Singh"}],"title":{"text":"Entropy and channel capacity in the re- generative setup with applications to Markov channels"}},{"authors":[{"name":"H. D. Pﬁster"},{"name":"J. B. Soriaga"},{"name":"P. H. Siegel"}],"title":{"text":"On the achievable information rates of ﬁnite state ISI channels"}},{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Information Theory and Reliable Communication"}},{"authors":[{"name":"X. Ma"},{"name":"N. Varnica"},{"name":"A. Kav˘ci´c"}],"title":{"text":"Matched information rate codes for binary ISI channels"}},{"authors":[{"name":"N. Varnica"},{"name":"X. Ma"},{"name":"A. Kavˇci´c"}],"title":{"text":"Capacity-approaching codes for par- tial response channels"}},{"authors":[{"name":"D. Doan"},{"name":"K. R. Narayanan"}],"title":{"text":"Design of good low rate codes for ISI channels based on spectral shaping"}},{"authors":[{"name":"J. Soriag"}],"title":{"text":"On Near-Capacity Code Design for Partial-Response Chan- nels"}},{"authors":[{"name":"J. Justesen"},{"name":"T. Høholdt"}],"title":{"text":"Maxentropic Markov chains"}},{"authors":[{"name":"K. Marton"},{"name":"P. C. Shields"}],"title":{"text":"Ergodic processes and zero divergence"}},{"authors":[{"name":"J. Chen"},{"name":"P. H. Siegel"}],"title":{"text":"Markov processes asymptotically achieve the capacity of ﬁnite state intersymbol interference channels"}},{"authors":[{"name":"Z. Rached"},{"name":"F. Alajaji"},{"name":"L. L. Campbell"}],"title":{"text":"The Kullback-Leibler diver- gence rate between Markov sources"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"B. H. Marcu"},{"name":"R. M. Rot"},{"name":"P. H. Siege"}],"title":{"text":"Handbook of Coding Theory, ch"}},{"authors":[{"name":"S. Shamai"},{"name":"S. Verdú"}],"title":{"text":"The empirical distribution of good codes"}},{"authors":[{"name":"A. S. Khayrallah"},{"name":"D. L. Neuhoff"}],"title":{"text":"Coding for channels with cost constraints"}},{"authors":[{"name":"H. D. Pﬁste"}],"title":{"text":"On the Capacity of Finite State Channels and the Analysis of Convolutional Accumulate- m Codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/ita2013/273"},"links":[{"id":"205","weight":4},{"id":"2760","weight":18},{"id":"1486","weight":2},{"id":"2741","weight":9},{"id":"3180","weight":5},{"id":"67","weight":20},{"id":"2206","weight":3},{"id":"1606","weight":7},{"id":"153","weight":2},{"id":"3037","weight":9},{"id":"93","weight":3},{"id":"436","weight":5},{"id":"3191","weight":6},{"id":"12","weight":4},{"id":"223","weight":2},{"id":"1547","weight":7},{"id":"1278","weight":4},{"id":"2287","weight":2},{"id":"3200","weight":11},{"id":"3008","weight":10},{"id":"2217","weight":5},{"id":"3010","weight":3},{"id":"3222","weight":2},{"id":"2291","weight":3},{"id":"307","weight":12},{"id":"981","weight":12},{"id":"1430","weight":6},{"id":"3021","weight":4},{"id":"3199","weight":4},{"id":"813","weight":4},{"id":"3069","weight":6},{"id":"2634","weight":10},{"id":"526","weight":5},{"id":"2442","weight":6},{"id":"3183","weight":14},{"id":"175","weight":4},{"id":"2763","weight":4},{"id":"2534","weight":2},{"id":"1235","weight":3},{"id":"135","weight":8},{"id":"404","weight":17},{"id":"702","weight":5},{"id":"450","weight":5},{"id":"770","weight":8},{"id":"1082","weight":6},{"id":"3182","weight":2},{"id":"1889","weight":4},{"id":"762","weight":2},{"id":"2002","weight":2},{"id":"3065","weight":29},{"id":"3195","weight":5},{"id":"284","weight":6},{"id":"1060","weight":5},{"id":"3132","weight":7},{"id":"1160","weight":4},{"id":"3187","weight":12},{"id":"475","weight":3},{"id":"944","weight":4},{"id":"3059","weight":6},{"id":"2646","weight":4},{"id":"104","weight":5},{"id":"90","weight":7},{"id":"236","weight":5},{"id":"117","weight":4},{"id":"3111","weight":8},{"id":"2330","weight":5},{"id":"1442","weight":8},{"id":"2745","weight":4},{"id":"2276","weight":6},{"id":"2352","weight":8},{"id":"3176","weight":7},{"id":"3255","weight":4},{"id":"470","weight":2},{"id":"1643","weight":8},{"id":"250","weight":4},{"id":"163","weight":5},{"id":"26","weight":3},{"id":"2455","weight":4},{"id":"1298","weight":9},{"id":"3175","weight":11},{"id":"3201","weight":7},{"id":"1268","weight":5},{"id":"2830","weight":6},{"id":"679","weight":15},{"id":"3135","weight":3},{"id":"123","weight":14},{"id":"3049","weight":5},{"id":"1479","weight":2},{"id":"1215","weight":8},{"id":"1223","weight":4},{"id":"107","weight":8},{"id":"1473","weight":2},{"id":"217","weight":2},{"id":"1462","weight":3},{"id":"13","weight":2},{"id":"3038","weight":8},{"id":"1996","weight":3},{"id":"325","weight":4},{"id":"2025","weight":2},{"id":"973","weight":8},{"id":"1279","weight":2},{"id":"2119","weight":2},{"id":"888","weight":3},{"id":"1436","weight":2},{"id":"155","weight":3},{"id":"1877","weight":7},{"id":"83","weight":4},{"id":"3186","weight":8},{"id":"3072","weight":2},{"id":"306","weight":11},{"id":"3057","weight":3},{"id":"3068","weight":2},{"id":"1107","weight":2},{"id":"2697","weight":7},{"id":"398","weight":5},{"id":"152","weight":6},{"id":"2750","weight":6},{"id":"1421","weight":7},{"id":"3105","weight":2},{"id":"2178","weight":4},{"id":"1866","weight":7},{"id":"3233","weight":2},{"id":"2488","weight":4},{"id":"3116","weight":3},{"id":"900","weight":11},{"id":"59","weight":5},{"id":"353","weight":3},{"id":"1846","weight":4},{"id":"1074","weight":9},{"id":"3168","weight":8},{"id":"1136","weight":5},{"id":"1103","weight":2},{"id":"3157","weight":13},{"id":"2317","weight":14},{"id":"3127","weight":3},{"id":"76","weight":4},{"id":"419","weight":4},{"id":"372","weight":4},{"id":"138","weight":17},{"id":"3073","weight":2},{"id":"54","weight":27},{"id":"1660","weight":4},{"id":"408","weight":4},{"id":"3005","weight":4},{"id":"1696","weight":16},{"id":"3016","weight":2},{"id":"1212","weight":2},{"id":"2324","weight":5},{"id":"487","weight":3},{"id":"508","weight":3},{"id":"3027","weight":5},{"id":"1096","weight":2},{"id":"71","weight":17},{"id":"642","weight":4},{"id":"3196","weight":6},{"id":"2033","weight":4},{"id":"1830","weight":6},{"id":"108","weight":5},{"id":"202","weight":3},{"id":"691","weight":7},{"id":"2740","weight":3},{"id":"3202","weight":4},{"id":"2812","weight":2},{"id":"3243","weight":4},{"id":"3134","weight":9},{"id":"1365","weight":4},{"id":"3185","weight":8},{"id":"1671","weight":11},{"id":"1325","weight":4},{"id":"1844","weight":2},{"id":"219","weight":2},{"id":"428","weight":6},{"id":"2174","weight":2},{"id":"2759","weight":7},{"id":"1128","weight":4},{"id":"2319","weight":3},{"id":"431","weight":4},{"id":"82","weight":7},{"id":"1376","weight":2},{"id":"1448","weight":17},{"id":"3056","weight":8},{"id":"1","weight":10},{"id":"2996","weight":14},{"id":"2617","weight":11},{"id":"2435","weight":4},{"id":"3174","weight":9},{"id":"3051","weight":4},{"id":"1499","weight":4},{"id":"1840","weight":9},{"id":"1988","weight":2},{"id":"1908","weight":10},{"id":"771","weight":6},{"id":"3088","weight":6},{"id":"221","weight":4},{"id":"25","weight":3},{"id":"658","weight":8},{"id":"2743","weight":2},{"id":"687","weight":4},{"id":"230","weight":5},{"id":"3123","weight":3},{"id":"309","weight":5},{"id":"2773","weight":14},{"id":"31","weight":15},{"id":"3071","weight":4},{"id":"503","weight":4},{"id":"3163","weight":2},{"id":"1439","weight":11},{"id":"278","weight":17},{"id":"764","weight":16},{"id":"3113","weight":7},{"id":"1444","weight":18},{"id":"151","weight":58},{"id":"887","weight":5},{"id":"95","weight":4},{"id":"1109","weight":3},{"id":"1438","weight":6},{"id":"264","weight":5},{"id":"257","weight":4},{"id":"3030","weight":3},{"id":"694","weight":18},{"id":"1573","weight":14},{"id":"203","weight":4},{"id":"1847","weight":30},{"id":"3205","weight":9},{"id":"1509","weight":19},{"id":"3108","weight":3},{"id":"1836","weight":3},{"id":"246","weight":8},{"id":"3167","weight":7},{"id":"115","weight":12},{"id":"1822","weight":3},{"id":"275","weight":9},{"id":"376","weight":8},{"id":"308","weight":3},{"id":"2838","weight":3},{"id":"3197","weight":7},{"id":"293","weight":8},{"id":"630","weight":6},{"id":"767","weight":8},{"id":"354","weight":2},{"id":"326","weight":14},{"id":"2939","weight":2},{"id":"1076","weight":2},{"id":"430","weight":2},{"id":"1915","weight":8},{"id":"1905","weight":2},{"id":"3189","weight":8},{"id":"2862","weight":5},{"id":"402","weight":3},{"id":"371","weight":9},{"id":"2443","weight":3},{"id":"1084","weight":8},{"id":"2729","weight":13},{"id":"2316","weight":2},{"id":"3156","weight":9},{"id":"1883","weight":2},{"id":"3074","weight":4},{"id":"1102","weight":2},{"id":"2173","weight":6},{"id":"2753","weight":2},{"id":"1517","weight":7},{"id":"198","weight":16},{"id":"1319","weight":3},{"id":"699","weight":5},{"id":"506","weight":17},{"id":"187","weight":7},{"id":"165","weight":10},{"id":"1627","weight":3},{"id":"92","weight":2},{"id":"705","weight":3},{"id":"621","weight":20},{"id":"3070","weight":6},{"id":"1503","weight":7},{"id":"3181","weight":4},{"id":"1546","weight":9},{"id":"3198","weight":3},{"id":"2778","weight":6},{"id":"1098","weight":2},{"id":"1861","weight":4},{"id":"660","weight":6},{"id":"183","weight":44},{"id":"688","weight":3},{"id":"3129","weight":7},{"id":"3169","weight":6},{"id":"982","weight":4},{"id":"359","weight":3},{"id":"1930","weight":3},{"id":"3173","weight":5},{"id":"288","weight":4},{"id":"3133","weight":2},{"id":"1553","weight":2},{"id":"1116","weight":2},{"id":"3184","weight":5},{"id":"3044","weight":4},{"id":"3170","weight":2},{"id":"485","weight":5},{"id":"3193","weight":9},{"id":"2738","weight":8},{"id":"240","weight":6},{"id":"1166","weight":8},{"id":"1550","weight":3},{"id":"782","weight":6},{"id":"3125","weight":7},{"id":"2495","weight":13},{"id":"420","weight":2},{"id":"323","weight":3},{"id":"3136","weight":4},{"id":"74","weight":10},{"id":"1456","weight":2},{"id":"243","weight":13},{"id":"1231","weight":6},{"id":"1214","weight":9},{"id":"1083","weight":21},{"id":"3114","weight":3},{"id":"1434","weight":9},{"id":"1871","weight":5},{"id":"3188","weight":14},{"id":"3177","weight":3},{"id":"643","weight":5},{"id":"3166","weight":7},{"id":"1187","weight":11},{"id":"449","weight":2},{"id":"377","weight":4},{"id":"424","weight":7},{"id":"3029","weight":11},{"id":"3151","weight":13},{"id":"229","weight":3},{"id":"2146","weight":9},{"id":"2638","weight":6},{"id":"2315","weight":15},{"id":"1423","weight":5},{"id":"1680","weight":10},{"id":"3208","weight":14},{"id":"1402","weight":5},{"id":"2299","weight":8},{"id":"2309","weight":3},{"id":"1258","weight":6},{"id":"2288","weight":6},{"id":"2340","weight":2},{"id":"2286","weight":22},{"id":"2038","weight":8},{"id":"752","weight":4},{"id":"1393","weight":2},{"id":"1714","weight":3},{"id":"3162","weight":5}],"meta":{"jsonClass":"Map$Map3","room":"The Mackaw","date":"1360584000000","session":"2"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
