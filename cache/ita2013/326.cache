{"id":"326","paper":{"title":{"text":"Polarity-balanced codes "},"authors":[{"name":"Weber, Jos"},{"name":"Immink, Kees"},{"name":"Siegel, Paul"},{"name":"Swart, Theo"}],"abstr":{"text":"Balanced bipolar codes consist of sequences in which the symbols `-1' and `+1' appear equally often. Several generalizations to larger alphabets have been considered in literature. For example, for the q-ary alphabet {-q+1, -q+3, ..., q-1}, known concepts are symbol balancing, i.e., all alphabet symbols appear equally often in each codeword, and charge balancing, i.e., the symbol sum in each codeword equals zero. These notions are equivalent for the bipolar case, but not for q>2. In this paper, a third perspective is introduced, called polarity balancing, where the number of positive symbols equals the number of negative symbols in each codeword. The minimum redundancy of such codes is determined and a generalization of Knuth's celebrated bipolar balancing algorithm is proposed."},"body":{"text":"The problem of diagnosing an unobserved \u201dstate of the world\u201d from a set of available measurements and/or tests is quite common in practice. Examples include medical diagnosis, computer system trou- bleshooting, and decoding messages sent through a noisy channel. However, there is a trade-off between the quality of diagnosis and its cost, which involves both the cost of testing (e.g., the number of test if their costs are equal) and the computational cost of performing diagnosis.\nOne way to look at diagnostic problem is to view it as a combined source-channel coding, where the unknown state of the world described by a set of hidden variables X = (X 1 , ..., X n ) represents an input message, while the set of observed test outcomes Y = (T 1 , ..., T m ) corresponds to the output message that results from sending some \u201dencoding\u201d of X, deﬁned by the nature of tests, through a \u201dnoisy channel\u201d, determined by the nature of environment. The main difference from classical coding problem is that (1) source and channel coding are not always separable and (b) coding is constrained: we can only choose from a set of available tests rather than freely select arbitrary encoding functions.\nParticularly, in this paper, we will focus on disjunc- tive testing motivated by fault diagnosis problem in distributed computer systems using probes. A probe is an end-to-end test transaction (e.g., ping, webpage access, database query, an e-commerce transaction, etc.) sent through the system for the purposes of performance monitoring. A probe can be viewed as a\ndisjunctive test over the components the probe depends (it returns OK if and only if all components on its path are OK). In case of noisy probe outcomes, we address diagnosis as a probabilistic inference in a Bayesian network that represents the dependencies between the unobserved states of system components and observed probe outcomes; conditional probabilities for probe outcomes given the corresponding components are deﬁned by the noisy-OR model which generalizes disjunctive tests to the case of noisy environment.\nWe consider the test selection problem in both non- adaptive and adaptive settings. Nonadaptive setting assumes that a subset of tests must be selected ofﬂine prior to diagnosis, while in adaptive case the outcomes of the previous tests are known prior to selecting the next test. The nonadaptive probe selection problem is NP-hard [9], but the greedy approaches based on maximizing information gain (i.e. minimizing the con- ditional entropy about the unobserved nodes) work quite well in practice, particularly in cases when the number of faults is small and thus the state space of unobserved variables can be easily enumerated.\nHowever, in a general multi-fault case the state space is exponential in the number of variables, and a compact representation such as Bayesian network must be used. Unfortunately, exact computation of conditional entropies in a general Bayesian network can be intractable. While much existing research has addressed the problem of efﬁcient and accurate prob- abilistic inference, other probabilistic quantities, such as conditional entropy and information gain, have not received nearly as much attention. Most of the existing literature on value of information and most- informative test selection [8], [2], [7], [15] does not seem to focus on the computational complexity of most-informative test selection in a general Bayesian network setting, except for the most recent work by [11]. However, [11] focus on the nonadaptive (\u201dnon- myopic\u201d) problem and provide algorithms for efﬁcient selection of a most-informative test subset given a bound on its size. Our problem is different as we consider adaptive (\u201dmyopic\u201d) test selection without a particular bound on the number of tests. We will describe our approximation algorithm for computing marginal conditional entropy [16]. The algorithm is based on loopy belief propagation, a successful ap- proximate inference method. We illustrate the algo-\nrithm at work in the setting of fault diagnosis for distributed computer networks. However, the method is general enough to be used in other applications of Bayesian networks that require the computation of information gain and conditional entropies of subsets of nodes.\nFinally, we present some theoretical results for efﬁciency versus accuracy trade-off in diagnosis. Mo- tivated by the Shannon\u2019s channel capacity result that provides conditions for asymptotically error-free de- coding, one may ask whether similar conditions can be stated for certain classes of diagnostic problems (such as noisy disjunctive testing, or noisy-OR problems) as both the number of hidden and observed variables increase. While deriving achievable limit for such constrained classes of coding problems appears to be challenging, we are able to derive a lower bound on the diagnostic accuracy that provides necessary conditions for the number of probes needed to achieve asymp- totically error-free diagnosis. Herein, we summarize our results from [13], providing lower bounds on the bit-error rate which assumes most-likely diagnosis for each unobserved variable (\u201dbit-wise decoding\u201d). See [14] for analysis of block-error rate, or Maximum-A- Posteriory (MAP) diagnosis.\nLet X = {X 1 , X 2 , . . . , X N } denote a set of N discrete random variables and x a possible re- alization of X. A Bayesian network is a directed acyclic graph (DAG) G with nodes corresponding to X 1 , X 2 , . . . , X N and edges representing direct de- pendencies [12]. The dependencies are quantiﬁed by associating each node X i with a local conditional probability distribution P (x i | pa i ), where pa i is an assignment to the parents of X i (nodes pointing to X i in the Bayesian network). The set of nodes {x i , pa i } is called a family. The joint probability distribution function (PDF) over X is given as product\nWe use E ⊆ X to denote a possibly empty set of evidence nodes for which observation is available.\nFor ease of presentation, we will also use the ter- minology of factor graphs [6], which uniﬁes directed and undirected graphical representations of joint PDFs. A factor graph is an undirected bipartite graph that contains factor nodes (usually shown as squares) and variable nodes (shown as circles). (See Fig. 1 for an example.) There is an edge between a variable node and a factor node if and only if the variable participates in the potential function of the corresponding factor. The joint distribution is assumed to be written in a factored form\nwhere Z is a normalization constant called the parti- tion function, and the index a ranges over all factors f a (x a ), deﬁned on the corresponding subsets X a of X.\nParticularly, we will consider the following diagnos- tic Bayesian networks. Let X = {X 1 , X 2 , . . . , X N } denote a set of unobserved random variables we wish to diagnose, and let T = {T 1 , T 2 , . . . , T M } denote the available set of tests. We assume that test outcomes are independent given the states of components, and that component failures are marginally independent. These assumptions are captured by a bipaprtite Bayesian network that represents the above independence as- sumptions about the joint probability P (x, t):\nFig. 1 shows a factor graph representation of our model.\nGiven the probe outcomes, we wish to ﬁnd the most- likely assignment (called maximum aposteriory prob- ability, or MAP) to all X i nodes given the probe out- comes, i.e. x ∗ = arg max x P (x|t). Since P (x|t) =\nP (t) does not depend on x, we get x ∗ = arg max x P (x, t). An alternative approach is to ﬁnd the most likely value x ∗ i of each node X i separately, i.e. to ﬁnd an assignment x \u2032 = (x \u2032 1 , ..., x \u2032 n ) where x \u2032 i = arg max x i P (x i |t), i = 1, ..., n. We refer to the latter approach as bit-wise diagnosis (bit-wise decoding), while the MAP approach can be viewed as a block-wise diagnosis (block-wise decoding). Bit-wise diagnosis is more suited when using belief updating al- gorithms that compute posterior probability P (X i |T) for each X i , rather than perform global optimization to ﬁnd MAP, using either search or dynamic program- ming [4].\nUnfortunately, both MAP inference and belief up- dating are known to be NP-hard [1], and the complex- ity of best-known inference techniques is exponential in the graph parameter known as treewidth, or induced width [3], which reﬂects the size of a largest clique in the graph (and thus the largest dependency) created by an inference algorithm. However, there exists a simple linear-time approximate inference algorithm known as belief propagation (BP) [12]. BP is provably correct on polytrees (i.e. Bayesian networks with no undirected cycles), and can be used as an approximation on\ngeneral networks. In belief propagation, probabilistic messages are iterated between the nodes. The process could diverge; convergence is guaranteed only for polytrees.\nIn many diagnosis problems, the user has an oppor- tunity to select tests in order to improve the accuracy of diagnosis. For example, in medical diagnosis, doc- tors face the experiment design problem of choosing which medical tests to perform next.\nOur objective is to maximize diagnostic quality while minimizing the cost of testing. The diagnostic quality of a subset of tests T ∗ can be measured by the amount of uncertainty about X that remains after observing T ∗ . From the information-theoretic perspective, a natural measurement of uncertainty is the conditional entropy H(X | T ∗ ). Clearly, H(X | T ) ≤ H(X | T ∗ ) for all T ∗ ⊆ T. Thus the problem is to ﬁnd T ∗ ⊆ T which minimizes both H(X | T ∗ ) and the cost of testing. When all tests have equal cost, this is equivalent to minimizing the number of tests.\nThis problem is known to be NP-hard [9]. A simple greedy approximation is to choose the next test to be T ∗ = arg min T H(X | T, T \u2032 ), where T \u2032 is the currently selected test set. The expected number of tests produced by the greedy strategy is known to be within a O(log N ) factor from optimal (see [16]). The same result holds for approximations (within a constant multiplicative factor) to the greedy approach. Furthermore, our empirical results show that the ap- proach works well in practice [9].\nWe make a distinction between nonadaptive (off- line) test selection and adaptive (online) test selec- tion. In online selection, previous test outcomes are available when selecting the next test. Off-line test selection attempts to plan a suite of tests before any observations have been made. We will focus on the online approach, sometimes called active diagnosis, which is typically much more efﬁcient in practice than its off-line counterpart [9].\nAdaptive Test Selection Problem: Given the ob- served outcome t \u2032 of previously selected sequence of tests T \u2032 , select the next test to be arg min T H(X | T, t \u2032 ).\nIn a Bayesian network, the joint entropy H(X) can be decomposed into sum of entropies over the families and thus can be easily computed using the input potential functions. Conditional marginal entropies, on the other hand, do not generally have this property - see the lemmas below (and [16] for the proofs). Un- der certain independence conditions they decompose into functions over the families. But computing those functions will require inference\nLemma 1: Given a Bayesian network representing a joint PDF P (X), the joint entropy H(X) can be de- composed into the sum of entropies over the families: H(X) = N i=1 H(X i | Pa i ).\nLemma 2: Given a Bayesian network representing a joint PDF P (X, T), where ∀i : pa T i ⊆ X (i.e. tests T i and T j are independent given a subset of X), the observation t \u2032 of previously selected test set, and a candidate test T , the conditional marginal entropy H(X | T, t \u2032 ) can be written as\nMinimizing conditional entropy is a particular in- stance of value-of-information (VOI) analysis [7], where tests are selected to minimize the expected value of a certain cost function c(x, t, t \u2032 ). The result of Lemma 2 can be generalized to this case if the cost function is decomposable over the families (see [16]).\nSince observations of test outcome correlate the parent nodes, the exact computation of all the pos- terior probabilities in Eqn. (4) is intractable. We can certainly use an existing approximation method to compute P (s pa T , t | t \u2032 ) and P (t | t \u2032 ). But a more efﬁcient approach is possible if we exploit the belief propagation infrastructure.\nLet us consider the problem of computing the conditional marginal entropy\nP (x | e), x\\x a representing variable nodes not in x a . The trick is to replace the marginal posterior P (x a | e) with its factorized BP approximation, and make use of the BP message passing mechanism to perform the summation over x a . We call this process Belief Propagation for Entropy Approximation (BPEA).\nPick any node X 0 from X a and designate it as the root node. We modify the ﬁnal message passed to X 0 as follows:\nHere, ˜ b a (x a ) is the unnormalized belief of X a (i.e., ˜b a (x a ) = σb a (x a ), where σ = x a ˜b a (x a )).\nPlugging in ˜ b a (x a ) in place of P (x a | e) in Eqn. 5, we see that it only remains to sum over the root node X 0 and normalize properly.\nσ + log σ. \t (8) It follows immediately that BPEA is exact whenever BP is exact.\nThe normalization constant σ is already computed during normal BP iterations. The computation of ˜ b a (·),\nm \u2032 a→i , and ˜ h(·) can all be piggy-backed onto the same BP infrastructure, and therefore does not impact its overall complexity. Furthermore, due to the local and parallel message update procedure in BP, we can compute the marginal posterior entropies of multiple families in one single sweep. This is an important advantage for the adaptive testing setup.\nIt is also easy to show that the approach is ex- tendible beyond the entropy computation, to an arbi- trary cost function decomposable over families (see [16]). The cost function replaces the negative loga- rithm in Eqns. (5) and (6).\nSuppose we wish to monitor a system of networked computers. Let X represent the binary state of N network elements. X i = 0 indicates that the element is in normal operation mode, and X i = 1 indicates that the element is faulty. We can take X i to be any system component whose state can be measured using a suite of tests. If the system is large, it is often impossible to test each individual component directly. A common solution is to test a subset of components with a single test probe. If all the test components are okay, the test would return a 0. Otherwise the test would return 1, but it does not reveal which components are faulty.\nWe assume there are machines designated as probe stations, which are instrumented to send out probes to test the response of the network elements represented by X. Let T denote the available set of probes. A probe can be as simple as a ping request, which detects network availability. A more sophisticated probe might be an e-mail message or a webpage-access request.\nIn the absence of noise a probe is a disjunctive test: it fails if an only if there is at least one failed node on its path. More generally, it is a noisy-OR test [12]. The joint PDF of all tests and network nodes forms the well-known QMR-DT model [10]:\nP (x j ) = (α j ) x j (1 − α j ) (1−x j ) , \t (9) P (t i = 0 | s pa i ) = ρ i0\nHere, α j := P (x j = 1) is the prior fault probability, ρ ij is the so-called inhibition probability, and (1−ρ i0 ) is the leak probability of an omitted faulty element. The inhibition probability is a measurement of the amount of noise in the network.\nAs discussed in Section III, we adopt the adap- tive testing framework for fault diagnosis, sequen- tially selecting probes to minimize the conditional entropy. Our previous work [14] makes the single- fault assumption, which effectively reduces S to one random variable with N +1 possible states. In general, however, multiple faults could exist in the system simultaneously, which requires the more complicated conditional entropy given in Eqn. (4).\nLet A(T, X pa T | t \u2032 ) denote the ﬁrst term in Eqn. (4). This is the cross entropy between the posterior prob- ability of T and its parents, and the conditional prob- ability of T given its parents. The second term in Eqn. (4) is simply the negative conditional entropy −H(T | t \u2032 ).\nWe deal with the two entropy terms separately. For H(T | t \u2032 ), we may use approximation methods such as BP or GBP to calculate the belief b(t | t \u2032 ), which can then be used to directly compute H(T | t \u2032 ). (Note that the summation over values of T is simple since T is binary-valued.) To calculate A(T, X pa T | t \u2032 ), we use the entropy approximation method BPEA, as described in Section IV. Because BP message updates are done locally, we can compute A(T, X pa T | t \u2032 ) for all unobserved T nodes during a single application of BP. Thus, picking the next probe requires only one run of the BPEA approximation algorithm.\nFor each candidate probe, we designate the probe node T itself as the root node. The unnormalized belief is ˜ b t (t, x pa T ) := P (t | x pa T ) j∈pa\nn j→t (x j ). This is used to calculate the modiﬁed message m \u2032 a→t (t) (cf. Eqn. (6)). However, since A(T, S pa T | t \u2032 ) is a cross entropy term, we do not take the log of ˜b, but rather take the logarithm of the known probabilities P (t | x pa T ). This simpliﬁes the normalization step described in Eqn. (8) to A(T, X pa T | t \u2032 ) = ˜ A(T, X pa T | t \u2032 )/σ, where σ = t,x\nAn interesting question one may ask is how many tests might be needed to guarantee accurate diagnostic results, assuming an ideal situation when the tests (probes) can be constructed rather than selected from a predeﬁned set of available probes. We can ask for Shannon-limit type of a result, i.e. what is the minimal redundancy given by the ratio between the number of probes versus the number of unobserved nodes, that can guarantee that diagnostic error will approach zero as the number of components and probes goes to inﬁnity? While deriving achievable limit is hard, we show below a lower bound on diagnostic error when using bit-wise most-likely diagnosis (measured as the bit error rate (BER)), for general bipartite Bayesian networks and particularly for noisy-OR bipartite net- works.\narg max x P (X i = x|T ) is the most-likely assignment to X i given observed vector T. Note that X \u2032 i (T) is a deterministic function if a deterministic tie-breaking rule is used for most-likely assignment (e.g., X \u2032 i = 0 if P (X i = 0|T ) = 0.5).\nTheorem 3: Given a bipartite Bayesian network that deﬁnes a joint distribution P (x, t) as speciﬁed by the equation 3, the bit error rate (BER) of bit-wise most-likely diagnosis is bounded from below as follows\nwhere c = max i |ch i |, |ch i being the number of X i \u2019s children, p max = max i max j∈{0,1} P (X i = j) is the maximum prior probability over all nodes, and α k = max j∈{1,...,m} max pa j (t j ) P (t j = k|pa j (t j )) is the maximum conditional probability of the test outcome k ∈ {0, 1}, over all test variables and over all assignments to their corresponding parent nodes.\nParticularly, this can be applied to diagnosis in noisy-OR networks. To simplify our analysis, let us assume a particular structure that we will call a (k,c)- regular bipartite graph, where each node in the lower layer has exactly k parents in the upper layer, and each node in the upper layer has c = km/n children in the lower layer (recall that there are n nodes in the upper layer and m nodes in the lower layer). Then the following results follow straightforwardly from the previous theorem:\nCorollary 4: Given a Bayesian network having the (k,c)-regular bipartite graph structure, where n is the number of hidden nodes, m is the number of tests, and where all conditional probabilities P (t j |pa(t j )) are noisy-OR functions having the link probability at least q and the leak probability at most q leak , the bit error rate (BER) of bit-wise most-likely diagnosis is bounded from below as follows: BER ≥\nL N OR BER = 1 − p max (1 + q leak (1 − q k )) km/n . (13) Corollary 5: Given a bipartite Bayesian network\nthat deﬁnes a joint distribution P (x, t) as speciﬁed by the equation 3, a necessary condition for achieving error-free bit-wise diagnosis is\nwhere c, α 0 and α 1 are deﬁned as in Theorem 2. Par- ticularly, for noisy-OR networks deﬁned in Corollary 13, the necessary condition is\n. (15) Assuming equal prior fault probabilities p =\nP (X i = 1), where p < 0.5 (typically, system\u2019s components are unlikely to be faulty), we get m n ≥\nk log(1+q leak (1−q k )) . In Figure 2a, we illustrate the growth of the lower bound on rate m/n with the increasing prior fault probability p, for different probe sizes k, and for a ﬁxed noise parameters. As expected, higher probe to node ratio is necessary for higher fault probability p. Also, somewhat intuitively, longer probes (larger k) allow to reduce the required number of probes per node. However, this does not always happen in practice, which indicates that the bound is not tight, and indeed provides only necessary, but not sufﬁcient, conditions for error-free diagnosis.\nOne direction for future work would be to provide achievable bounds, similar to Shannon limit, for the above constrained coding problem that only permits disjunctive codes, and a particular type of channel deﬁned by noisy-OR model. Namely, one would like\nto know if asymptotically error-free diagnosis is ac- tually achievable at ﬁnite rate m/n, and under what conditions on prior p, noise parameters, and probe set construction. While there is a large amount of related work in the area of group testing (e.g., see [5]), this particular setting does not seem to be studied before. Moreover, taking into account constraints on probe construction (e.g., due to the network topology restric- tions) makes the analysis much more complicated."},"refs":[{"authors":[{"name":"F. Cooper"}],"title":{"text":"G"}},{"authors":[{"name":"J. de Klee"},{"name":"C. Williams"}],"title":{"text":"B"}},{"authors":[{"name":"R. Dechter"}],"title":{"text":"Bucket elimination: A unifying framework for probabilistic inference"}},{"authors":[{"name":"R. Dechte"},{"name":"I. Rish"},{"name":"J. of AC"}],"title":{"text":"Mini-buckets: A General Scheme for Approximating Inference"}},{"authors":[{"name":"Z. D"},{"name":"K. Hwang"}],"title":{"text":"D- F"}},{"authors":[{"name":"F. R. Kschischan"},{"name":"B. J. Fre"},{"name":"H.-A. Loeliger"}],"title":{"text":"Factor graphs and the sum-product algorithm"}},{"authors":[{"name":"D. E. Heckerma"},{"name":"E. J. Horvit"},{"name":"B. Middleton"}],"title":{"text":"An approx- imate nonmyopic computation for value of information"}},{"authors":[{"name":"R. Howard"}],"title":{"text":"Information value theory"}},{"authors":[{"name":"S. Ma N"},{"name":"A. Beygelzimer G"},{"name":"K. Her- nandez I"},{"name":"M. Brodie"}],"title":{"text":"Odintsova  Grabarnik  Rish,  Adaptive diagnosis in distributed systems"}},{"authors":[{"name":"T. Jaakkol"},{"name":"M. Jordan"}],"title":{"text":"Variational probabilistic inference and the qmr-dt database"}},{"authors":[{"name":"A. Kraus"},{"name":"C. Guestrin"}],"title":{"text":"Near-optimal Nonmyopic Value of Information in Graphical Models"}},{"authors":[{"name":"J. Pearl"}],"title":{"text":"Probabilistic reasoning in intelligent systems: net- works of plausible inference"}},{"authors":[{"name":"I. Rish"}],"title":{"text":"Distributed Systems Diagnosis Using Belief Propaga- tion"}},{"authors":[{"name":"I. Ris"},{"name":"M. Brodi"},{"name":"S. Ma"}],"title":{"text":"Accuracy vs"}},{"authors":[{"name":"I. Ris"},{"name":"M. Brodi"},{"name":"N. Odintsov"},{"name":"S. M"},{"name":"G. Grabarnik"}],"title":{"text":"Real-time Problem Determination in Distributed Systems us- ing Active Probing"}},{"authors":[{"name":"A. Zhen"},{"name":"I. Ris"},{"name":"A. Beygelzimer"}],"title":{"text":"Efﬁcient Test Se- lection in Active Diagnosis via Entropy Approximation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/ita2013/326"},"links":[{"id":"205","weight":2},{"id":"2760","weight":5},{"id":"2741","weight":32},{"id":"3180","weight":4},{"id":"67","weight":2},{"id":"2206","weight":26},{"id":"1606","weight":4},{"id":"3037","weight":6},{"id":"436","weight":5},{"id":"3191","weight":11},{"id":"223","weight":2},{"id":"1547","weight":3},{"id":"1278","weight":2},{"id":"2287","weight":2},{"id":"3200","weight":19},{"id":"3008","weight":2},{"id":"3083","weight":6},{"id":"2217","weight":6},{"id":"3222","weight":2},{"id":"307","weight":5},{"id":"1430","weight":3},{"id":"3021","weight":5},{"id":"170","weight":2},{"id":"813","weight":3},{"id":"2634","weight":3},{"id":"2442","weight":3},{"id":"3183","weight":9},{"id":"2763","weight":3},{"id":"2534","weight":5},{"id":"1235","weight":2},{"id":"135","weight":2},{"id":"404","weight":7},{"id":"702","weight":5},{"id":"770","weight":10},{"id":"3165","weight":7},{"id":"1082","weight":2},{"id":"1889","weight":3},{"id":"762","weight":2},{"id":"3154","weight":2},{"id":"3065","weight":2},{"id":"3195","weight":5},{"id":"284","weight":8},{"id":"1060","weight":2},{"id":"3132","weight":6},{"id":"1160","weight":6},{"id":"3187","weight":11},{"id":"944","weight":2},{"id":"3137","weight":5},{"id":"3059","weight":5},{"id":"2646","weight":3},{"id":"90","weight":2},{"id":"236","weight":3},{"id":"273","weight":14},{"id":"2330","weight":2},{"id":"1442","weight":3},{"id":"2276","weight":2},{"id":"2352","weight":5},{"id":"3255","weight":2},{"id":"1643","weight":8},{"id":"250","weight":2},{"id":"3036","weight":2},{"id":"26","weight":2},{"id":"1298","weight":15},{"id":"1268","weight":2},{"id":"2830","weight":3},{"id":"679","weight":9},{"id":"123","weight":14},{"id":"3049","weight":10},{"id":"1479","weight":4},{"id":"1215","weight":7},{"id":"107","weight":3},{"id":"1462","weight":11},{"id":"13","weight":9},{"id":"3038","weight":7},{"id":"1996","weight":6},{"id":"325","weight":3},{"id":"1458","weight":2},{"id":"2025","weight":2},{"id":"1279","weight":9},{"id":"888","weight":2},{"id":"459","weight":3},{"id":"1877","weight":9},{"id":"83","weight":2},{"id":"306","weight":3},{"id":"1121","weight":3},{"id":"3057","weight":8},{"id":"3068","weight":8},{"id":"3164","weight":6},{"id":"1107","weight":4},{"id":"398","weight":3},{"id":"152","weight":5},{"id":"2750","weight":15},{"id":"1421","weight":5},{"id":"3105","weight":2},{"id":"1866","weight":3},{"id":"3233","weight":5},{"id":"2488","weight":3},{"id":"3116","weight":3},{"id":"900","weight":4},{"id":"1846","weight":4},{"id":"1074","weight":3},{"id":"3168","weight":5},{"id":"263","weight":2},{"id":"1136","weight":4},{"id":"2751","weight":2},{"id":"1103","weight":3},{"id":"3157","weight":2},{"id":"2317","weight":15},{"id":"3127","weight":2},{"id":"419","weight":15},{"id":"372","weight":8},{"id":"138","weight":5},{"id":"54","weight":4},{"id":"1696","weight":5},{"id":"1212","weight":5},{"id":"487","weight":2},{"id":"508","weight":2},{"id":"65","weight":2},{"id":"1096","weight":3},{"id":"71","weight":6},{"id":"642","weight":18},{"id":"3196","weight":4},{"id":"2033","weight":4},{"id":"1830","weight":3},{"id":"108","weight":2},{"id":"202","weight":5},{"id":"691","weight":3},{"id":"2740","weight":2},{"id":"1443","weight":5},{"id":"2812","weight":2},{"id":"3243","weight":2},{"id":"3134","weight":5},{"id":"1365","weight":2},{"id":"445","weight":3},{"id":"3185","weight":2},{"id":"1671","weight":2},{"id":"1325","weight":28},{"id":"1844","weight":4},{"id":"219","weight":2},{"id":"2174","weight":7},{"id":"3001","weight":3},{"id":"2759","weight":6},{"id":"431","weight":4},{"id":"82","weight":6},{"id":"1376","weight":7},{"id":"1448","weight":18},{"id":"3056","weight":5},{"id":"1","weight":4},{"id":"2996","weight":8},{"id":"2617","weight":2},{"id":"2435","weight":3},{"id":"3174","weight":5},{"id":"3051","weight":2},{"id":"1840","weight":7},{"id":"1908","weight":5},{"id":"771","weight":3},{"id":"3088","weight":8},{"id":"221","weight":2},{"id":"25","weight":2},{"id":"658","weight":3},{"id":"2743","weight":3},{"id":"230","weight":2},{"id":"309","weight":2},{"id":"2773","weight":4},{"id":"31","weight":2},{"id":"3071","weight":3},{"id":"503","weight":5},{"id":"3163","weight":3},{"id":"278","weight":10},{"id":"3113","weight":3},{"id":"1444","weight":8},{"id":"151","weight":2},{"id":"887","weight":4},{"id":"95","weight":5},{"id":"1109","weight":2},{"id":"1438","weight":2},{"id":"184","weight":10},{"id":"264","weight":2},{"id":"257","weight":10},{"id":"3030","weight":3},{"id":"694","weight":6},{"id":"1573","weight":6},{"id":"525","weight":2},{"id":"1847","weight":3},{"id":"3205","weight":34},{"id":"75","weight":2},{"id":"1509","weight":3},{"id":"3108","weight":4},{"id":"3167","weight":4},{"id":"275","weight":3},{"id":"376","weight":7},{"id":"723","weight":5},{"id":"308","weight":2},{"id":"2838","weight":2},{"id":"293","weight":3},{"id":"630","weight":2},{"id":"767","weight":8},{"id":"354","weight":7},{"id":"2939","weight":3},{"id":"1076","weight":2},{"id":"430","weight":2},{"id":"1915","weight":14},{"id":"1233","weight":2},{"id":"641","weight":4},{"id":"1905","weight":5},{"id":"3189","weight":7},{"id":"575","weight":3},{"id":"831","weight":2},{"id":"2862","weight":4},{"id":"402","weight":2},{"id":"371","weight":11},{"id":"1084","weight":4},{"id":"2729","weight":2},{"id":"423","weight":2},{"id":"2316","weight":2},{"id":"3156","weight":2},{"id":"1883","weight":3},{"id":"3074","weight":4},{"id":"1102","weight":8},{"id":"2173","weight":2},{"id":"2753","weight":3},{"id":"1517","weight":9},{"id":"1319","weight":2},{"id":"699","weight":4},{"id":"506","weight":3},{"id":"187","weight":9},{"id":"1528","weight":19},{"id":"621","weight":17},{"id":"3070","weight":3},{"id":"1503","weight":4},{"id":"261","weight":3},{"id":"438","weight":2},{"id":"3181","weight":2},{"id":"1546","weight":11},{"id":"3198","weight":2},{"id":"2778","weight":7},{"id":"1098","weight":3},{"id":"1861","weight":2},{"id":"183","weight":5},{"id":"688","weight":3},{"id":"3129","weight":2},{"id":"3169","weight":3},{"id":"982","weight":2},{"id":"359","weight":5},{"id":"1930","weight":3},{"id":"3133","weight":2},{"id":"1553","weight":4},{"id":"3170","weight":3},{"id":"485","weight":13},{"id":"2738","weight":3},{"id":"240","weight":7},{"id":"1166","weight":10},{"id":"1550","weight":25},{"id":"211","weight":2},{"id":"782","weight":2},{"id":"3125","weight":2},{"id":"2495","weight":5},{"id":"420","weight":3},{"id":"323","weight":3},{"id":"3136","weight":2},{"id":"74","weight":3},{"id":"1456","weight":3},{"id":"243","weight":5},{"id":"1231","weight":9},{"id":"1214","weight":2},{"id":"1083","weight":5},{"id":"3114","weight":3},{"id":"1434","weight":5},{"id":"1871","weight":2},{"id":"3188","weight":3},{"id":"3177","weight":10},{"id":"643","weight":2},{"id":"3166","weight":7},{"id":"1187","weight":3},{"id":"449","weight":13},{"id":"377","weight":5},{"id":"424","weight":2},{"id":"3029","weight":2},{"id":"3151","weight":2},{"id":"2146","weight":3},{"id":"2638","weight":4},{"id":"2315","weight":2},{"id":"1423","weight":2},{"id":"1680","weight":3},{"id":"3208","weight":8},{"id":"370","weight":7},{"id":"1402","weight":7},{"id":"2299","weight":5},{"id":"2309","weight":7},{"id":"1258","weight":11},{"id":"2288","weight":2},{"id":"2286","weight":10},{"id":"2038","weight":6},{"id":"752","weight":2},{"id":"1393","weight":2},{"id":"3162","weight":2}],"meta":{"jsonClass":"Map$Map3","room":"The Tookan","date":"1360517400000","session":"3"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
