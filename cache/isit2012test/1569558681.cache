{"id":"1569558681","paper":{"title":{"text":"Universal Estimation of Directed Information via Sequential Probability Assignments"},"authors":[{"name":"Jiantao Jiao"},{"name":"Haim H. Permuter"},{"name":"Lei Zhao"},{"name":"Young-Han Kim"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014We propose four approaches to estimating the di- rected information rate between a pair of jointly stationary ergodic processes with the help of universal probability assign- ments. The four approaches yield estimators with different merits such as nonnegativity and boundedness. We establish consistency of these estimators in various senses and derive near-optimal rates of convergence in the minimax sense under mild conditions. The estimators carry over directly to estimating other information measures of stationary ergodic processes, such as entropy rate and mutual information rate, and provide alternatives to classical approaches in the existing literature. Guided by the theoretical results, we use context tree weighting as the vehicle for the implementations of the proposed estimators. Experiments on synthetic and real data are presented, demonstrating the potential of the proposed schemes in practice and the efﬁcacy of directed information estimation as a tool for detecting and measuring causality and delay.\nIndex Terms\u2014Causal inﬂuence, context tree weighting, di- rected information, rate of convergence, universal probability assignment"},"body":{"text":"First introduced by Marko [1] and Massey [2], directed in- formation arises as a natural counterpart of mutual information for channel capacity when causal feedback from the receiver to the sender is present. In [3] and [4], Kramer extended the use of directed information to discrete memoryless networks with feedback, including the two-way channel and the mul- tiple access channel. Tatikonda and Mitter [5] used directed information spectrum to establish a general feedback channel coding theorem for channels with memory. Kim [6] established the feedback capacity for a class of stationary channels using directed information. In [7], Permuter, Weissman, and Gold- smith considered the capacity of discrete-time channels with feedback where the feedback is a time-invariant deterministic function of the output, and used directed information to de- scribe the capacity under mild conditions. Recently, Permuter, Kim, and Weissman [8] showed that directed information plays an important role in portfolio theory, data compression, and hypothesis testing, in the presence of causality constraints.\nBeyond information theory, directed information is a valu- able tool in biology, for it provides an alternative to identify causal inferences between two processes. In Mathai, Martins, and Shapiro [9], directed information was used to identify pairwise inﬂuence. Rao, Hero, States, and Engel [10] used\ndirected information to test the direction of inﬂuence in gene networks.\nSince directed information has signiﬁcance in various ﬁelds, it is of both theoretical and practical importance to develop efﬁcient ways for estimating it. The problem of estimating information measures, such as entropy, relative entropy and mutual information, has been extensively studied in the liter- ature. Verd´u [11] gave an overview of universal estimation of information measures. Wyner and Ziv [12] applied the idea of Lempel\u2013Ziv parsing to estimate the entropy rate, which converges in probability for all stationary ergodic processes. Ziv and Merhav [13] used Lempel\u2013Ziv parsing to estimate relative entropy (Kullback\u2013Leibler divergence) and established consistency under the assumption that the observations are generated by independent Markov sources. Cai, Kulkarni, and Verd´u [14] proposed two universal divergence estimators for ﬁnite-alphabet sources, one based on the Burrows\u2013Wheeler transform (BWT) [15] and the other based on the context tree weighting method (CTW) [16]. The BWT-based estimator was applied in universal entropy estimation in Cai, Kulkarni, and Verd´u [17], while the CTW-based one was applied in universal erasure entropy estimation in Yu and Verd´u [18].\nFor the problem of estimating directed information, Quinn, Coleman, Kiyavashi, and Hatspoulous [19] developed an estimator to infer causality in ensemble neural spike train recordings. Based on parametric generalized linear model (GLM) assumption and stationary ergodic Markov assumption [19], they showed strong consistency results. Compared to [19], Zhao, Kim, Permuter, and Weissman [20] focused on universal methods and showed L 1 consistency for all jointly stationary ergodic process pairs with ﬁnite alphabet.\nAs an improvement and further development of [20], and a reﬂection of Jiao, Permuter, Zhao, Kim, and Weissman [21], the main contribution of this paper is a general framework for estimating information measures of stationary ergodic pro- cesses, using \u201csingle-letter\u201d information-theoretic functionals. Although our methods can be applied in estimating a number of information measures, we focus\u2014for concreteness and relevance to emerging applications\u2014on estimating the directed information rate between a pair of jointly stationary ergodic processes. The ﬁrst proposed estimator is adapted from the universal divergence estimator in [14] using the CTW method, and we give a reﬁned analysis yielding strong consistency\nresults. We further propose three additional estimators in a uniﬁed framework to estimate the directed information rate, present both weak and strong consistency results, and establish near-optimal rates of convergence under mild conditions. We then employ our estimators on both simulated and real data, showing their effectiveness in measuring channel delays and causal inﬂuences between processes. In particular, we use these estimators to establish signiﬁcant causal inﬂuence from the Dow Jones Industrial Average to the Hang Seng Index, but relatively low causal inﬂuence in the reverse direction, based on the daily market data in the period from 1990 to 2011.\nThe rest of the paper is organized as follows. Section II reviews some preliminaries and Section III presents our proposed estimators and some of their basic properties. Section IV is dedicated to performance guarantees for the proposed estimators, rates of convergence results under mild conditions, and minimax optimality. Section V shows experimental results applying the proposed estimators, both on simulated and real data, and demonstrates the effectiveness of these estimators in inferring delay of channels and causal inﬂuences between processes. For proofs of stated results please see [21].\nWe begin with deﬁnitions of directed information, universal and pointwise universal probability assignments. We then introduce the context tree weighting (CTW) method used in our implementations.\nWe use uppercase letters X, Y, . . . to denote random vari- ables, and lowercase letters x, y, . . . to denote values they assume. We denote the n-tuple (X 1 , X 2 , . . . , X n ) as X n and (x 1 , x 2 , . . . , x n ) as x n . Calligraphic letters X , Y, . . . denote alphabets of X, Y, . . ., and |X | denotes the cardinality of X . Given a probability law P , P (x i |X i−1 ) denotes the condi- tional pmf P (x i |x i−1 ) evaluated for the random sequence X i−1 , while P (X i |X i−1 ) is the random variable denoting the X i th component of P (x i |X i−1 ). Throughout this paper, log(·) means log 2 (·).\n(2) where H(Y n X n ) is the causally conditional entropy [3], concretely,\ndirected information has the causally conditional entropy in place of the conditional entropy. Unlike mutual information, directed information is not symmetric, i.e., I(Y n → X n ) = I(X n → Y n ) in general.\ndenotes the reverse directed information. Other interesting properties of directed information can be found in [3], [22].\nThe directed information rate [3] between a pair of jointly stationary random processes X and Y is deﬁned as\nwhere H(Y) = H(Y 0 |Y − 1 −∞ ) is the entropy rate of process Y, H(Y X) is the causally conditional entropy rate deﬁned as H(Y X) \t lim n→∞ (1/n)H(Y n X n ) =\nEquation (9) shows that we can estimate H(Y) and H(Y X) separately to estimate the directed information rate.\nA probability assignment Q consists of a set of conditional pmfs Q(x i |x i−1 ) for every x i−1 ∈ X i−1 . Note that Q induces a probability measure on a random process X.\nDeﬁnition 1 (Universal probability assignment): A proba- bility assignment Q is said to be universal for a class P if the normalized Kullback\u2013Leibler divergence satisﬁes\nfor every probability measure P in P. A probability assign- ment Q is said to be universal (without a qualiﬁer) if it is universal for the class of stationary probability measures.\nA probability assignment Q is said to be pointwise universal for a class P if\n(11) for every probability measure P in P. A probability assign- ment Q is said to be pointwise universal (without a qualiﬁer) if it is pointwise universal for the class of stationary ergodic probability measures.\nIt is well known that there exist universal and pointwise universal probability assignments, see, for example, [23].\nOne particularly celebrated universal probability assignment is the context tree weighting (CTW) algorithm by Willems, Shtarkov, and Tjalken [16]. The computational complexity of the CTW is linear in the block length n, and the algorithm provides the probability assignments Q directly, which is the weighted probability at the root node. For details, see [16], [24] and [21]. Note that here we use the extended version of CTW for non-binary alphabets, which is discussed in [25].\nThe probability assignment Q in CTW is both universal and pointwise universal for the class of stationary ergodic Markov processes. For the proof of universality, see, [16], and for the pointwise universality, please see [21].\nIn this section, we introduce four algorithms to estimate the directed information rate I(X → Y) of a pair of jointly stationary ergodic processes X and Y. Let M(X , Y) be the set of all probability distributions on X × Y. Deﬁne f as the function that maps a joint pmf P (x, y) of a random pair (X, Y ) to the corresponding conditional entropy H(Y |X), i.e.,\nwhere P (y|x) is the conditional pmf induced by P (x, y). Take Q as a universal probability assignment.\nˆ I 1 (X n → Y n ) ˆ H 1 (Y n ) − ˆ H 1 (Y n X n ), \t (13) ˆ I 2 (X n → Y n ) ˆ H 2 (Y n ) − ˆ H 2 (Y n X n ), \t (14)\n(15) ˆ I 4 (X n → Y n )\n1 n\n(16) where\nand ˆ H 1 (Y n ) = ˆ H 1 (Y n ∅), ˆ H 2 (Y n ) = ˆ H 2 (Y n ∅). Note that an entropy estimate such as ˆ H 1 (Y n X n ) is a random variable (since it is a function of (X n , Y n )), as opposed to entropy terms such as H(Y n X n ), which are deterministic and depend on the distribution of (X n , Y n ).\nNote that the universal probability assignments conditioned on different data are calculated separately. For example, Q(y i |Y i−1 ) is not computed from Q(x i , y i |X i−1 , Y i−1 ), but by running the universal probability assignment algorithm\nagain on dataset Y i−1 . Of course, Q(y i |x i , X i−1 , Y i−1 ) is computed from Q(x i , y i |X i−1 , Y i−1 ).\nThe estimator ˆ I 1 is adapted from one universal divergence estimator in [14]. One disadvantage of ˆ I 1 (X n → Y n ) is that it has a nonzero probability of being very large, which is overcome by ˆ I 2 , the estimator introduced in [20], by using information-theoretic functionals to \u201csmooth\u201d the estimate. Evidently we can show | ˆ I 2 | ≤ log |Y|.\nThe common disadvantage of ˆ I 1 and ˆ I 2 is that they are computed by subtraction of two quantities, and have a nonzero probability of being negative. ˆ I 3 and ˆ I 4 are introduced to overcome this, since they take the form of a Kullback\u2013Leibler divergence and are always nonnegative.\nIn this section, we present consistency results for the proposed estimators. Under some mild conditions, we derive near-optimal rates of convergence in the minimax sense. For proofs, please see [21].\nTheorem 1: Let Q be a universal probability assignment and (X, Y) be jointly stationary ergodic. Then\nFurthermore, if Q is also a pointwise universal probability assignment, then the limit in (19) holds almost surely as well.\nIf (X, Y) is a stationary ergodic aperiodic Markov process, we can say more about the performance of ˆ I 1 using the probability assignment in CTW method.\nProposition 1: Let Q be the probability assignment in CTW. If (X, Y) is a jointly stationary ergodic aperiodic Markov process whose order does not exceed the prescribed maximum depth in CTW, then there exists a constant C 1 such that\nˆ I 1 (X n → Y n ) − I(X → Y) = o(n − 1/2 (log n) 5/2+ ) P -a.s. (21)\nWe can establish similar consistency results for the second estimator ˆ I 2 in (14).\nTheorem 2: Let Q be a universal probability assignment, and (X, Y) be jointly stationary ergodic. Then\nAs was the case for ˆ I 1 , if the process (X, Y) is a jointly stationary ergodic aperiodic Markov process, we can say more about the performance of ˆ I 2 as follows:\nProposition 2: Let Q be the probability assignment in CTW. If (X, Y) is a jointly stationary ergodic Markov process whose order does not exceed the prescribed maximum depth in CTW, then\nFurthermore, if (X, Y) is also aperiodic, there exists a con- stant C 2 such that\n(24) The rates of convergence for the ﬁrst two estimators are\nProposition 3: Let P(X, Y) be any class of processes that includes the class of i.i.d. processes. Then, there exists a positive constant C 3 such that\nwhere the inﬁmum is over all estimators ˆ I of the directed information rate based on (X n , Y n ).\nEvidently, convergence rate better than O(n − 1/2 ) is not attain- able even with respect to the class of i.i.d. sources and thus, a fortiori, in our setting of a much larger uncertainty set.\nFor the third and fourth estimators, we establish the follow- ing results.\nTheorem 3: Let Q be the probability assignment in CTW. If (X, Y) is a stationary ergodic Markov process whose order does not exceed the prescribed maximum depth in CTW, then\nTheorem 4: Let Q be the probability assignment in CTW. If (X, Y) is a stationary ergodic Markov process whose order does not exceed the prescribed maximum depth in CTW, then\nIn this section, we show how one can use the directed information estimator to detect delay of a channel, and to measure the \u201ccausal inﬂuence\u201d of one sequence on another. We generate simulated data to detect the channel delay and use real stock market data to detect and measure the causal inﬂuence that exists between the Chinese and the US stock markets.\nA. Channel Delay Estimation via Shifted Directed Information Assume a setting depicted as follows: in Fig. 1.\nOur goal is to ﬁnd the delay D. We use the shifted directed information I(Y n+d → X n ) to estimate D, where I(Y n+d → X n ) is deﬁned as\nTo illustrate the idea, suppose that the binary processes X and Y are related as\nwhere W i ∼ Bernouli( ) and addition in (29) is modulo 2. Note that the mutual information rate lim 1 n I(Y n ; X n ) is not inﬂuenced by D. However, the shifted directed in- formation rate lim 1 n I(Y n+d → X n ) is highly inﬂuenced by D. Assuming that there is no feedback, for d < D we have the Markov chain Y i+d → X i−1 → X i due to (29), and therefore I(Y n+d → X n ) = 0. However, for d ≥ D, I(Y n+d → X n ) > 0. For instance, in the channel example (29), if W i = 0 almost surely, then for d ≥ D, I(Y n+d → X n ) = H(X n ). Therefore, we can use the shifted directed information I(Y n+d → X n ) to estimate D.\nFor the sake of simplicity, we only show the estimation results using I 2 , other estimators have similar outputs. Fig. 2 depicts ˆ I 2 (Y n+d → X n ) where n = 10 6 for the setting in Fig. 1, where the input is a binary stationary Markov process of order one and the channel is given by (29). The delay of the channel is D = 2. One can note clearly that for d < D, ˆ I 2 (Y n+d → X n ) is very close to zero and for d ≥ D, ˆ I 2 (Y n+d → X n ) is signiﬁcantly larger than zero.\nThere is extensive literature on detecting and measuring causal inﬂuence. See, for example, [26] for a recent survey of some of the common tools and approaches in biomedical informatics. One particularly celebrated tool - in both the life and economics sciences - for assessing whether and to what extent one time series inﬂuences another is the Granger causal- ity test [27]. It is a simple exercise to verify that under jointly Gauss-Markov assumptions, the Granger causality coincides with the directed information (up to a multiplicative constant).\nAssuming for every pair (X i , Y i ), X i happens earlier than Y i . It can be easily veriﬁed that I(X n → Y n ) = 0 if and only if P (y i |x i , y i−1 ) = P (y i |y i−1 ) for i ≥ 1, and I(Y n−1 →\nX n ) = 0 if and only if P (x i |x i−1 , y i−1 ) = P (x i |x i−1 ) for i ≥ 1. More generally, the directed information I(X n → Y n ) quantiﬁes how much X causally inﬂuences Y, while the directed information in the reverse direction I(Y n−1 → X n ) quantiﬁes how much Y inﬂuences X.\nTo illustrate this idea, we compute the directed information rate between the Hang Seng Index (HSI) and the Dow Jones Index (DJIA) using data from 1990 and 2011 on a daily scale. Since everyday the HSI changes before DJIA, HSI should play the role of process X in the estimation. We discretize the value of stock market into three values: −1, 1, and 0, by going down by more than 0.8%, going up by more than 0.8%, and changes between them, respectively.\nWe denote by X i and Y i the (quantized ternary valued) change in the HSI and the DJIA in day i, respectively, and es- timate 1 n I(X n ; Y n ), 1 n I(X n → Y n ), and 1 n I(Y n−1 → X n ), using all four algorithms. Fig. 3 plots our estimates of these information-theoretic measures.\nEvidently, the reverse directed information is much higher than the directed information; hence we can say that between 1990 and 2011, it was the Chinese market that was inﬂuenced more by the US market rather than the other way around."},"refs":[{"authors":[{"name":"H. Marko"}],"title":{"text":"The bidirectional communication theory\u2013a generalization of information theory"}},{"authors":[{"name":"J. L. Massey"}],"title":{"text":"Causality, feedback, and directed information"}},{"authors":[{"name":"G. Krame"}],"title":{"text":"Directed Information for Channels with Feedback"}},{"authors":[],"title":{"text":"Capacity results for the discrete memoryless network"}},{"authors":[{"name":"S. Tatikonda"},{"name":"S. Mitter"}],"title":{"text":"The capacity of channels with feedback"}},{"authors":[{"name":"Y.-H. Kim"}],"title":{"text":"A coding theorem for a class of stationary channels with feedback"}},{"authors":[{"name":"H. H. Permuter"},{"name":"T. Weissman"},{"name":"A. J. Goldsmith"}],"title":{"text":"Finite state channels with time-invariant deterministic feedback"}},{"authors":[{"name":"H. H. Permuter"},{"name":"Y.-H. Kim"},{"name":"T. Weissman"}],"title":{"text":"Interpretations of di- rected information in portfolio theory, data compression, and hypothesis testing"}},{"authors":[{"name":"P. Mathai"},{"name":"N. C. Martins"},{"name":"B. Shapiro"}],"title":{"text":"On the detection of gene network interconnections using directed mutual information"}},{"authors":[{"name":"A. Rao"},{"name":"A. O. Hero"},{"name":"D. J. States"},{"name":"J. D. Engel"}],"title":{"text":"Using directed information to build biologically relevant inﬂuence networks"}},{"authors":[{"name":"S. Verd´u"}],"title":{"text":"Universal estimation of information measures"}},{"authors":[{"name":"A. D. Wyner"},{"name":"J. Ziv"}],"title":{"text":"Some asymptotic properties of the entropy of a stationary ergodic data source with applications to data compression"}},{"authors":[{"name":"J. Ziv"},{"name":"N. Merhav"}],"title":{"text":"A measure of relative entropy between individual sequences with application to universal classiﬁcation"}},{"authors":[{"name":"H. Cai"},{"name":"S. R. Kulkarni"},{"name":"S. Verd´u"}],"title":{"text":"Universal divergence estimation for ﬁnite-alphabet sources"}},{"authors":[{"name":"M. Burrow"},{"name":"D. J. Wheele"}],"title":{"text":"A block-sorting lossless data compres- sion algorithm "}},{"authors":[{"name":"F. M. J. Willems"},{"name":"Y. M. Shtarkov"},{"name":"T. J. Tjalkens"}],"title":{"text":"The context-tree weighting method: Basic properties"}},{"authors":[{"name":"H. Cai"},{"name":"S. R. Kulkarni"},{"name":"S. Verd´u"}],"title":{"text":"Universal entropy estimation via block sorting"}},{"authors":[{"name":"J. Yu"},{"name":"S. Verd´u"}],"title":{"text":"Universal erasure entropy estimation"}},{"authors":[{"name":"C. J. Quinn"},{"name":"T. P. Coleman"},{"name":"N. Kiyavash"},{"name":"N. G. Hatsopoulos"}],"title":{"text":"Estimating the directed information to infer causal relationships in ensemble neural spike train recordings"}},{"authors":[{"name":"L. Zhao"},{"name":"Y.-H. Kim"},{"name":"H. H. Permuter"},{"name":"T. Weissman"}],"title":{"text":"Universal estimation of directed information"}},{"authors":[{"name":"J. Jiao"},{"name":"H. H. Permuter"},{"name":"L. Zhao"},{"name":"Y.-H. Kim"},{"name":"T. Weissman"}],"title":{"text":"Universal estimation of directed information"}},{"authors":[{"name":"J. L. Massey"},{"name":"P. C. Massey"}],"title":{"text":"Conservation of mutual and directed information"}},{"authors":[{"name":"N. Merhav"},{"name":"M. Feder"}],"title":{"text":"Universal prediction"}},{"authors":[{"name":"F. Willem"},{"name":"T. Tjalken"}],"title":{"text":"Complexity Reduction of the Context-Tree Weighting Algorithm: A Study for KPN Research "}},{"authors":[{"name":"T. J. Tjalkens"},{"name":"Y. M. Shtarkov"},{"name":"F. M. J. Willems"}],"title":{"text":"Sequential weighting algorithms for multi-alphabet sources"}},{"authors":[{"name":"S. Kleinberg"},{"name":"G. Hripcsak"}],"title":{"text":"A review of causal inference for biomedical informatics"}},{"authors":[{"name":"C. Granger"}],"title":{"text":"Investigating causal relations by econometric models and cross-spectral methods"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569558681.pdf"},"links":[{"id":"1569559259","weight":15},{"id":"1569559541","weight":22},{"id":"1569559221","weight":25},{"id":"1569558785","weight":10},{"id":"1569559565","weight":9},{"id":"1569559195","weight":4},{"id":"1569558859","weight":9},{"id":"1569566489","weight":20},{"id":"1569558901","weight":10},{"id":"1569559111","weight":6},{"id":"1569558985","weight":57},{"id":"1569558509","weight":13},{"id":"1569565705","weight":3},{"id":"1569551347","weight":5},{"id":"1569559199","weight":19},{"id":"1569559035","weight":23},{"id":"1569558779","weight":9},{"id":"1569559597","weight":35},{"id":"1569559251","weight":22},{"id":"1569550425","weight":30},{"id":"1569564509","weight":12},{"id":"1569558697","weight":8},{"id":"1569559233","weight":17}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T8.2","endtime":"15:20","authors":"Jiantao Jiao, Haim H Permuter, Lei Zhao, Young-Han Kim, Tsachy Weissman","date":"1341241200000","papertitle":"Universal Estimation of Directed Information via Sequential Probability Assignments","starttime":"15:00","session":"S3.T8: Directed Information, Common Information, and Divergence","room":"Stratton (491)","paperid":"1569558681"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
