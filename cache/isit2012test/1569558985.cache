{"id":"1569558985","paper":{"title":{"text":"An Information Theoretic Perspective Over an Extremal Entropy Inequality"},"authors":[{"name":"Sangwoo Park"},{"name":"Erchin Serpedin"},{"name":"Khalid Qaraqe"}],"abstr":{"text":"Abstract\u2014This paper focuses on developing an alternative proof for an extremal entropy inequality, originally presented in [1]. The proposed alternative proof is simply based on the classi- cal entropy power inequality and the data processing inequality. Compared with the proofs in [1], the proposed alternative proof is simpler, more direct, and information theoretic, and presents the advantage of providing the structure of the optimal solution covariance matrix. Also, the proposed proof might also be used as a novel method to address applications such as calculation of the vector Gaussian broadcast channel capacity, establishing a lower bound for the achievable rate of distributed source coding with a single quadratic distortion constraint, and the secrecy capacity of the Gaussian wire-tap channel."},"body":{"text":"An extremal entropy inequality (EEI), involving the dif- ference of two random vector differential entropies subject to certain constraints and motivated by multi-terminal in- formation theoretic problems such as the vector Gaussian broadcast channel and the distributed source coding with a single quadratic distortion constraint, was established recently by Liu and Viswanath in [1]. This extremal entropy inequality was ignited by the question: \u201cWhat is the optimal solution for the classical entropy power inequality (EPI) under a covariance matrix constraint?\u201d Even though it is assumed that the expected solution is a Gaussian random vector, the solution cannot be obtained by using the classical EPI due to the covariance matrix constraint. Therefore, a novel method, referred to as the channel enhancement technique [2], was adopted in the proofs illustrated in [1].\nThe proposed proofs in [1] proceed as follows. First, EEI is recast as an optimization problem (P ). An auxiliary opti- mization problem ( ˜ P ) is considered for the original problem (P ) by means of the channel enhancement technique, which relies mainly on the Karush-Kuhn-Tucker (KKT) conditions. The maximum value of ( ˜ P ) is shown to be larger than the maximum value of (P ). Then, ( ˜ P ) is solved using the classical EPI. Finally, the proof is accomplished by showing that the maximum value of ( ˜ P ) is equal to the maximum value of (P ). Even though Liu and Viswanath proposed two distinctive proofs, a direct proof and a perturbation proof, both proofs are commonly based on the channel enhancement technique, and they are derived in a similar way.\nThe main theme of this paper is to prove the EEI without using the channel enhancement technique. Our proof is mainly based on four techniques: the data processing inequality,\nthe moment generating function (MGF), the worst additive noise lemma, and the classical EPI. By using data processing inequality, worst additive noise lemma, and classical EPI, we calculate an upper bound. Then, by applying the equality condition in data processing inequality, we prove that the upper bound can be achieved. MGFs are implemented to prove the achievement of the equality condition in data processing inequality.\nThe proposed proof in this paper connotes the following major contributions. First, our proof is simpler and more direct compared with the proofs in [1]. Second, we adapt a more information-theoretic approach without using the KKT conditions. The method based on data processing inequality and MGF enables us to circumvent the step of using the KKT conditions. Moreover, by simply analyzing some properties of positive semi-deﬁnite matrices, we can omit the step of proving the existence of the optimal solution which satisﬁes the KKT conditions, a step which is very complicated to accomplish. In addition, the optimal solution\u2019s covariance matrix structure is mentioned in detail by using properties of positive semi-deﬁnite matrices. Third, the proposed proof represents a novel investigation method not only for EEI but also for applications such as establishing the Gaussian broadcast channel capacity, secrecy capacity of Gaussian wire- tap channel, etc. These considerations support the versatility of EEI.\nThe rest of this paper is organized as follows. EEI without a covariance constraint is considered in Section II. EEI and the proposed new proof, which are the main results of this paper, are presented in Section III. Applications are brieﬂy mentioned in Section IV. Finally, Section V concludes the paper.\nSince EEI is similar to the classical EPI, we ﬁrst investigate a relationship between EEI and EPI. Without a covariance constraint, EEI is equivalent to EPI as shown in Theorem 1.\nTheorem 1: For an arbitrary random vector X with a co- variance matrix Σ X and a Gaussian random vector W G with a covariance matrix Σ W , there exist Gaussian random vectors\nh(X) − µh(X + W G ) ≤ h( ˜ X G ) − µh( ˜ X G + W G ), (1) ≤ h(X ∗ G ) − µh(X ∗ G + W G ), (2)\nwhere the constant µ ≥ 1, h(·) stands for differential entropy, all random vectors are independent of one another, Σ W is a positive deﬁnite matrix, and ˜ X G and X ∗ G are Gaussian random vectors satisfying the following conditions:\ni) Covariance matrices of ˜ X G and X ∗ G are represented by Σ ˜ X and Σ X ∗ , respectively.\nii) Σ ˜ X is proportional to Σ W , and Σ X ∗ = (µ − 1) −1 Σ W . iii) Differential entropy h( ˜ X G ) is the same as h(X).\nProof: Inequality (1) can be proved using EPI, and (2) can be shown exploiting concavity. The details of the proof are delegated to the journal paper version of this work [6].\nAs stated in Theorem 1, for µ ≥ 1, h(X) − µh(X + W G ) is maximized when random vector X is Gaussian. However, when a covariance constraint is added to (1) and (2), we cannot establish whether a Gaussian random vector still maximizes h(X) − µh(X + W G ) or not, using the techniques previously employed in the proof of Theorem 1. This is due to the fact that the covariance constraint may impede the proportionality relationship between the covariance matrices Σ X ∗ and Σ W .\nIn [1], it is shown that a Gaussian random vector still maximizes h(X) − µh(X + W G ) even when a covariance constraint is considered. Inequality (2) can be reformulated as an optimization problem with a covariance constraint as follows:\nwhere W G and V G are independent Gaussian random vectors with positive deﬁnite covariance matrices Σ W and Σ V , re- spectively. The notation ≼ stands for positive (semi)deﬁnite partial ordering between positive deﬁnite matrices. Also, all random vectors in (3) are assumed independent of one another, and the maximization is conducted over the distribution of ran- dom vector X. Two proofs, a direct proof and a perturbation proof, are provided in [1]. Each proof approaches the problem in a different way but both proofs share an important key com- mon component, namely the channel enhancement technique based on the KKT conditions and proposed originally in [2].\nUnlike the proofs in [1], we will solve (3) without using the channel enhancement technique. Before dealing with the general problem (3), we ﬁrst consider a simpler case of it in the form of Theorem 2.\nTheorem 2: Assume that µ is an arbitrary but ﬁxed con- stant, where µ ≥ 1, and R is a positive semi-deﬁnite matrix. A Gaussian random vector W G is assumed to have a covariance matrix Σ W , which is positive deﬁnite. Given an arbitrary random vector X, which is independent of W G , with a covariance matrix Σ X ≼ R, there exists a Gaussian random vector X ∗ G with a covariance matrix Σ X ∗ which satisﬁes the following inequality:\nProof: The following lemma, which is necessary in constructing the proof, will be ﬁrst presented.\nLemma 1 (Worst Additive Noise Lemma [1], [3], [4]): For independent random vectors Y 1 , Y G1 , and Y G2 ,\nwhere I( ·; ·) denotes mutual information, Y 1 is an arbitrary random vector, Y G1 and Y G2 are Gaussian random vectors with covariance matrices equal to Y 1 and covariance matrix Σ Y 2 , respectively. Also, all random vectors are independent of one another.\nWhen R is a positive deﬁnite but singular matrix, i.e., |R| = 0, using eigenvalue decomposition, the inequality (4) is equivalently changed into the inequality, which has a non- singular covariance matrix constraint, as mentioned in [1]. When µ = 1, the inequality (4) is easily proved by the Lemma 1. Therefore, without loss of generality, we assume that µ > 1 and R is a positive deﬁnite matrix [1], [6].\nh(X+W G ) −h(X+W G |W \u2032 G ) ≥h(X G +W G ) −h(X G +W G |W \u2032 G ) ⇔ h(X+W G ) ≥h(X+ ˜ W G )+h(X G +W G ) −h(X G + ˜ W G ), (5)\nwhere ⇔ denotes equivalence. Inequalities in (5) are due to Lemma 1 by replacing Y 1 , Y G1 , and Y G2 with X + ˜ W G , X G + ˜ W G , and W \u2032 G , respectively. Also, notice that the Gaussian random vector W G can be expressed as the sum of two independent Gaussian random vectors ˜ W G and W \u2032 G whose covariance matrices satisfy:\nwhere Σ W , Σ ˜ W , and Σ W \u2032 are the covariance matrices of W G , ˜ W\n, and W \u2032 G , respectively. Henceforth, the Gaussian random vector W G is represented as W G = ˜ W G + W \u2032 G .\nBased on (5) and (6), the left-hand side (LHS) of (4) is upper-bounded as follows:\nUsing Theorem 1, if (µ −1) −1 Σ ˜ W ≼ R, (7) is upper-bounded as\nwhere X ∗ G is a Gaussian random vector, whose covariance matrix Σ X ∗ is deﬁned as (µ −1) −1 Σ ˜ W . Due to the covariance constraint, unlike Theorem 1, we additionally have to prove that there exists a random vector X ∗ G whose covariance matrix Σ X ∗ satisﬁes\nSince Σ X ≼ R, instead of proving (9), we will show that there exists a random vector X ∗ G whose covariance matrix Σ X ∗ satisﬁes\nwhere all random vectors are independent of one another. Since a Gaussian random vector X G can be expressed as the sum of two independent Gaussian random vectors X \u2032 G and X ∗ G whose covariance matrices satisfy\nwhere Σ X , Σ X \u2032 , and Σ X ∗ stand for the covariance matrices of X G , X \u2032 G , and X ∗ G , respectively, the Gaussian random vector X G will be represented as X G = X \u2032 G + X ∗ G .\nBased on the data processing inequality and the Markov chain (11), we obtain\nAlthough we need an upper bound of (8), (12) provides a lower bound of (8) as follows:\nthe equality condition of the data processing inequality is satisﬁed. It turns out that\nNow, we will show that we can actually construct the Markov chain (13) using the following lemmas.\nLemma 2: For conditionally independent random vectors Y 1 and Y 2 given a random vector Y 3 , the following equality between the MGFs is satisﬁed:\nwhere M X |Y (S) = E X |Y [e X T S ], E X |Y [ ·] denotes the expecta- tion with respect to random vector X given Y , and superscript T denotes transposition. For jointly Gaussian random vectors Y 1 and Y 2 given Y 3 , this equality is a necessary and sufﬁcient condition for the conditional independence between Y 1 and Y 2 given Y 3 .\nLemma 3: For a Gaussian random vector Y G with a mean U Y and a covariance matrix Σ Y , MGF is expressed as\nIn the Markov chain (13), since all random vectors are Gaussian (without loss of generality, they are assumed to have zero means), using Lemma 3, the following MGFs are expressed in closed-form:\nW \u2032 G , and their covariance matrices are represented by Σ Y 1 , Σ Y 2 , and Σ Y 3 , respectively. Since Σ ˜ W + Σ W \u2032 is a positive deﬁnite matrix, there exists the inverse of Σ Y 3 . On the other hand, MGF of Y 1 + Y 2 given Y 3 is expressed as\nIf (A) in (15) vanishes, Y 1 and Y 2 are independent given Y 3 , and the Markov chain (13) is obtained. Using Lemma 11, (1) in [2], deﬁne the covariance matrix Σ ˜ W as\nwhere L ≽ 0, and 0 denotes an n-by-n zero matrix. The positive semi-deﬁnite matrix L must be chosen to satisfy\nLemma 4: There exists a positive semi-deﬁnite matrix L which satisﬁes:\n− 1) −1 Σ ˜ W , Σ X \u2032 = Σ X − Σ X ∗ , and Σ X and Σ W stand for a positive semi-deﬁnite and a positive deﬁnite matrix, respectively.\nProof: Proving Σ X ∗ ≼ Σ X is equivalent to proving the following inequalities:\nSince there always exists a non-singular matrix which simulta- neously diagonalizes two positive semi-deﬁnite matrices [7], there exists a non-singular matrix Q which simultaneously diagonalize both Σ X and Σ W :\nwhere I stands for the identity matrix, and D W is a diagonal matrix. If we deﬁne D L as a diagonal matrix whose i th diagonal entry is d L i ,\nEquation (21) always holds since D L and L are deﬁned as in (19) and (20) to satisfy (21). Therefore, the ﬁrst inequality in (17) is also satisﬁed.\nNotice that Σ X \u2032 =Σ X −Σ X ∗ . Since Σ X ∗ =(µ − 1) −1 Σ ˜ W , Σ X \u2032 is expressed as Σ X − (µ − 1) −1 Σ ˜ W , and\nEquality (22) is due to (18) and (20), and equality (23) is due to (19). Therefore, by deﬁning Σ ˜ W = ((Σ X +Σ W ) −1 +L) −1 −Σ X , we can make Σ ˜ W satisfy\nRemark 1: Since the optimization problem in [1] is gen- erally nonconvex, the existence of optimal solution must be proved [1], [2], and this step is very complicated. However, in our proof, Lemmas 4 and 5 serve as a substitute for this step since we by-pass the KKT-conditions related parts using the data processing inequality. This makes the proposed proof much simpler.\nSince LΣ X \u2032 = Σ X \u2032 L = 0, by multiplying with Σ X \u2032 both sides of (24),\n(Σ X + Σ ˜ W ) −1 Σ X \u2032 = (Σ X + Σ W ) −1 Σ X \u2032 , (25) Σ X \u2032 (Σ X + Σ ˜ W ) −1 = Σ X \u2032 (Σ X + Σ W ) −1 . (26)\nSince random vectors Y 1 , Y 2 , and Y 3 are deﬁned as Y 1 = X \u2032 G , Y 2 = X G + ˜ W G , and Y 3 = X G + W G , respectively, and they are independent of each other, their covariance matrices are represented as\nFrom (25) and (27), and (26) and (27), respectively, it turns out\nThe more general problem, originally proved in [1], is now considered in Theorem 3.\nTheorem 3: Assume that µ is an arbitrary but ﬁxed con- stant, where µ ≥ 1, and R is a positive semi-deﬁnite matrix. Gaussian random vectors W G and V G are assumed to be independent of each other and have positive deﬁnite covariance matrices Σ W and Σ V , respectively. Given an arbitrary random vector X, which is independent of W G and V G , with a covariance matrix Σ X ≼ R, there exists a Gaussian random vector X ∗ G with a covariance matrix Σ X ∗ which satisﬁes the following inequality:\nProof: Due to the same reason mentioned in the proof of Theorem 2, without loss of generality, we assume µ > 1 and R is a positive deﬁnite matrix. The proof is generally similar to the proof of Theorem 2. The LHS of (28) is upper-bounded as\n≤ h(X ∗ G + ˜ W G ) − µh(X ∗ G + V G ) + h(W G ) − h( ˜ W G ) (29) = h(X ∗ G + W G ) − µh(X ∗ G + V G ), \t (30)\nwhere ˜ W G is chosen to be a Gaussian random vector whose covariance matrix, Σ ˜ W , satisﬁes\nThe inequality (29) is due to Lemma 1 and Theorem 2, and the equality (30) will be proved using the equality condition in data processing inequality. We will also prove that there exists a Gaussian random vector ˜ W G which satisﬁes (31) by proving later Lemma 5.\nTo satisfy the equality in (30), the equality condition in data processing inequality must be satisﬁed, and the following two Markov chains must be formed:\nwhere all random vectors are normally distributed, ˜ W G and W \u2032 G are independent of each other, W G = ˜ W G + W \u2032 G , and X ∗ G is independent of other random vectors.\nThe Markov chain (32) is naturally formed since X ∗ G , ˜ W\n, and W \u2032 G are independent Gaussian random vectors. The validity of the Markov chain (33) is proved using the concept of MGF. In the Markov chain (33), since all random vectors are Gaussian (without loss of generality, they are assumed to have zero means), using Lemma 3, the following relationship\nIf (B) in (34) vanishes, Y 1 and Y 2 are independent given Y 3 , and the Markov chain (33) is obtained. Using Lemma 11, (1) in [2], we deﬁne a covariance matrix Σ ˜ W as follows:\nwhere K ≽ 0, KΣ X ∗ = Σ X ∗ K = 0, and 0 denotes an n- by-n zero matrix. Then, there exists a positive semi-deﬁnite matrix K which satisﬁes Σ ˜ W ≼ µ −1 Σ V and KΣ X ∗ = 0, where Σ X ∗ = (µ −1) −1 (Σ V −µΣ ˜ W ). The existence of matrix K is proved by the following lemma.\nLemma 5: There always exists a positive semi-deﬁnite ma- trix K which satisﬁes\nwhere Σ X ∗ =(µ −1) −1 (Σ V −µΣ ˜ W ), and Σ ˜ W =(Σ −1 W +K) −1 . Proof: The proof is similar to the one in Lemma 4. The\nSince Σ ˜ W is deﬁned as (Σ −1 W + K) −1 in (35), Σ ˜ W satisﬁes (Σ X ∗ + Σ ˜ W ) −1 = (Σ X ∗ + Σ W ) −1 + K, (36)\nbased on Lemma 11, (1) in [2]. Since KΣ X ∗ = Σ X ∗ K = 0, multiplying with Σ X ∗ both sides of (36), (36) is expressed as\n(Σ X ∗ + Σ ˜ W ) −1 Σ X ∗ = (Σ X ∗ + Σ W ) −1 Σ X ∗ , \t (37) Σ X ∗ (Σ X ∗ + Σ ˜ W ) −1 = Σ X ∗ (Σ X ∗ + Σ W ) −1 . \t (38)\nIn this case, random vectors Y 1 , Y 2 , and Y 3 are deﬁned as Y 1 = X ∗ G , Y 2 = X ∗ G + ˜ W G , and Y 3 = X ∗ G + W G , respectively. W = W \u2032 G + ˜ W G , and X ∗ G , ˜ W G , and W \u2032 G are independent of one another. Therefore, their covariance matrices are represented as\nFrom (37) and (39), and (38) and (39), respectively, it turns out that\nSince the inverse of Σ ˜ W exists, (Σ X ∗ + Σ ˜ W ) −1 also ex- ists. Therefore, (B) in (34) is zero, and M Y 1 +Y 2 |Y 3 (S) = M Y 1 |Y 3 (S)M Y 2 |Y 3 (S). It means Y 1 and Y 3 are independent given Y 2 , i.e., X ∗ G and X ∗ G + ˜ W G are independent given X ∗ G + ˜ W G + W \u2032 G , and the Markov chain (33) is valid. The equality in (30) is achieved by the above procedure, and the proof is completed.\nDue to space limitations, the details of applications are not shown herein paper. However, either by replacing the channel enhancement technique by the proposed novel method or by simply adapting the extremal entropy inequality without using the channel enhancement technique, the following applica- tions can be established: the capacity of the vector Gaussian broadcast channel, the lower bound for the achievable rate of distributed source coding with a single quadratic distortion constraint [1], and the secrecy capacity of the Gaussian wire- tap channel [8]. As a distinctive feature to what was shown in [1], [2], [8], and [9], the channel enhancement technique is bypassed by the simple equality condition of data processing inequality.\nThe major contributions of this paper can be summarized as follows. First, an alternative proof- simpler, more direct, and information-theoretic than the original proofs- of EEI was provided. The alternative proof is mainly based on data processing inequality which enables to bypass the KKT con- ditions. Moreover, using properties of positive semi-deﬁnite matrices, one can skip the step of proving the existence of the optimal solution which satisﬁes the KKT conditions, a step which is quite complicated to justify. Finally, this paper proposed a novel method to investigate several applications such as the capacity of the vector Gaussian broadcast channel, the secrecy capacity of the Gaussian wire-tap channel, etc. This novel technique is based on data processing inequality, and it is very unique and creative in respect that it presents a novel paradigm for lots of applications, which were proved commonly based on the channel enhancement technique [1], [2], [8], and [9]."},"refs":[{"authors":[{"name":"T. Liu"},{"name":"P. Viswanath"}],"title":{"text":"An Extremal Inequality Motivated by Multi- terminal Information-Theoretic Problems"}},{"authors":[{"name":"H. Weingarten"},{"name":"Y. Steinberg"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"The Capacity Region of the Gaussian Mutiple-Input Multiple-Output Broadcast Channel"}},{"authors":[{"name":"O. Rioul"}],"title":{"text":"Information Theoretic Proofs of Entropy Power Inequalities"}},{"authors":[{"name":"S. N. Diggavi"},{"name":"T. M. Cover"}],"title":{"text":"The Worst Additive Noise under a Covariance Constraint"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, New York: Wiley-Interscience, 1991"}},{"authors":[{"name":"S. Park"},{"name":"E. Serpedin"},{"name":"K. Qaraqe"}],"title":{"text":"An Alternative Proof of an Extremal Inequality"}},{"authors":[{"name":"R. A. Hor"},{"name":"C. R. Johnso"}],"title":{"text":"Matrix Analysis, Cambridge University Press, 1985"}},{"authors":[{"name":"T. Liu"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"A Note on the Secrecy Capacity of the Multiple-Antenna Wiretap Channel"}},{"authors":[{"name":"E. Ekrem"},{"name":"S. Ulukus"}],"title":{"text":"The Secrecy Capacity Region of the Gaussian MIMO Multi-Receiver Wiretap Channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569558985.pdf"},"links":[{"id":"1569559259","weight":30},{"id":"1569559541","weight":24},{"id":"1569559221","weight":21},{"id":"1569558785","weight":10},{"id":"1569559565","weight":43},{"id":"1569558681","weight":57},{"id":"1569559195","weight":8},{"id":"1569558859","weight":11},{"id":"1569566489","weight":15},{"id":"1569558901","weight":10},{"id":"1569559111","weight":12},{"id":"1569558509","weight":16},{"id":"1569565705","weight":5},{"id":"1569551347","weight":47},{"id":"1569559199","weight":19},{"id":"1569559035","weight":19},{"id":"1569558779","weight":26},{"id":"1569559523","weight":2},{"id":"1569559597","weight":33},{"id":"1569559251","weight":23},{"id":"1569550425","weight":24},{"id":"1569564509","weight":31},{"id":"1569558697","weight":10},{"id":"1569559233","weight":14}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T8.4","endtime":"16:00","authors":"Sangwoo Park, Erchin Serpedin, Khalid A. Qaraqe","date":"1341330000000","papertitle":"An Information Theoretic Perspective Over an Extremal Entropy Inequality","starttime":"15:40","session":"S7.T8: Information Inequalities","room":"Stratton (491)","paperid":"1569558985"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
