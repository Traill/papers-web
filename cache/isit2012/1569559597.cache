{"id":"1569559597","paper":{"title":{"text":"Mismatched MMSE Estimation of Multivariate Gaussian Sources"},"authors":[{"name":"I˜naki Esnaola"},{"name":"Antonia M. Tulino"},{"name":"H. Vincent Poor"}],"abstr":{"text":"Abstract\u2014The distortion increase in minimum mean-square error (MMSE) estimation of multivariate Gaussian sources is analyzed for the situation in which the statistics are mismatched, i.e., the covariance matrix is not perfectly known during the estimation process. First a deterministic mismatch model with an additive perturbation matrix is considered, for which we provide closed form expressions for the distortion excess caused by the mismatch. The mismatch study is then generalized by using random matrix theory tools which allow an asymptotic result for a broad class of perturbation matrices to be proved."},"body":{"text":"We model a source signal as a Gaussian random vector with mean µ ∈ R n and covariance matrix Σ ∈ R n×n . Multivariate Gaussian distributions describe a wide range of real signals and provide a ﬂexible way of modeling prior knowledge by means of the covariance matrix. However, in general, perfect knowledge of second order statistics is not available, due to the need for estimating them during a training period, or because source statistics are nonstationary. For this reason, a practical system operates with a postulated covariance matrix, Σ ∗ , which for the case of perfect prior knowledge is Σ ∗ = Σ. The degree of mismatch between postulated and real covariance matrices characterizes the amount of prior knowledge. We assume µ is known, and thus without loss of generality, throughout the rest of the paper we assume µ = 0.\nAlthough the theoretical results presented in this paper apply to any positive deﬁnite covariance matrix Σ, for the sake of discussion and in order to illustrate the results, we introduce a particular covariance matrix model. Since covariance matrices of weakly stationary random processes are Toeplitz [1], we consider an exponentially decaying Toeplitz model that re- sembles a physical temporal correlation where the correlation decreases as the temporal distance increases and the strength of the correlation is set by a parameter ρ. This covariance matrix is given by Σ = s ij = ρ |i−j| ; i, j = 1, 2, . . . , n .\nThe source sequence, x ∈ R n , is transmitted through an additive white Gaussian noise (AWGN) channel such that y =\nx + z where z ∼ N (0, I n ). Based in this channel model, we deﬁne the signal to noise ratio as snr = tr(Σ)/n. The minimum mean-square error (MMSE) estimator of the input x given the channel output y is\nWhen the covariance matrix is known, this estimation method minimizes the reconstruction error measured by the per- symbol mean square error (MSE) distortion given by\nIn practical scenarios, access to perfect source statistics is not a realistic assumption. Instead, postulated statistics are available which do not match real statistics, and the resulting distortion of the mismatched estimator is D ∗ = D+D Γ , where D Γ is the excess distortion incurred by the system due to imperfect knowledge of the distribution. Since we consider multivariate Gaussian sources, we analyze the excess distortion for the case in which the mismatch between real and postulated distributions is modeled through the mismatch between the true and the postulated covariance matrices.\nFor a given true covariance matrix, Σ, and a given postu- lated covariance matrix, Σ ∗ , we quantify the distortion excess as a function of the additive perturbation matrix γΓ = Σ ∗ −Σ. Without loss of generality, we assume tr[Γ]/n = 1, and thus γ = tr[Σ − Σ ∗ ]/n quantiﬁes the mismatch strength.\nTheorem 1 Let Σ ∗ = Σ + γΓ be the postulated covariance matrix available in the MMSE estimation recovery process, where Σ is the real covariance matrix, Γ ∈ R n×n is an additive mismatch which ensures Σ ∗ is positive deﬁnite, and γ is a parameter governing the strength of the mismatch. The excess distortion incurred by the mismatched estimator is\nand the covariance matrix for the received signal is Σ yy = Σ + I n .\nProof: The estimate with the mismatched covariance matrix can be written as\n+γΓ \u2020 Σ \u2020 + I n + γΓ \u2020 −1 y \t (5) and by applying the inversion lemma it follows that\nwhich combined with (5) and after some manipulation allows us to isolate the effect of the mismatch in the linear estimation such that\nˆ x ∗ = (Υ + Υ e ) y = ˆ x + Υ e y . \t (6) The result follows by performing simple algebraic operations on the error covariance matrix.\nBased in the previous theorem we characterize the total excess distortion deﬁned as\nwhich can be seen as an integral value of the distortion excess averaged over all possible values of the signal to noise ratio.\nLemma 1 The total excess distortion, ∆, induced by the mismatched MMSE estimator is given by\nProof: The proof follows by using the link between mismatch estimation and relative entropy given in [2] which states\nThe result follows by integrating the right hand side term of (9) which was done in [3]. B. Random Mismatch\nIn real systems, the mismatch between prior and postulated statistics arises from an estimation process in which the system learns the statistics of the source. For that reason, it is reasonable to assume that the perturbation matrix, Γ, admits a statistical model and γ is linked to the average of the variances of the entries of the perturbation matrix. That being the case, the results given in Theorem 1 and Lemma 1 can be averaged over an ensemble of random matrices which allows us to deﬁne the average excess distortion (AED) as\nwhere the expectations are with respect to the distribution induced by the perturbation statistics.\nThe average quantities given in (10) and (11) are in general difﬁcult to express in an explicit form and even for classical Gaussian Wigner perturbation statistics [4] the resulting ex- pression can be very involved. For this reason, we use random matrix theory tools and analyze the asymptotic case when n → ∞. In the following, we obtain closed form expressions for the ATED for a large class of random matrices in the asymptotic regime.\nIn this section we collect several useful deﬁnitions and some results in random matrix theory. We also present new results for a broader class of matrices of the Wishart counterparts in [4]. Speciﬁcally, Theorems 3, 4 and 5 are new results which allow us to prove Theorem 6.\nDeﬁnition 1 [4] The η-transform of a nonnegative random variable X is\nand the Shannon transform of a nonnegative random variable X is deﬁned as\nDeﬁnition 2 [4] The asymptotic eigenvalue distribution, F A (·), of an n × n Hermitian random matrix A is deﬁned as\n1 n\nwhere λ 1 (A), . . . , λ n (A) are the eigenvalues of A and 1{·} is the indicator function.\nDeﬁnition 3 [4] Consider an n× n random Hermitian matrix Γ whose entries have variances Var[Γ i,j ] = V i,j n such that V i,j = V j,i with V an n × n deterministic symmetric matrix whose entries are uniformly bounded. For each n, let\nv n : [0, 1) × [0, 1) → R \t (15) be the variance proﬁle function given by the 1-exchangable 1 function\nWhenever v n (x, y) converges uniformly to a limiting bounded measurable function, v(x, y), we deﬁne this limit as the asymptotic variance proﬁle of Γ.\nBased on this deﬁnition we can claim the following theorem which is an immediate consequence of [5].\nTheorem 2 [5] Let Γ be an n × n Hermitian complex ran- dom matrix whose upper-triangular entries are independent zero-mean complex random variables (arbitrarily distributed) satisfying a Lindeberg-type condition\nn , \t (18) where V is an n × n deterministic symmetric matrix whose entries are uniformly bounded and from which the asymptotic variance proﬁle of Γ, denoted by v(x, y), can be obtained as per Deﬁnition 3. As n → ∞, the empirical eigenvalue distri- bution of Γ converges almost surely to a limiting distribution whose η-transform is\nwhere X and Y are independent random variables uniform on [0, 1].\nThe zero-mean hypothesis in Theorem 2 can be relaxed using [4, Lemma 2.23]. Speciﬁcally, if the rank of E[Γ] is o(n), then Theorem 2 still holds.\nSome new results follow which represent a Wigner matrix counterpart of the results in [4].\nTheorem 3 Let Γ be an n×n complex random matrix deﬁned as in Theorem 2. Its Shannon transform is\nwhere X and Y are independent random variables uniform on [0, 1].\nProof: Based on its deﬁnition, the derivative of V Γ (γ) with respect to γ, denoted by ˙ (·), is\n1 + γ λ dF (λ) \t (23) = log e γ (1 − E [A(X, γ)]) \t (24)\nwhere (24) follows from Theorem 2 and A(x, γ) satisﬁes (22). We deﬁne G(Y, γ) = E[v(X, Y)A(X, γ)|Y] and notice also that\n1 + γ E[v(X, Y) A(X, γ)|Y] log e = −E ˙ G(Y, γ) ˜ A(X, γ) log e \t (27)\nwhere we have further used (22), which yields ˜ A(y, γ) = \t γ\nwhere (28) follows from the 1-exchangability of the variance proﬁle v(x, y). Note that\nwhere (29) follows from the 1-exchangability of the variance proﬁle v(x, y). Integrating over γ and using V Γ (0) = 0 the claimed result is found.\nTheorem 4 Let Γ be an n×n complex random matrix deﬁned as\nwith A = diag A 2 1 , . . . , A 2 n , and Γ w denoting an n × n Hermitian complex random matrix whose upper-triangular entries are independent identically distributed zero-mean com- plex random variables. Its Shannon transform is\nV Γ (γ) = 2 E [log(1 + γ a(X) (γ))] − γ (γ) 2 log e (31) with (γ) satisfying the ﬁxed-point equation\nwhere Z is a uniform random variable on [0, 1], and a(·) such that the distribution of a(Z) equals the asymptotic empirical distribution of A 2 .\nProof: In order to prove Theorem 4, we need to evaluate the η and the Shannon transforms of the Hermitian matrix Γ = AΓ w A whose entries are independent zero-mean random variables with variance\nn \t (33) and whose variance proﬁle is\nv(x, y) = a(x)a(y) \t (34) with a(·) such that the distribution of a(Z) (with Z uniform on [0, 1]) equals the asymptotic empirical distribution of A 2 = diag A 2 1 , . . . , A 2 n . Hence, Theorem 4 can be proved as a special case of Theorem 3 when the function v(x, y) can be factored. In this case, the expressions for A(x, γ) and G(y, γ) given by Equations (21) and (22) in Theorem 3 become\nwhere X and Y are independent random variables uniform on [0, 1]. Using (19), we have that\nη Γ (γ) = E [ A(X, γ) ] \t (37) = E 1 − γ \t a(X) 1 + γa(X)E[a(Y)A(Y, γ)] . (38)\nFrom (36), we have that (γ) satisﬁes the following ﬁxed- point equation\nη Γ (γ) = 1 − γ (γ). \t (42) Moreover,\nE [G(Y, γ)A(Y, γ)] = E [a(Y)E[a(X)A(X, γ)]A(Y, γ)] = E [a(Y)A(Y, γ)] E[a(X)A(X, γ)]\nTheorem 5 Let Γ be an n×n complex random matrix deﬁned as\nΓ = Σ 1 /2 Γ w Σ 1 /2 \t (44) with Σ an n×n Hermitian semi-positive deﬁnite deterministic matrix whose empirical eigenvalue distribution converges to a compactly supported measure, and Γ w denoting an n × n Hermitian complex random matrix whose upper-triangular entries are independent identically distributed zero-mean com- plex random variables. Furthermore, let ˜ Γ be an n×n complex random matrix deﬁned as\nwith Γ w deﬁned as before and Λ denoting the diagonal matrix whose diagonal elements are the eigenvalues of Σ. Then, Γ and ˜ Γ admit the same asymptotic eigenvalue distribution.\nProof: The proof of Theorem 5 is an immediate con- sequence of free probability theory [4]. From [7] and [8] it follows that Γ w is almost surely free with any deterministic matrix. Hence, using [4, Theorem 2.67], we have that\nΣ Γ (x) = Σ Σ (x)Σ Γ w (x) \t (46) = Σ Λ (x)Σ Γ w (x) \t (47) = Σ ˜ Γ (x) \t (48)\nfrom which it follows that Γ and ˜ Γ w admit the same asymp- totic eigenvalue distribution. B. Main Result\nTheorem 6 Let Γ w be an n × n Hermitian complex random matrix whose upper-triangular entries are independent zero- mean complex random variables with variances 1 n , and let Σ a n × n deterministic matrix. Then as n → ∞\n(49) converges almost surely to ∆ ∞ , with ∆ ∞ given by\nV(γ) = 2 E [log(1 + γ Λ Σ (γ))] − γ (γ) 2 log e, (51) and\nwhere Λ Σ is a random variable whose distribution is the asymptotic spectrum of Σ −1 .\nProof: Using the fact that Σ is a full rank matrix, after some simple algebraic manipulation, we can write ∆ as\n+ 1 2n tr (I + γΣ −1/2 Γ w Σ −1/2 ) −1 − n (55) from which, via Theorem 3, it is immediate to prove that it converges to almost surely to ∆ ∞ , with ∆ ∞ given by the following expression\nwhere V Ψ (γ) and η Ψ (γ) denote, respectively, the Shannon- and the η-transform of the Hermitian matrix given by\nΨ = Σ −1/2 Γ w Σ −1/2 \t (57) from which using Theorems 5 and 4, the claim follows.\nIn order to illustrate numerically the effect of the average mismatch, we assume a perturbation model where Γ = HH \u2020 with H ∈ R n×n a real random matrix with entries distributed as N (0, n −1 ). Figure 1 shows how asymptotic ATED increases with the mismatch strength parameter γ. The curves show that the distortion increases rapidly for small values of γ and saturate as the mismatch increases. As expected, the case of ρ = 0.99 is the most sensitive one to variations of γ. This can be intuitively interpreted due to the fact that the higher parameter ρ is, the stronger correlation the source presents, and as a result reliable prior knowledge plays an important role. Figure 2 depicts total mismatch distortion as a function\nof the correlation parameter ρ for different values of γ. In fact, Figure 2 shows that the distortion growth increases for highly correlated sources, reaching a maximum in the limit when ρ = 1. Note that although the case for which ρ = 1 makes no physical sense, the vicinity of it is of practical interest.\nWe have studied the effect of incomplete statistical in- formation on MMSE estimation of multivariate Gaussian sources corrupted by additive white Gaussian noise. For an additive perturbation matrix modeling the mismatch, closed form expressions for the excess distortion and the total excess distortion have been obtained. By using random matrix theory tools we have analytically derived the asymptotic average total excess distortion of an MMSE estimation process. In order to do so, we have generalized for a broader class of matrices some results which were previously known for Wishart matrices."},"refs":[{"authors":[{"name":"R. M. Gray"}],"title":{"text":"Toeplitz and Circulant Matrices: A Review"}},{"authors":[{"name":"S. Verd´u"}],"title":{"text":"Mismatched Estimation and Relative Entropy"}},{"authors":[{"name":"R. Kleeman"}],"title":{"text":"Measuring Dynamical Prediction Utility Using Relative Entropy"}},{"authors":[{"name":"A. Tulino"},{"name":"S. Verd´u"}],"title":{"text":"Random Matrix Theory and Wireless Commu- nications"}},{"authors":[{"name":"D. Shlyankhtenko"}],"title":{"text":"Random Gaussian Band Matrices and Freeness with Amalgamation"}},{"authors":[{"name":"Y. Q. Yin"}],"title":{"text":"Limiting Spectral Distributions for a Class of Random Matrices"}},{"authors":[{"name":"M. Capitaine"},{"name":"C. Donati-Martin"}],"title":{"text":"Strong Asymptotic Freeness for Wigner and Wishart Matrices"}},{"authors":[{"name":"K. Dykema"}],"title":{"text":"On Certain Free Product Factors via an Extended Matrix Model"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569559597.pdf"},"links":[{"id":"1569565383","weight":8},{"id":"1569565223","weight":16},{"id":"1569566385","weight":8},{"id":"1569564635","weight":8},{"id":"1569559617","weight":8},{"id":"1569566683","weight":8},{"id":"1569566227","weight":8},{"id":"1569552245","weight":8},{"id":"1569566469","weight":8},{"id":"1569565355","weight":8},{"id":"1569551535","weight":8},{"id":"1569565461","weight":8},{"id":"1569564245","weight":8},{"id":"1569564227","weight":8},{"id":"1569565123","weight":8},{"id":"1569558459","weight":8},{"id":"1569565771","weight":8},{"id":"1569566999","weight":8},{"id":"1569566963","weight":8},{"id":"1569564989","weight":8},{"id":"1569566523","weight":8},{"id":"1569565897","weight":8},{"id":"1569566239","weight":8},{"id":"1569566753","weight":8},{"id":"1569558681","weight":8},{"id":"1569566437","weight":8},{"id":"1569553909","weight":8},{"id":"1569565427","weight":8},{"id":"1569552251","weight":8},{"id":"1569554971","weight":8},{"id":"1569566209","weight":8},{"id":"1569562821","weight":8},{"id":"1569566629","weight":8},{"id":"1569563897","weight":8},{"id":"1569566505","weight":8},{"id":"1569565393","weight":8},{"id":"1569562207","weight":8},{"id":"1569567033","weight":8},{"id":"1569565463","weight":8},{"id":"1569562551","weight":8},{"id":"1569551347","weight":8},{"id":"1569555367","weight":8},{"id":"1569561623","weight":8},{"id":"1569566983","weight":25},{"id":"1569565397","weight":8},{"id":"1569566873","weight":8},{"id":"1569566267","weight":8},{"id":"1569565013","weight":8},{"id":"1569565293","weight":8},{"id":"1569566641","weight":8},{"id":"1569551905","weight":8},{"id":"1569565529","weight":8},{"id":"1569566619","weight":8},{"id":"1569566147","weight":8},{"id":"1569565337","weight":8},{"id":"1569566341","weight":8},{"id":"1569565889","weight":8},{"id":"1569566375","weight":8},{"id":"1569566067","weight":8},{"id":"1569566825","weight":8},{"id":"1569566609","weight":8},{"id":"1569566443","weight":8}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T8.4","endtime":"18:00","authors":"Iñaki Esnaola, Antonia Tulino, H. Vincent Poor","date":"1341250800000","papertitle":"Mismatched MMSE Estimation of Multivariate Gaussian Sources","starttime":"17:40","session":"S4.T8: Information and Estimation","room":"Stratton (491)","paperid":"1569559597"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
