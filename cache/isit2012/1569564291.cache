{"id":"1569564291","paper":{"title":{"text":"Biff (Bloom Filter) Codes: Fast Error Correction for Large Data Sets"},"authors":[{"name":"Michael Mitzenmacher ∗"},{"name":"George Varghese \u2020"}],"abstr":{"text":"Abstract\u2014Large data sets are increasingly common in cloud and virtualized environments. For example, transfers of multiple gigabytes are commonplace, as are replicated blocks of such sizes. There is a need for fast error-correction or data reconciliation in such settings even when the expected number of errors is small.\nMotivated by such cloud reconciliation problems, we consider error-correction schemes designed for large data, after explaining why previous approaches appear unsuitable. We introduce Biff codes, which are based on Bloom ﬁlters and are designed for large data. For Biff codes with a message of length L and E errors, the encoding time is O(L), decoding time is O(L + E) and the space overhead is O(E). Biff codes are low-density parity- check codes; they are similar to Tornado codes, but are designed for errors instead of erasures. Further, Biff codes are designed to be very simple, removing any explicit graph structures and based entirely on hash tables. We derive Biff codes by a simple reduction from a set reconciliation algorithm for a recently developed data structure, invertible Bloom lookup tables. While the underlying theory is extremely simple, what makes this code especially attractive is the ease with which it can be implemented and the speed of decoding. We present results from a prototype implementation that decodes messages of 1 million words with thousands of errors in well under a second."},"body":{"text":"Motivated by the frequent need to transfer and reconcile large data sets in virtualized and cloud environments, we provide a very simple and fast error-correcting code designed for very large data streams. For example, consider the speciﬁc problem of reconciling two memories of 2 Gbytes whose contents may differ by a small number of 32-bit words. Alternatively, one can picture transferring a memory of this size, and needing to check for errors after it is written to the new storage. We assume errors are mutation errors; data order remains intact.\nOther possible applications include deduplication, as ex- empliﬁed by the Difference Engine [6]. While storage may seem cheap, great cost savings can be effected by replacing redundant copies of data with a single copy and pointers in other locations. For example, in virtualized environments, it is not surprising that two virtual machines might have virtual memories with a great deal of redundancy. For example, both VMs may include similar copies of the operating system. More generally, we are concerned with any setting with large data transfers over networks.\nIn this setting, our primary notion of efﬁciency differs somewhat from standard coding. While we still want the re- dundancy added for the code to be as small as possible, speed appears to be a more important criterion for large data sets. In particular, for a message of length L and E errors, while we may want close to the minimum overhead of E words, O(E)\nwords with a reasonably small constant should sufﬁce. More importantly, we require very fast encoding and decoding times; encoding should be O(L) and decoding should be O(L + E), with very small constant factors implied in the asymptotic notation. Typically, E will be very small compared to L; we expect very small error rates, or even subconstant error rates (such as a bounded number of errors).\nIn this paper, we describe new codes that are designed for large data. We also show why other approaches (such as Reed-Solomon Codes or Tornado codes with with block- based checksum) are unsuitable. Our codes are extremely attractive from the point of view of engineering effectiveness: our software prototype implementation is very fast, decoding messages of 1 million words with thousands of errors in under a second.\nWe call our codes Biff codes, where Biff denotes how we pronounce BF, for Bloom ﬁlter. Biff codes are motivated by recent Bloom ﬁlter variations, the invertible Bloom ﬁlter [3] and invertible Bloom lookup table [5], and their uses for set reconciliation [4], as explained below. Alternatively, Biff codes are similar to Tornado codes [1], [9], and can be viewed as a practical, randomized low-density parity-check code with an especially simple structure designed speciﬁcally for word-level mutation errors. Also, while Tornado codes were designed using multiple levels of random graphs with carefully chosen degree distributions, Biff codes reduce this structure to its barest elements; our basic structure is single-layer, and regular, in that each message symbol takes part in the same number of encoded symbols. As a result, programming efﬁcient encoding and decoding routines can easily be done in a matter of hours. We expect this simplicity will be prized as a virtue in practical settings; indeed, we believe Biff codes reﬂect the essential ideas behind other related low-density parity-check (LDPC) codes, in their simplest form.\nWe also provide a simple (and apparently new) general reduction from error correcting codes to set reconciliation. While reductions from erasure and error correcting codes to set reconciliation are well known [7], [10], our reduction may be useful independent of Biff Codes.\nFinally, a related approach for reconciliation, focused on the setting of data streams, appears in [14].\nWe now describe how to construct Biff codes from invertible Bloom lookup tables (IBLTs). The source of the stream of ideas we exploit is a seminal paper called Invertible Bloom\nFilters by Eppstein and Goodrich that invented a streaming data structure for the so-called straggler problem [3]. The basic idea was generalized for set reconciliation by Eppstein, Goodrich, Uyeda, and Varghese in [4] and generalized and improved further by Goodrich and Mitzenmacher to IBLTs [5]. We choose to use the framework of IBLTs in the exposition that follows though we could have used the set difference algorithm in [4] instead.\nWe start by reviewing the main aspects of IBLTs that we require from [5]. We note that we do not require the full IBLT structure for our application, so we discuss only the elements that we need, and refer readers to [5] for further details on IBLT performance.\nOur IBLT construction uses a table T of m cells, and a set of k random hash functions, h 1 , h 2 , . . ., h k , to store a collection of key-value pairs. In our setting, keys will be distinct, and each key will have a value determined by the key. On an insertion, each key-value pair is placed into cells T [h 1 (x)], T [h 2 (x)], . . . T [h k (x)]. We assume the hash func- tions are fully random (hash values independent and uniformly distributed); in practice this assumption appears suitable (see, e.g., [11], [13] for related work on this point). For technical reasons, we assume that distinct hash functions yield distinct locations. This can be accomplished in various ways, such as by splitting the m cells into k subtables each of size m/k, and having each hash function choose one cell (uniformly) from each subtable. Such splitting does not affect the asymptotic behavior in our analysis.\nIn a standard IBLT, each cell contains three ﬁelds: a keySum ﬁeld, which is the exclusive-or (XOR) of all the keys that have been inserted that map to this cell; a valueSum ﬁeld, which is the XOR of all the values of the keys that have been inserted that map to this cell; and a count ﬁeld, which counts the number of keys that have been inserted into the cell.\nAs all operations are XORs, deletions are handled in an equivalent manner: on deletion of a previously inserted key- value pair, the IBLT XORs the key and value with the ﬁelds in the appropriate cells, and the count is reversed. This reverses a corresponding insertion. We will discuss later how to deal with deletions without corresponding insertions, a case that can usefully occur in our setting.\nWe now consider how to list the entries of the IBLT. The approach is straightforward. We do a ﬁrst pass through the cells to ﬁnd cells with a count of 1, and construct a list of those cells. We recover the key and corresponding value from this cell, and then delete the corresponding pair from the table. In the course of performing deletions, we check the count of the relevant cells. If a cell\u2019s count becomes 1, we add it to the list; if it drops from 1 to 0, we can remove it from the list. This approach can easily be implemented O(m) time.\nIf at the end of this process all the cells have a count of 0, then we have succeeded in recovering all the entries in the IBLT. Otherwise, the method only outputs a partial list of the key-value pairs in B.\nThis \u201cpeeling process\u201d is well known in the context of random graphs and hypergraphs as the process used to ﬁnd the 2-core of a random hypergraph (e.g., see [2], [12]). This peeling process is similarly used for various codes, including Tornado codes and their derivatives (e.g., see [9]). Previous results therefore give tight thresholds: when the number of hash values k for each pair is at least 2 and there are n key- value pairs in the IBLT, there are constants c k > 1 such that if m > (c k + )n for any constant > 0, the listing process succeeds with probability 1 − o(1); similarly, if m < (c k − )n for any constant > 0, the listing process fails with probability 1 − o(1). As shown in [2], [12], these values are given by\nThe choice of k affects the probability of the listing process failing. By choosing k sufﬁciently large and m above the 2-core threshold, standard results give that the bottleneck is the possibility of having two key-value pairs with the same collection of hash values, giving a failure probability of O(m −k+2 ).\nWe note that, with some additional effort, there are various ways to save space with the IBLT structure that are known in the literature. including using compressed arrays, quotienting, and irregular constructions (where different keys can utilize a different number of hash values, as in irregular LDPC codes). In practice the constant factors are small, and such approaches may interfere with the simplicity we aim for with the IBLT approach; we therefore do not consider them further here.\nWe consider two users, Alice and Bob, referred to as A and B. Suppose Alice and Bob hold distinct but similar sets of keys, and they would like to reconcile the differences. This is the well known set reconciliation problem. To achieve such a reconciliation with low overhead [4], Alice constructs an IBLT. The value associated with each key is a ﬁngerprint (or checksum) obtained from the key. In what follows, we assume the value is taken by hashing the key, yielding an uniform value over all b-bit values for an appropriate b, and that the hash function is shared between Alice and Bob. Alice sends Bob her IBLT, and he correspondingly deletes from the IBLT the key-value pairs from his set.\nIn this setting, when Bob deletes a key-value pair not held by Alice, it is possible for a cell count to become negative. The remaining key-value pairs left in the IBLT correspond exactly to items that exactly one of Alice or Bob has. Bob can use the IBLT structure to recover these pairs efﬁciently. For lack of space, we present the argument informally; the IBLT properties we use were formally derived in [5].\nWe ﬁrst note that, in this setting, because deletions may reduce the count of cells, it is possible that a cell can have a\ncount of 1 but not contain a single key-value pair. For example, if two pairs are inserted by Alice into a cell, and Bob deletes a pair that does not match Alice\u2019s in that cell, the count will be 1. Hence, in this setting, the proper way to check if a cell contains a valid pair is to test that the checksum for the keySum ﬁeld matches the valueSum. In fact, because the value corresponds to a checksum, the count ﬁeld is extraneous. (It can be a useful additional check, but strictly is unnecessary. Moreover, it may not be space-effective; since the counts will depend on the number of items inserted, not on the size of the difference between the sets.) Instead, the list of cells that allow us to recover a value in our listing process are determined by a match of the key and checksum value. Importantly, because Bob\u2019s deletion operation is symmetric to Alice\u2019s insertion operation, this holds true for cells containing a pair deleted by Bob as well as cells containing a pair inserted by Alice. (In this case, the corresponding count, if used, should be −1 for cells with a deleted pair.)\nBob can therefore use the IBLT to recover these pairs efﬁ- ciently. (Strictly speaking, Bob need only recover Alice\u2019s keys, but this simpliﬁcation does not make a noticeable difference in our context.) If ∆ is an upper bound on the number of keys not shared between Alice and Bob, then from the argument sketched above, an IBLT with only O(∆) cells is necessary, with the constant factor dependent on the success probability desired.\nWe now show how to use the above scheme to obtain a computationally efﬁcient error-correcting code. Our error- correcting code can be viewed as a reduction using set recon- ciliation. Let B have a message for A corresponding to the se- quence of values x 1 , x 2 , . . . , x n . Then B sends A the message along with set reconciliation information \u2013 in our case, the IBLT \u2013 for the set of ordered pairs (x 1 , 1), (x 2 , 2), . . . , (x n , n). For now we assume the set reconciliation information A obtains is without error; errors only occur in message values. When A obtains the sequence y 1 , y 2 , . . . , y n , she constructs her own set of pairs (y 1 , 1), (y 2 , 2), . . . , (y n , n), and recon- ciles the two sets to ﬁnd erroneous positions. Notice that this approach requires random symbol errors as opposed to adversarial errors for our IBLT approach, as we require the checksums to accurately determine when key-value pairs are valid. However, there are standard approaches that overcome this problem that would make it suitable for adversarial errors with a suitably limited adversary (by applying a pseudo- random permutation on the symbols that is secret from the adversary; see, for example, [8]). Also, the positions of the errors can be anywhere in the message (as long as the positions are chosen independently of the method used to generate the set reconciliation information).\nIf there are no errors in the data for the IBLT structure, then this reduction can be directly applied. However, assuming the IBLT is sent over the same channel as the data, then some cells in the IBLT will have erroneous keySum or valueSum ﬁelds. If errors are randomly distributed and the error rate is sufﬁciently small, this is not a concern; as shown in [5],\nIBLT listing is quite robust against errors in the IBLT structure. Speciﬁcally, an error will cause the keySum and valueSum ﬁelds of an IBLT cell not to match, and as such it will not be used for decoding; this can be problematic if all the cells hashed to be an erroneous message cell are themselves in error, as the value cannot then be recovered, but under appropriate parameter settings this will be rare in practice. As a summary, using the 1-layer scheme, where errors can occur in the IBLT, the main contribution to the failure probability is when an erroneous symbol suffers from all k of its hash locations in the IBLT being in error. If z is the fraction of IBLT cells in error, the expected number of such symbols is Ez k , and the distribution of such failures is binomial (and approximately Poisson, when the expectation is small). Hence, when such errors occur, there is usually only one of them, and instead of using recursive error correction on the IBLT one could instead use a very small amount of error correction in the original message.\nFor bursty errors or other error models, we may need to randomly intersperse the IBLT structure with the original message; note, however, that the randomness used in hashing the message values protects us from bursty errors over the message.\nBasic pseudocode for encoding and decoding of Biff codes is given below (using C-style notation in places); the code is very simple, and is written entirely in terms of hash table operations.\n(T j [a].valueSum == Check(T j [a].keySum)) do (z, i) = T j [a].keySum if z = y i then\nset y i to z when decoding terminates for j = 1 . . . k do\nIn our pseudocode, there is some leeway in how one im- plements the while statement. One natural implementation would keep a list (such as a linked list) of pairs a, j that satisfy the conditions. This list can be initialized by a walk through the arrays, and then updated as the while loop modiﬁes the contents of the table. The total work will clearly be proportional to the size of the tables, which will be O(E) when the table size is chosen appropriately.\nWe may also recursively apply a further IBLT, treating the ﬁrst IBLT as data,; or we can use a more expensive error- correcting code, such as a Reed-Solomon code, to protect the\nmuch smaller IBLT. This approach is similar to that used under the original scheme for Tornado codes, but appears unneces- sary for many natural error models. For ease of exposition, we assume random locations for errors henceforth.\nThe resulting error-correcting code is not space-optimal, but the overhead in terms of the space required for the error- correction information is small when the error-rate is small. If there are e errors, then there will be 2e key-value pairs in the IBLT; the overhead with having 3, 4, or 5 choices, as seen from Table I, will then correspond to less than 3e cells. Each cell contains both a keySum or valueSum, each of which will be (depending on the implementation) roughly the same size as the original key. Note here the key in our setting includes a position as well as the original message symbol, so this is additional overhead. Putting these together, we can expect that the error-correction overhead is roughly a factor of 6 over the optimal amount of overhead, which would be e times the size of a message symbol.\nWhile this is a non-trivial price, it is important to place it in context. For large keys, with a 1% error rate, even an optimal code for a message of length M bytes would require at least (1/0.99)M ≈ 1.01M bytes to be sent, and a standard Reed-Solomon code (correcting E errors with 2E additional values) would require at least 1.02M bytes. Biff codes would require about 1.06M bytes. The resulting advantages, again, are simplicity and speed. We expect that in many engineering contexts, the advantages of the IBLT approach will outweigh the small additional space cost.\nFor very large messages, parallelization can speed things up further; key-value pairs can be inserted or deleted in parallel easily, with the bottleneck being atomic writes when XORing into a cell. The listing step also offers opportunities for parallelization, with threads being based on cells, and cells becoming active when their checksum value matches the key. We don\u2019t explore parallelization further here, but we note the simple, regular framework at the heart of Biff codes.\nWe also note that, naturally, the approach of using IBLTs can be applied to design a simple erasure-correcting code. This corresponds to a set reconciliation problem where one set is slightly larger than the other; nothing is inserted at A\u2019s end for missing elements. Other error models may also be handled using the same technique.\nOther natural approaches fail to have both fast encoding and decoding, and maintain O(E) overhead. While asymptotically faster algorithms exist, the computational overhead of Reed- Solomon codes is generally Θ(EL) in practice, making a straightforward implementation infeasible in this setting, once the number of errors is non-trivial. Breaking the data into blocks and encoding each would be ineffective with bursty errors. One could randomly permute the message data before breaking it into blocks, to randomize the position of errors and thereby spread them among blocks. In practice, however, taking a large memory block and then permuting it is ex- tremely expensive as it destroys natural data locality. Once a memory or disk page is read it is almost \u201cfree\u201d to read the\nremaining words in sequence; randomizing positions becomes hugely expensive. (By contrast, the method we suggested to make Biff more resilient to adversarial errors does not destroy locality because it randomizes over symbol values, not positions .) Finally, there are issues in ﬁnding a suitable ﬁeld size to compute over, particularly for large messages.\nThe problems we describe above are not original; similar discussions, for example, appear with the early work on Tornado codes [1]. Experiments comparing Reed-Solomon codes for erasures with Tornado codes from the original paper demonstrate that Reed-Solomon codes are orders of magnitude slower at this scale.\nAn alternative approach is to use Tornado codes (or similar LDPC codes) directly, using checksums to ensure that suitably sized blocks are accurate. For example, we could divide the message of length L into L/B blocks of B symbols and add an error-detection checksum of c bits to each block. If we assume blocks with detected errors are dropped, then E errors could result in EB symbols being dropped, requiring the code to send at least an additional kEB bits for a suitably small constant k. The total overhead would then be Lc/B + kEB; simple calculus yields the minimum overhead is when B = 2\nckLE, with block sizes of O( L/E) and resulting space overhead of O(\nOn the other hand, for Biff codes the redundancy overhead is O(E) with small constants hidden in the O notation, because only the values in the cells of the hash table, and not the original data, require checksums. This is a key beneﬁt of the Biff code approach; only the hash table cells need to be protected with checksums.\nIn order to test our approach, we have implemented Biff codes in software. Our code uses pseudorandom hash values generated from the C drand function (randomly seeded using the clock), and therefore our timing information does not include the time to hash. However, we point out that hashing is unlikely to be a major bottleneck. For example, even if for each one wants 4 hash locations for each key into 4 subtables of size 1024, and an additional 24 bit hash for the checksum for each key, all the necessary values can be obtained with a single 64-bit hash operation.\nSetup: Our has table is split into k equal subtables. As mentioned, to determine locations in each subtable, we use pseudorandom hash values. For convenience we use random 20 bit keys as our original message symbols and 20 bits to describe the location in the sequence. While these keys are small, it allows us to do all computation with 64-bit operations. For a checksum, we use a simple invertible function: the pair (x i , i) gives a checksum of (2i + 1) ∗ x i + i 2 .\nOne standard test case uses 1 million 20-bit message sym- bols and an IBLT of 30000 cells, with errors introduced in 10000 message symbols and 600 IBLT cells. Note that with only 20 bit keys and 20 bits to record the length, an IBLT cell is actually 4 times the size of a message cell (so our IBLT is 2400000 bits); however, we use a 2% error rate in the IBLT as we expect message symbols will generally be much longer.\nFor example, in practice a key might be a 1KB packet, in which case 1 million message symbols would correspond to a gigabyte.\nTiming: Our results show Biff codes to be extremely fast. There are two decoding stages, as can be seen in the previously given pseudocode. First, the received sequence values must be placed into the hash table. Second, the hash table must be processed and the erroneous values recovered. Generally, the bulk of the work will actually be in the ﬁrst stage, when the number of errors are small. We had to utilize messages of 1 million symbols in order to obtain suitable timing data; otherwise processing was too fast. On our standard test case over 1000 trials, using 4 hash functions the ﬁrst stage took 0.0561 seconds on average and the second took 0.0069 seconds on average. With 5 hash functions, the numbers were 0.0651 second and 0.0078 seconds.\nThresholds: Our threshold calculations are very accurate. For example, in a setting where no errors are introduced in the IBLT, with 4 hash functions and 10000 errors we would expect to require approximately 26000 cells in order to recover fully. (Recall that 10000 errors means 20000 keys are placed into the IBLT.) Our experiments yielded that with and IBLT of 26000 cells, complete recovery occurred in 803 out of 1000 trials; for 26500 cells, complete recovery occurred in 10000 out of 10000 trials.\nFailure probabilities: We have purposely chosen parameters that would lead to failures, in order to check our analysis. Under our standard test case with four hash functions, we estimate the probability of failure during any single trial as 10000 · (600/30000) 4 = 1.6 × 10 −3 . Over an experiment with 10000 trials, we indeed found 16 trials with failures, and in each failure, there was just one unrecovered erroneous message symbol. Reducing to 500 errors in the IBLT reduces the failure probability to 10000 · (500/30000) 4 ≈ 7.7 × 10 −4 ; an experiment with 10000 trials led to a seven failures, each with just one unrecovered erroneous message symbol. Finally, with 5 hash functions and 600 IBLT errors, we would estimate the failure probability as 10000 · (600/30000) 5 = 3.2 × 10 −5 ; a run of 10000 trials yielded no failures.\nOur goal was to design an error-correcting code that would be extremely fast and simple for use in networking applications such as large-scale data transfer and reconciliation in cloud computing systems. While not optimal in terms of rate, the amount of redundancy used is a small constant factor more than optimal; we expect this will be suitable for many appli- cations, given the other advantages. Although we have focused on error correction of large data, Biff codes may also be useful for smaller messages, in settings where computational efﬁciency is paramount and where small block sizes were introduced at least partially to reduce Reed-Solomon decoding overheads.\nWe note that in the large data setting we can adapt the sampling technique described in [4] to estimate the number of errors E in O(log L) time with an error bound that is\nquantiﬁed in Theorem 2 of [4]. This allows the Biff code to be sized correctly to O(E) without requiring any a priori bound on E to be known in advance. For example, when two large virtual memories are to be reconciled it is difﬁcult to have a reasonable bound on the number of errors or differences. In the communications setting this is akin to estimating the channel error rate and adapting the code. However, such error rate estimation in the communication setting is done infrequently to reduce overhead. In our large data setting, the cost of estimation is so cheap that it can be done on each large data reconciliation.\nFinally, we note that modern low-density parity-check codes are sufﬁciently complex that they are difﬁcult to teach without without going through a number of preliminaries. By contrast, Biff codes are sufﬁciently simple that we believe they could be taught in an introductory computer science class, and even introductory level programmers could implement them. Beyond their practical applications, Biff codes might prove worthwhile as a gateway to modern coding techniques.\nAcknowledgments: Michael Mitzenmacher was supported by NSF grants CCF-0915922 and IIS-0964473. George Varghese was supported by NSF Grant CSR-0964395."},"refs":[{"authors":[{"name":"W. Byer"},{"name":"M. Lub"},{"name":"M. Mitzenmacher"}],"title":{"text":"J"}},{"authors":[{"name":"M. Dietzfelbinge"},{"name":"A. Goerd"},{"name":"M. Mitzenmache"},{"name":"A. Montanar"},{"name":"R. Pag"},{"name":"M. Rink"},{"name":"T. In Proceedings of ICAL"}],"title":{"text":"Tight thresholds for cuckoo hashing via XORSA pp"}},{"authors":[{"name":"D. Eppstei"},{"name":"M. T. Goodrich"}],"title":{"text":"Straggler identiﬁcation in round-trip data streams via Newton\u2019s identities and invertible Bloom ﬁlters"}},{"authors":[{"name":"D. Eppstei"},{"name":"M. T. Goodric"},{"name":"F. Uyed"},{"name":"G. Varghese"}],"title":{"text":"What\u2019s the Difference?"}},{"authors":[{"name":"M. Goodric"},{"name":"M. Mitzenmacher"}],"title":{"text":"Invertible Bloom Lookup Tables"}},{"authors":[{"name":"D. Gupt"},{"name":"S. Le"},{"name":"M. Vrabl"},{"name":"S. Savag"},{"name":"C. Snoere"},{"name":"G. Varghes"}],"title":{"text":"A"}},{"authors":[{"name":"M. Karpovsk"},{"name":"L. Leviti"},{"name":"A. Trachtenberg"}],"title":{"text":"Data veriﬁcation and reconciliation with generalized error-correction codes"}},{"authors":[{"name":"Y. Minsk"},{"name":"A. Trachtenber"},{"name":"R. Zippel"}],"title":{"text":"Set Reconciliation with Nearly Optimal Communication Complexity"}},{"authors":[{"name":"M. Mitzenmache"},{"name":"S. Vadhan"}],"title":{"text":"Why simple hash functions work: exploiting the entropy in a data stream"}},{"authors":[{"name":"M. Patrasc"},{"name":"M. Thorup"}],"title":{"text":"The power of simple tabulation hashing"}},{"authors":[{"name":"E. Pora"},{"name":"O. Lipsky"}],"title":{"text":"Improved Sketching of Hamming Distance with Error Correction"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564291.pdf"},"links":[{"id":"1569566567","weight":4},{"id":"1569564843","weight":3},{"id":"1569565383","weight":2},{"id":"1569565883","weight":3},{"id":"1569564889","weight":3},{"id":"1569566725","weight":3},{"id":"1569565377","weight":2},{"id":"1569566385","weight":3},{"id":"1569564635","weight":2},{"id":"1569565867","weight":2},{"id":"1569566799","weight":2},{"id":"1569565067","weight":2},{"id":"1569561021","weight":3},{"id":"1569564669","weight":3},{"id":"1569565691","weight":2},{"id":"1569566875","weight":12},{"id":"1569564605","weight":2},{"id":"1569559617","weight":2},{"id":"1569566981","weight":2},{"id":"1569566321","weight":2},{"id":"1569566605","weight":2},{"id":"1569566683","weight":3},{"id":"1569566855","weight":2},{"id":"1569560629","weight":2},{"id":"1569566869","weight":5},{"id":"1569566091","weight":2},{"id":"1569559259","weight":2},{"id":"1569566697","weight":3},{"id":"1569566597","weight":4},{"id":"1569565551","weight":2},{"id":"1569566761","weight":2},{"id":"1569566943","weight":2},{"id":"1569565091","weight":3},{"id":"1569566591","weight":3},{"id":"1569566571","weight":7},{"id":"1569552245","weight":2},{"id":"1569565495","weight":4},{"id":"1569567045","weight":4},{"id":"1569564481","weight":2},{"id":"1569560833","weight":3},{"id":"1569566415","weight":3},{"id":"1569564805","weight":5},{"id":"1569567005","weight":12},{"id":"1569566081","weight":3},{"id":"1569565355","weight":2},{"id":"1569564469","weight":3},{"id":"1569565931","weight":2},{"id":"1569566647","weight":2},{"id":"1569551535","weight":3},{"id":"1569566765","weight":2},{"id":"1569564897","weight":2},{"id":"1569565775","weight":3},{"id":"1569566871","weight":3},{"id":"1569566653","weight":3},{"id":"1569565461","weight":3},{"id":"1569564731","weight":2},{"id":"1569565171","weight":2},{"id":"1569566207","weight":4},{"id":"1569558325","weight":2},{"id":"1569565837","weight":2},{"id":"1569566671","weight":2},{"id":"1569566303","weight":2},{"id":"1569564233","weight":2},{"id":"1569566459","weight":2},{"id":"1569567535","weight":2},{"id":"1569563411","weight":3},{"id":"1569560427","weight":3},{"id":"1569564401","weight":3},{"id":"1569564849","weight":2},{"id":"1569565317","weight":10},{"id":"1569565123","weight":2},{"id":"1569566941","weight":2},{"id":"1569558459","weight":2},{"id":"1569565291","weight":4},{"id":"1569564203","weight":2},{"id":"1569566821","weight":3},{"id":"1569556713","weight":3},{"id":"1569562685","weight":2},{"id":"1569566467","weight":2},{"id":"1569565771","weight":3},{"id":"1569566157","weight":2},{"id":"1569566903","weight":2},{"id":"1569565859","weight":8},{"id":"1569564249","weight":11},{"id":"1569565809","weight":3},{"id":"1569566843","weight":2},{"id":"1569566579","weight":2},{"id":"1569566563","weight":3},{"id":"1569566089","weight":2},{"id":"1569559221","weight":2},{"id":"1569556091","weight":2},{"id":"1569565347","weight":2},{"id":"1569566925","weight":3},{"id":"1569564387","weight":3},{"id":"1569565455","weight":2},{"id":"1569566497","weight":3},{"id":"1569566963","weight":4},{"id":"1569561679","weight":2},{"id":"1569566709","weight":2},{"id":"1569564989","weight":2},{"id":"1569566787","weight":5},{"id":"1569566717","weight":4},{"id":"1569566015","weight":2},{"id":"1569565897","weight":7},{"id":"1569551763","weight":3},{"id":"1569565953","weight":2},{"id":"1569566895","weight":4},{"id":"1569566749","weight":3},{"id":"1569566269","weight":6},{"id":"1569564189","weight":2},{"id":"1569564195","weight":2},{"id":"1569564613","weight":2},{"id":"1569567009","weight":3},{"id":"1569566865","weight":2},{"id":"1569565321","weight":2},{"id":"1569566193","weight":6},{"id":"1569566343","weight":6},{"id":"1569564311","weight":2},{"id":"1569565785","weight":4},{"id":"1569566239","weight":2},{"id":"1569566167","weight":2},{"id":"1569566679","weight":6},{"id":"1569565989","weight":2},{"id":"1569566575","weight":3},{"id":"1569563981","weight":2},{"id":"1569561085","weight":3},{"id":"1569566617","weight":4},{"id":"1569559565","weight":3},{"id":"1569566905","weight":4},{"id":"1569566733","weight":5},{"id":"1569563307","weight":3},{"id":"1569558681","weight":2},{"id":"1569566759","weight":2},{"id":"1569559195","weight":2},{"id":"1569566149","weight":3},{"id":"1569566657","weight":3},{"id":"1569558859","weight":20},{"id":"1569565199","weight":3},{"id":"1569565365","weight":3},{"id":"1569566511","weight":2},{"id":"1569566719","weight":2},{"id":"1569565841","weight":10},{"id":"1569566369","weight":2},{"id":"1569566531","weight":2},{"id":"1569567665","weight":4},{"id":"1569561143","weight":3},{"id":"1569566581","weight":2},{"id":"1569565833","weight":3},{"id":"1569564611","weight":2},{"id":"1569565535","weight":2},{"id":"1569562867","weight":3},{"id":"1569561795","weight":20},{"id":"1569566845","weight":2},{"id":"1569566325","weight":2},{"id":"1569566423","weight":9},{"id":"1569564795","weight":2},{"id":"1569559805","weight":2},{"id":"1569566437","weight":4},{"id":"1569558901","weight":9},{"id":"1569565735","weight":2},{"id":"1569559111","weight":2},{"id":"1569566939","weight":2},{"id":"1569553537","weight":3},{"id":"1569565427","weight":2},{"id":"1569566403","weight":3},{"id":"1569565915","weight":2},{"id":"1569552251","weight":2},{"id":"1569567051","weight":2},{"id":"1569566885","weight":2},{"id":"1569564441","weight":2},{"id":"1569564209","weight":4},{"id":"1569566513","weight":11},{"id":"1569566425","weight":4},{"id":"1569554881","weight":2},{"id":"1569554971","weight":4},{"id":"1569565501","weight":2},{"id":"1569566445","weight":2},{"id":"1569566209","weight":2},{"id":"1569566649","weight":3},{"id":"1569565559","weight":2},{"id":"1569565655","weight":2},{"id":"1569566909","weight":3},{"id":"1569566127","weight":2},{"id":"1569563763","weight":3},{"id":"1569565087","weight":2},{"id":"1569566473","weight":3},{"id":"1569564857","weight":3},{"id":"1569566913","weight":3},{"id":"1569566809","weight":3},{"id":"1569566629","weight":2},{"id":"1569566257","weight":3},{"id":"1569565033","weight":7},{"id":"1569566447","weight":3},{"id":"1569565817","weight":3},{"id":"1569565847","weight":6},{"id":"1569564353","weight":3},{"id":"1569557083","weight":2},{"id":"1569566141","weight":3},{"id":"1569565633","weight":9},{"id":"1569565279","weight":4},{"id":"1569555879","weight":2},{"id":"1569566115","weight":2},{"id":"1569565219","weight":12},{"id":"1569554759","weight":4},{"id":"1569565185","weight":2},{"id":"1569566773","weight":2},{"id":"1569566037","weight":2},{"id":"1569566223","weight":3},{"id":"1569566553","weight":2},{"id":"1569564973","weight":2},{"id":"1569564969","weight":3},{"id":"1569565029","weight":2},{"id":"1569566505","weight":2},{"id":"1569565393","weight":2},{"id":"1569565933","weight":2},{"id":"1569562207","weight":2},{"id":"1569567033","weight":3},{"id":"1569565527","weight":3},{"id":"1569566853","weight":2},{"id":"1569566603","weight":10},{"id":"1569567029","weight":2},{"id":"1569566159","weight":3},{"id":"1569566695","weight":2},{"id":"1569561379","weight":2},{"id":"1569561123","weight":8},{"id":"1569565467","weight":7},{"id":"1569566655","weight":2},{"id":"1569565441","weight":2},{"id":"1569566893","weight":3},{"id":"1569566317","weight":4},{"id":"1569564097","weight":2},{"id":"1569560997","weight":8},{"id":"1569563845","weight":2},{"id":"1569566407","weight":2},{"id":"1569566501","weight":2},{"id":"1569565741","weight":3},{"id":"1569566275","weight":5},{"id":"1569566481","weight":3},{"id":"1569565545","weight":2},{"id":"1569566857","weight":3},{"id":"1569565961","weight":2},{"id":"1569560503","weight":3},{"id":"1569565463","weight":6},{"id":"1569566219","weight":6},{"id":"1569565439","weight":2},{"id":"1569566229","weight":2},{"id":"1569566949","weight":6},{"id":"1569562551","weight":2},{"id":"1569566901","weight":2},{"id":"1569551347","weight":2},{"id":"1569561623","weight":2},{"id":"1569566383","weight":4},{"id":"1569565155","weight":2},{"id":"1569566631","weight":4},{"id":"1569565571","weight":4},{"id":"1569566177","weight":3},{"id":"1569565493","weight":2},{"id":"1569564411","weight":4},{"id":"1569566805","weight":2},{"id":"1569566293","weight":2},{"id":"1569565665","weight":3},{"id":"1569566831","weight":4},{"id":"1569565549","weight":3},{"id":"1569564175","weight":2},{"id":"1569566983","weight":2},{"id":"1569566479","weight":2},{"id":"1569566431","weight":2},{"id":"1569566873","weight":3},{"id":"1569565765","weight":4},{"id":"1569565925","weight":3},{"id":"1569565263","weight":3},{"id":"1569565215","weight":2},{"id":"1569565385","weight":17},{"id":"1569565575","weight":2},{"id":"1569565919","weight":5},{"id":"1569565181","weight":3},{"id":"1569566711","weight":4},{"id":"1569565241","weight":2},{"id":"1569566927","weight":4},{"id":"1569565661","weight":2},{"id":"1569565865","weight":2},{"id":"1569566887","weight":4},{"id":"1569565273","weight":4},{"id":"1569565319","weight":4},{"id":"1569566267","weight":2},{"id":"1569564131","weight":5},{"id":"1569552037","weight":2},{"id":"1569564919","weight":5},{"id":"1569566737","weight":3},{"id":"1569566429","weight":3},{"id":"1569561221","weight":2},{"id":"1569564595","weight":16},{"id":"1569566917","weight":4},{"id":"1569566253","weight":3},{"id":"1569565353","weight":3},{"id":"1569564683","weight":2},{"id":"1569564305","weight":3},{"id":"1569566691","weight":2},{"id":"1569565421","weight":2},{"id":"1569566547","weight":2},{"id":"1569566651","weight":2},{"id":"1569566823","weight":2},{"id":"1569566137","weight":2},{"id":"1569566237","weight":3},{"id":"1569566529","weight":2},{"id":"1569565375","weight":2},{"id":"1569566715","weight":2},{"id":"1569566639","weight":2},{"id":"1569565041","weight":2},{"id":"1569564703","weight":3},{"id":"1569565541","weight":2},{"id":"1569566813","weight":4},{"id":"1569566771","weight":2},{"id":"1569562277","weight":2},{"id":"1569566641","weight":5},{"id":"1569564247","weight":2},{"id":"1569564437","weight":8},{"id":"1569566533","weight":2},{"id":"1569551905","weight":2},{"id":"1569564861","weight":3},{"id":"1569565457","weight":9},{"id":"1569566487","weight":3},{"id":"1569565529","weight":3},{"id":"1569556759","weight":2},{"id":"1569566619","weight":2},{"id":"1569561185","weight":2},{"id":"1569566075","weight":7},{"id":"1569566397","weight":2},{"id":"1569565233","weight":10},{"id":"1569565593","weight":2},{"id":"1569566817","weight":2},{"id":"1569564157","weight":4},{"id":"1569566389","weight":2},{"id":"1569567483","weight":2},{"id":"1569564923","weight":2},{"id":"1569565367","weight":2},{"id":"1569566299","weight":2},{"id":"1569564281","weight":2},{"id":"1569565039","weight":2},{"id":"1569565769","weight":4},{"id":"1569565805","weight":4},{"id":"1569566933","weight":5},{"id":"1569566577","weight":3},{"id":"1569559919","weight":3},{"id":"1569565861","weight":2},{"id":"1569566147","weight":5},{"id":"1569565537","weight":2},{"id":"1569566057","weight":2},{"id":"1569560785","weight":4},{"id":"1569565561","weight":3},{"id":"1569560213","weight":3},{"id":"1569555891","weight":2},{"id":"1569565997","weight":3},{"id":"1569565035","weight":4},{"id":"1569564961","weight":3},{"id":"1569559251","weight":5},{"id":"1569565737","weight":3},{"id":"1569560459","weight":2},{"id":"1569565853","weight":3},{"id":"1569550425","weight":2},{"id":"1569564123","weight":4},{"id":"1569565889","weight":3},{"id":"1569566635","weight":3},{"id":"1569566611","weight":2},{"id":"1569551539","weight":2},{"id":"1569564505","weight":2},{"id":"1569565165","weight":8},{"id":"1569565565","weight":4},{"id":"1569565635","weight":2},{"id":"1569561397","weight":2},{"id":"1569565731","weight":2},{"id":"1569566797","weight":7},{"id":"1569566413","weight":2},{"id":"1569565707","weight":2},{"id":"1569565113","weight":3},{"id":"1569566375","weight":3},{"id":"1569564257","weight":2},{"id":"1569565583","weight":4},{"id":"1569566555","weight":2},{"id":"1569565373","weight":20},{"id":"1569566973","weight":5},{"id":"1569561579","weight":2},{"id":"1569566987","weight":2},{"id":"1569565031","weight":3},{"id":"1569566839","weight":2},{"id":"1569551751","weight":2},{"id":"1569565139","weight":3},{"id":"1569566663","weight":3},{"id":"1569565579","weight":2},{"id":"1569566067","weight":4},{"id":"1569566825","weight":2},{"id":"1569566241","weight":2},{"id":"1569563007","weight":2},{"id":"1569566113","weight":4},{"id":"1569566443","weight":2},{"id":"1569566727","weight":5},{"id":"1569565315","weight":2},{"id":"1569560581","weight":3},{"id":"1569559233","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T6.2","endtime":"15:20","authors":"Michael Mitzenmacher, George Varghese","date":"1341241200000","papertitle":"Biff (Bloom Filter) Codes: Fast Error Correction for Large Data Sets","starttime":"15:00","session":"S3.T6: Codes and Their Applications","room":"Kresge Rehearsal A (033)","paperid":"1569564291"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"0","spectral14":"5","spectral20":"14","spectral9":"4","spectral3":"1","spectral17":"5","louvain":"479","spectral10":"7","spectral15":"13","spectral5":"3","spectral8":"4","spectral11":"7","spectral4":"0","spectral12":"4","spectral19":"2","spectral7":"2","spectral13":"5","spectral18":"13","spectral2":"0","spectral16":"10"}}
