{"id":"1569559233","paper":{"title":{"text":"SISO MAP Decoding of Rate-1 Recursive Convolutional Codes: A Revisit"},"authors":[{"name":"Yonghui Li"},{"name":"Md. Shahriar Rahman"},{"name":"Branka Vucetic"}],"abstr":{"text":"Abstract\u2014 In this paper, we revisit the BCJR soft-input soft- output (SISO) maximum a posteriori probability (MAP) decoding process of rate-1 recursive convolutional (RC) codes. From this we establish some interesting duality properties between encoding and decoding of RC codes. We observe that the forward and backward BCJR decoders can be simply represented by their dual SISO channel encoders using shift registers in the complex ﬁeld. Similarly, the bidirectional MAP decoding can be implemented by linearly combining the outputs of the dual SISO encoders of the respective forward and backward decoders."},"body":{"text":"Convolutional codes have been widely used in various modern communications systems. Its popularity stems from its simple encoder structure. The main complexity associated with systems using convolutional coding is situated in the decoder. Various decoding algorithms have been developed to achieve the optimal decoding performance in the most efﬁcient manner. There are two commonly used decoding algorithms, the Viterbi algorithm (VA) [1-4] and the maximum a posteriori probability (MAP) algorithm, originally proposed by Bahl, Cocke, Jelinek and Raviv (BCJR) in 1974 [6]. The VA algorithm essentially consists of ﬁnding an optimal path in a trellis based graph and produces the minimum sequence error probability. In the standard VA, the decoder delivers hard-decision outputs, which are the estimates of transmitted binary information symbols. In contrast to the VA, the MAP algorithm performs symbol-by-symbol decoding and achieves the optimal symbol error probability.\nThe BCJR algorithm has been forgotten for almost a decade due to its increased complexity compared to the VA algorithm, and was re-discovered in early 90\u2019s as an optimal decoding algorithm for a class of capacity-approaching concatenated codes with random interleaving, such as turbo codes and serially concatenated codes. Though the VA algorithm can be modiﬁed to deliver not only the most-likely binary signal sequence, but also the soft output containing the a posteriori probabilities (APPs) of transmitted binary symbols [5, 7], only the MAP algorithm achieves the optimal BER performance. The MAP algorithm has now become a widely used decoding algorithm in communication systems.\nThe BCJR decoding algorithm is a bidirectional decoding process, consisting of a forward and a backward recursion process, which dominates the main complexity of the MAP decoder. In each direction, the decoder infers the probabilities of current states and information symbols based on the proba- bilities of previous and next states in the forward and backward\ntrellis, the received signals, and the a priori probabilities of the transmitted signals. The complexity of forward and backward recursion exponentially increases with the constraint length of convolutional codes.\nIn this paper, we revisit the forward, backward and bidirec- tional BCJR decoding of rate-1 recursive convolutional (RC) codes. We observe some duality properties between a SISO forward/backward MAP decoder of a RC code and its encoder. The forward and backward decoder of a rate-1 RC code can ac- tually be represented by its corresponding dual encoder using shift registers in the complex ﬁeld. This signiﬁcantly reduces the the original exponential computational complexity of MAP forward and backward recursion to the linear complexity. Sim- ilarly the bidirectional BCJR decoding can be implemented by linearly combining the outputs of the dual SISO encoders of the respective forward and backward decoders. With logarithm of the soft coded symbol estimate, obtained from the received signals, as the input to the dual encoder, the dual encoder output produces the logarithm of the soft symbol estimates of the binary information symbols. All the theorems presented in the paper have been rigorously proved and validated by simulations. Due to the space limitation, we will focus on the rate-1 RC codes and only present the key ﬁndings about their encoding and decoding duality and omit their proofs. The detailed proof and extensions to other classes of rate-1 convolutional codes can be found in [8].\nThe remainder of the paper is organized as follows. In Section II, we ﬁrst brieﬂy review the BCJR forward decoding algorithm and derive the dual encoder structures of forward MAP decoder. The duality for backward decoding is presented in Section III. The representation of bidirectional MAP decod- ing by using the derived dual encoder structures of forward and backward decoding is described in Section IV. Conclusions are drawn in Section V.\nIn this section, we ﬁrst revisit the forward BCJR decoding algorithm. We will focus on the decoding of a class of rate-1 RC codes. Let a(x) = x n + a n −1 x n −1 + · · · + a 1 x 1 + 1 and q(x) = x n + q n −1 x n −1 + · · · + q 1 x 1 + 1, where n is called the degree of monic polynomials a(x) and q(x). We deﬁne a convolutional code, generated by g RC (x) = a(x)/q(x) as a recursive convolutional (RC) code. For a given polynomial a(x), we deﬁne its minimum complementary polynomial as the\nSince a(x) = x n + · · ·+a 1 x+1 always divides x 2 n −1 +1, the minimum complementary polynomial of a(x) always exists.\nLet b = (b 1 , b 2 , . . . , b K ) be a binary information symbol sequence to be transmitted, where K is the frame length. Let c = (c 1 , c 2 , . . . , c K ) be the binary codeword of b, generated by the code generator polynomial g RC (x), and x = (x 1 , x 2 , . . . , x K ) be the modulated symbol sequence of c. For simplicity, we consider a BPSK modulation. Let y = (y 1 , y 2 , . . . , y K ) represent the received signal sequence at the output of channel.\nLet p c k (l) = p(c k = l |y k ), l = 0, 1, denote the a posteriori probabilities (APP) of the encoded symbol c k = l, given the received signal y k , where c k is the transmitted binary coded symbol at time k. Let us fur- ther denote P c = {(p c 1 (0), p c 1 (1)), · · · , (p c k (0), p c k (1)), · · · , (p c K (0), p c K (1)) }. Now let us follow the BCJR MAP forward decoding algorithm to use P c to calculate the APPs of binary information symbols b k . Let −→ p b k (w) = p(b k = w |y 1 ∼k ) represent the probability of information symbol b k = w, w=0, 1, given the received signals y 1 ∼k = {y 1 , · · · , y k }. It can be\ncalculated in the following recursive way −→ p b\nwhere U (b(k) = w) is the set of trellis branches from the state m \u2032 at time k-1 to the state m at time k, that are caused by the input binary symbol b(k) = w, and c k (m \u2032 , m) represents the encoder output of the corresponding trellis branch.\n· · · , −−→ ˆ x b K ) denote the soft symbol estimate (SSE) sequence of codeword c and SSE of information sequence b in the forward decoding, respectively. We assume that 0 and 1 are modulated into the symbol 1 and -1. Then the soft symbol estimates ˆ x c k and −→\nˆ x b k , which represent the probabilistic average of estimates of symbols x c k and x b k given y, can be calculated as\nWe deﬁne the decoder with the input and output being the logarithm of the soft symbol estimates (SSE) of the coded symbols and SSEs of the information symbols, as the Log- domain soft-input-soft-output (SISO) decoder. As shown in Fig. 1, the SISO decoder can be implemented by adding a\nlogarithm module and an exponential module at the front and rear end of the log-domain SISO decoder, respectively. We should note that in the logarithm operation, for a real number x, if x < 0, lnx = ln |x| + jπ. Hence the logarithm of a SSE is real number when it is positive, and otherwise is a complex number.\nThen for a rate-1 recursive convolutional code, its encoding and the log-domain SISO forward decoding have the following duality property.\nTheorem 1 - Encoding and forward decoding duality of a rate-1 recursive convolutional (RC) code : For a RC code, generated by a generator polynomial g RC (x) =\n, let z(x) be the degree-l minimum complementary polynomial of a(x). The log-domain SISO forward decoding of the RC code can be simply implemented by its dual encoder with the generator polynomial of\n1 x + 1 x n+l + 1\n1 x x n+l + 1\nwhere q(x)z(x) = x n+l + h n+l −1 x n+l −1 + · · ·+h 1 x + 1. The relationship of a binary encoder and its dual encoder is shown in Fig. 1.\nFrom Theorem 1 and Fig. 1, we can see that the forward decoding of a RC code can be easily implemented by its dual encoder using shift-registers.\nIn this section, we investigate the MAP backward decoding of rate-1 convolutional codes and derive its dual encoder struc- ture. Before discussing the backward decoding, we ﬁrst deﬁne a reverse memory-labeling encoder of a recursive convolution- al (RC) code. Given the encoder of a RC code with rational\n, if we change the labeling of the k-th shift register in the encoder from S k to S n+1 −k , and change their respective feed-forward coefﬁcient from a k to a n −k , k=1, 2, . . . , n, and feedback coefﬁcients from b k to b n −k , k=1, 2, . . . , n, we will derive an encoder with a new trellis. The resulting encoder is referred to as the reverse memory-labeling encoder of g RC (x). Figs. 3(a) and 3(b) show the encoder and the reverse memory-labeling encoder of g RC (x).\nIn the BCJR backward decoding, the decoder calculates the APP of information symbol b k based on received signals y k ∼K = {y k , · · · , y K } as follows\nwhere ˆ U (b(k) = w) is the set of trellis branches from the state m at time k to the state m \u2032 at time k + 1, that are caused by the input binary symbol b(k) = w, and c k (m \u2032 , m) represents the encoder output of the corresponding trellis branch.\nIn backward decoding, the received signals are decoded backward in a time-reverse order. Thus given the received signal sequence y = (y 1 , y 2 , . . . , y K ), the order of signals to be decoded is from y K , y K −1 , till y 1 . In order to decode the received signals backward, the decoder has to follow the trellis in a reverse direction. For the decoder with the backward trellis, the input to the decoder is at the right hand side of the decoder and its output is at the left hand side, which operates in a reverse direction of the conventional decoder. If we describe the backward trellis using corresponding forward representation, where the decoder input and output are changed to the conventional order, we will derive a new trellis. Then the relationship of the encoders for backward trellis and its corre- sponding forward trellis representation can be summarized in the following theorem.\n, the forward representation of its backward trellis can be implemented by its reverse memory- labeling encoder of the same generator polynomial g RC (x).\nProof is omitted due to the space limitation. From Theorem 2, we know that the log-domain SISO forward decoding of a given recursive convolutional encoder with a generator polyno- mial g RC (x) = a(x) q(x) can be implemented by its dual encoder with the generator polynomial q RC (x) = q(x)z(x) a(x)z(x) , where z(x) is the degree-l minimum complementary polynomial of a(x). Then according to Theorem 2, the log-domain SISO backward decoding of the RC code can be implemented by the reverse memory-labeling encoder of q RC (x). By combining Theorems 1 and 2, we can obtain the backward decoding duality, which is summarized in the following Theorem.\nTheorem 3 - Backward decoding duality of a recursive convolutional (RC) code: We consider a rate-1 recursive con-\n. Let z(x) be the degree-l minimum complementary polynomial of a(x). Its log-domain SISO backward decoding can be implemented by its dual encoder with reverse memory-labeling and the generator polynomial of\n1 x x n+l + 1\nFrom Theorem 1 and 3, we can note that the dual encoders for the forward and backward decoding are generated by the same polynomial. The only difference is that the dual encoder for the forward decoding uses the conventional convolutional encoder structure, but that for the backward decoding uses the reverse memory labelling encoder.\nIn the previous two sections, we have introduced the duality of channel encoding and SISO MAP forward/backward decod- ing. Based on the derived encoding-decoding duality proper- ties, in this section, we represent the bidirectional BCJR MAP decoder by linearly combining outputs of the dual encoders for the forward and backward decoders. By comparing the bidirectional BCJR MAP decoding outputs with the forward and backward dual encoder outputs, combining coefﬁcients are identiﬁed in a way such that the resulting combined forward and backward dual encoder outputs are exactly the same as the bidirectional MAP decoding outputs. In this paper, we\nfound the expressions of these combining coefﬁcients for some commonly used 4-state and 8-state RC codes. The expressions for higher-states codes can be obtained in the same way.\nLet us ﬁrst call the dual encoder of the forward and backward decoding as the forward dual encoder and backward dual encoder , respectively. Let\nb k represent the soft outputs of the forward and backward dual encoders. They can be calculated based on Theorems 1-3 in Sections II and III, respectively. Then the SISO MAP decoder output, x b k , can be represented as the following linear combination of forward and backward dual encoder outputs,\napplied to the forward and backward dual encoder outputs, respectively. The combining coefﬁcients for some 4-state and 8-state codes are shown below.\nThe combining coefﬁcients for the 4 states RC code [5/7] 8 can be calculated as\nwhere ˆ x c k is the soft symbol estimate of the received encoded symbol c k , where k = 1, 2, ..., K, K is the frame of codeword, and\nwhere ⌈x⌉ is the ceiling operation, representing the smallest integer not less than x.\nSimilarly, for the 8-state code [15/13] 8 , their forward and backward combining coefﬁcients can be calculated as\n(19) and\nN d,l = ˆ x c 7(l −1)+d , d = 0, 1, 2, 3, 4, 5, 6, \t (20) l = 0, 1, 2, 3, ..., ⌈K/7⌉ + 1 \t (21)\nwhere N d,l = 1, if the subscript of ˆ x c is less than one and greater than K.\nThe BER performance of these 4-state and 8-state rate-1 RC codes have been veriﬁed through simulations and they have shown that the proposed bidirectional dual encoder structures have exactly the same performance as the optimal MAP decoding.\nIn this paper, we revisited the BCJR forward and backward decoding process for the rate-1 recursive convolutional codes. Dual encoder structures of forward and backward decoding for rate-1 GC codes are derived. The input to the dual encoder is the logarithm of soft symbol estimates of the coded symbols obtained from the received signals, and the dual encoder output produces the logarithm of the soft symbol estimates of the information symbols. For a RC code, generated by a gener- ator polynomial g RC (x) = a(x) g(x) , the forward and backward decoding can be implemented by their corresponding dual encoders, which are generated by the polynomial, 1 g\n, where z(x) is the minimum complementary polynomial of a(x). The derived duality property signiﬁcantly reduced the the exponential computational complexity of MAP forward and backward recursion to a linear one with respect to the code constraint length. Similarly, the bidirectional MAP decoder of a rate-1 RC code can be implemented by linearly combining the outputs of dual encoders for the forward and backward decoding. The proposed decoding algorithm achieves exactly the same performance as the optimal BCJR decoding algorith- m with less complexity.\nIn this paper, we have only focused on a class of rate- 1 RC codes. Its signiﬁcance is mainly as component codes in concatenated coding schemes, such as turbo coding. The duality properties derived in this paper can also be applied to other rate-1 codes and applications."},"refs":[{"authors":[{"name":"S. Li"},{"name":"D. J. Costell"}],"title":{"text":"Error Control Coding, 2nd ed"}},{"authors":[{"name":"A. Viterb"}],"title":{"text":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,\u201d IEEE Trans"}},{"authors":[{"name":"G. D. Forne"}],"title":{"text":"The Viterbi algorithm\u201d, Proc"}},{"authors":[{"name":"G. D. Forne"}],"title":{"text":"Convolutional codes II: Maximum-likelihood decoding,\u201d Inform"}},{"authors":[{"name":"J. Hagenaue"},{"name":"P. Hoehe"}],"title":{"text":"A Viterbi algorithm with soft-decision outputs and its applications,\u201d Proc"}},{"authors":[{"name":"L. Bah"},{"name":"J. Cock"},{"name":"F. Jeline"},{"name":"J. Ravi"}],"title":{"text":"Optimal decoding of linear codes for minimizing symbol error rate,\u201d IEEE Transactions on Information Theory, vol"}},{"authors":[{"name":"J. Hagenaue"},{"name":"E. Offe"},{"name":"L. Papk"}],"title":{"text":"Iterative decoding of binary block and convolutional codes,\u201d IEEE Trans"}},{"authors":[{"name":"Y. L"},{"name":"M. Rahma"},{"name":"B. Vuceti"}],"title":{"text":"Duality of channel encoding and cecoding - Part I: rate-1 convolutional codes,\u201d submitted, available online http://arxiv"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569559233.pdf"},"links":[{"id":"1569565691","weight":11},{"id":"1569559617","weight":11},{"id":"1569565613","weight":5},{"id":"1569564897","weight":5},{"id":"1569566167","weight":5},{"id":"1569565257","weight":5},{"id":"1569564353","weight":5},{"id":"1569558401","weight":5},{"id":"1569566927","weight":11},{"id":"1569565177","weight":11},{"id":"1569565829","weight":11},{"id":"1569566457","weight":5},{"id":"1569565889","weight":5},{"id":"1569566635","weight":5},{"id":"1569566611","weight":5},{"id":"1569566375","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T6.3","endtime":"15:40","authors":"Yonghui Li, Md. Shahriar Rahman, Branka Vucetic","date":"1341501600000","papertitle":"SISO MAP Decoding of Rate-1 Recursive Convolutional Codes: A Revisit","starttime":"15:20","session":"S13.T6: Convolutional and Turbo Codes","room":"Kresge Rehearsal A (033)","paperid":"1569559233"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
