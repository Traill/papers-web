{"id":"1569565393","paper":{"title":{"text":"Variable-Length Extractors"},"authors":[{"name":"Hongchao Zhou"},{"name":"Jehoshua Bruck"}],"abstr":{"text":"Abstract\u2014We study the problem of extracting a prescribed number of random bits by reading the smallest possible number of symbols from non-ideal stochastic processes. The related interval algorithm proposed by Han and Hoshi has asymptotically optimal performance; however, it assumes that the distribution of the input stochastic process is known. The motivation for our work is the fact that, in practice, sources of randomness have inherent correlations and are affected by measurement\u2019s noise. Namely, it is hard to obtain an accurate estimation of the distribution. This challenge was addressed by the concepts of seeded and seedless extractors that can handle general random sources with unknown distributions. However, known seeded and seedless extractors provide extraction efﬁciencies that are substantially smaller than Shannon\u2019s entropy limit. Our main contribution is the design of extractors that have a variable input- length and a ﬁxed output length, are efﬁcient in the consumption of symbols from the source, are capable of generating random bits from general stochastic processes and approach the information theoretic upper bound on efﬁciency."},"body":{"text":"The problem of generating random bits from imperfect ran- dom sources, essential for many randomized applications [8], has been extensively studied. It dates back to von Neumann [15] in 1951, who ﬁrst considered the problem of simulating unbiased coins by using a biased coin with unknown probabil- ity. The optimal algorithms were later derived by Elias [3] and Peres [10]. In those algorithms the expected number of random bits generated is asymptotically equal to the entropy of source. In 1986, Blum [1] studied the problem of generating random bits from a correlated source, speciﬁcally, he considered ﬁnite Markov chains. We generalized Blum\u2019s method and proposed the ﬁrst known algorithm that generates random bits from an arbitrary Markov chain, runs in expected linear time and achieves the information-theoretic upper bound on efﬁciency [16]. However, the above algorithms, although very efﬁcient, only work for blocks, namely, input sequences with ﬁxed length, and they generate variable numbers of random bits. In many occasions, it requires to generate a prescribed number of random bits by reading the smallest possible number of symbols from the source. In [17], we introduced an optimal algorithm that generates a prescribed number of random bits for an arbitrary biased coin with unknown bias. In [5], Han and Hoshi proposed an interval algorithm for generating a prescribed number of random bits from any known stochastic process (whose distribution is given) with an optimal perfor- mance. Generally, given an arbitrary biased coin or a known\nstochastic process (an ideal source), it is know how to generate a certain number of random bits very efﬁciently. But those approaches can not be easily generalized for non-ideal sources that have unknown inherent correlations and are affected by measurement\u2019s noise.\nExtracting randomness from non-ideal sources has been an active research topic in the last two decades. In 1990, Zuck- erman introduced a general model of weak random sources, called k-sources, namely whose min-entropy is at least k. It was shown that given a source on {0, 1} n with min-entropy k < n, it is impossible to devise a single function that extracts even one bit of randomness. This observation led to the introduction of seeded extractors, which using a small number of additional truly random bits as the seed (catalyst). When simulating a probabilistic algorithm, one can simply eliminate the requirement of truly random bits by enumerating all the possible strings for the seed and taking a majority vote on the ﬁnal results. There are a variety of very efﬁcient constructions of seeded extractors, summarized in [2], [9], [13]. On the other hand, people study seedless extractors for some speciﬁc classes of random sources, including independent sources [11], bit-ﬁxing sources [6], and samplable sources [7]. Almost all known constructions of seeded or seedless extractors have ﬁxed input length and ﬁxed output length, hence, we call them ﬁxed-length extractors. For many weak random sources, the maximal number of random bits extracted based on a ﬁxed-length construction is upper bounded by the source\u2019s min-entropy. However, the information theoretic limit for randomness extraction is the source\u2019s entropy, which is larger than the min-entropy. The concept of min-entropy and entropy is deﬁned as follows.\nDeﬁnition 1. Given a random source X on {0, 1} n , the min- entropy of X is deﬁned as\nExample 2. Let X be a random variable such that P [X = 0] = 0.9 and P [X = 1] = 0.1, then H min (X) = 0.152 and\nH(X) = 0.469. In this case, the entropy of X is three times its min-entropy. \t 2\nIn this paper, we focus on the notion and constructions of variable-length extractors, namely, extractors with variable input-length and ﬁxed output-length. Here, we would like to ﬁx the output length because the demand of random bits is application-dependent and usually ﬁxed. The input length can be variable because many natural sources for randomness ex- traction are stochastic processes. So our goal is to extract a pre- scribed number of random bits from a slightly-unpredictable stochastic process by reading the smallest possible number of symbols. Since the source may not be stationary ergodic, we deﬁne the efﬁciency η of a variable-length extractor as the asymptotic ratio between its output length and the entropy of its input sequence, which is upper bounded by 1.\nGiven a real stochastic process R, we use β indicate its non- idealness, deﬁned by the minimum distance between R and ideal sources (such as stationary ergodic processes). This β is an important parameter, which can be easily estimated in real applications. In this paper, we introduce several constructions of variable-length extractors, whose efﬁciencies can reach η ≥ 1 − β. For instance, let R be an arbitrary independent process such that the probability of each bit is slightly un- predictable, i.e., P [x i = 1] ∈ [0.9, 0.91] for i ≥ 1. For this source R, β ≥ 0.0315, hence, we can construct a variable- length extractor with efﬁciency at least 0.9685, very close to the theoretical upper bound 1. But the existing constructions of seeded or seedless extractors can only achieve 0.3117 on efﬁciency. In general, variable-length extractors are proposed for two purposes: they are generalizations of the algorithms for ideal sources (as those proposed in [17] and [5]) to manage general noisy sources; and they are improvements of ﬁxed- length extractors for ﬁlling the gap between min-entropy and entropy on efﬁciency.\nThe remainder of this paper is organized as follows. Section II goes over some preliminaries and lists some related results. Section III, Section IV and Section V present and analyze different constructions of variable-length extractors. Due to space limitation, we omit some of the proofs.\nStatistical Difference is used in computer science to measure the difference between two distributions. Let X and Y be two random sequences with range {0, 1} m , then the statistical difference between X and Y is deﬁned as\nover a boolean function T . We say that X and Y are ϵ- close if ∥X − Y ∥ ≤ ϵ. In this paper, we want to extract m almost-random bits such that they are ϵ-close to the uniform distribution U m on {0, 1} m with speciﬁed small ϵ > 0.\nSeeded-extractors were introduced to extract randomness from a single source by using a small number additional truly random bits [9], [13]. A seeded extractor is a function\nsuch that for every distribution X on {0, 1} n with H min (X) ≥ k, the distribution E(X, U d ) is ϵ-close to the uniform distri- bution U m . Here, d is the seed length, and we call such an extractor as a (k, ϵ) extractor. There are a lot of works focusing on constructions of seeded-extractors. A standard application of the probabilistic method [12] shows that there exists a seeded-extractor that can extract asymptotically H min (X) ran- dom bits with log(n −H min (X)) additional truly random bits. Recently, Guruswami, Umans and Vadhan [4] provided an explicit construction of seeded-extractors, whose efﬁciency is very close to the bound obtained based on the probabilistic method. Their main result is described as follows:\nLemma 3. [4] For every constant α > 0, and all positive integers n, k and all ϵ > 0, there is an explicit construction of a (k, ϵ) extractor E : {0, 1} n × {0, 1} d → {0, 1} m with d ≤ log n + O(log(k/ϵ)) and m ≥ (1 − α)k.\nsuch that given a real source R, the output sequence is ϵ- close to the uniform distribution U m . Here, S p is the set of all possible input sequences. It forms a preﬁx-free code such that for any sequence y ∈ {0, 1} ∞ , there is exactly one sequence x ∈ S p such that x is a preﬁx of y. The general procedure of extracting randomness by using a variable-length extractor can be divided into two steps:\n1) First, we read bits from the source R one by one until the current input sequence x is a sequence in S p . In this case, we construct a function\nto map the current input sequence into a binary sequence of ﬁxed length. Our goal is to get a random sequence Z with length n and min-entropy k.\nto the random sequence Z, we extract a sequence of m almost- random bits that is ϵ-close to the uniform distribution U m .\nWe can see that the construction of a variable-length ex- tractor is a cascade of a function V and a seeded extractor E, namely,\nNote that our requirement is to extract m almost-random bits that are ϵ-close to U m . According to the constructions of seeded extractors, see Lemma 3, the value of k can be\npredetermined by m and ϵ. So the key of constructing variable- length extractors is to ﬁnd the input set S p and the function V\nsuch that the min-entropy of the random sequence Z is at least k and the expected length of the elements in S p is minimized.\nFor some speciﬁc types of sources, including independent sources and samplable sources, by applying the ideas in [11] and [7] we can remove the requirement on seed without degrading the asymptotic performance. As a result, we have seedless variable-length extractors. For example, if R is an independent source, we can ﬁrst apply the method in [11] to extract d almost-random bits from the ﬁrst Θ(log m ϵ ) bits, and then use them as the seed of a seeded variable-length extractor to extract randomness from the rest bits of the process. Due to space limitation, in this paper we mainly focus on seeded constructions of variable-length extractors.\nThe main idea of constructing variable-length extractors is base on model approximation, namely, given a real source R, we use a simple model M to approximate it and then based on which we construct the extractor. The performance of the resulting variable-length extractor strongly depends on the difference between R and M. In this paper, we use β t ( R, M) to measure the distance between the source R and the model M, deﬁned by\nwhere t is a constant, P R (x) is the probability of generating x from R when the sequence length is |x|, P M (x) is the probability of generating x from M when the sequence length is |x|, and the term log 2 1 P M (x) is used for normalization. Then\nfor any source R and model M. Note that β t ( R, M) is a non-increasing function of t. In our applications, we only care about those input sequences in S p , so we consider\nwhere t ∗ = min x ∈S p |x|, namely, only the sequences that reach a certain length. According to this deﬁnition, for any sequence x ∈ S p ,\nIf M is an ideal process, then β reﬂects the non-idealness of the real source R, which can be easily estimated in real applications without knowing P R (x).\nExample 4. Let x 1 x 2 ... ∈ {0, 1} ∗ be a sequence generated from an arbitrary indepedent source R such that\nIf we let M be a biased coin with probability 0.8132, then β t ( R, M) = β(R, M)\nIn our construction, n = Θ(k) and the seed length d is very small. To consider the performance of a construction, we are interested in the expected cost (we ignore d because it can be treated as ﬁxed in our constructions), so we deﬁne its efﬁciency as\nsuch that the output sequence is ϵ-close to the uniform distribution U m on {0, 1} m , where m is the output length and H R (X m ) is the entropy of the input sequence X m .\nLemma 5. For any construction of variable-length extractors with d = o(m), its efﬁciency η ≤ 1.\nIf R is a stationary ergodic process, we can deﬁne its entropy rate as\nwhere X l is a random sequence of length l generated from the source R. In this case, the entropy of the input sequence is proportional to the expected input length.\nLemma 6. Given a stationary ergodic source R, let X m be the input sequence of a variable-length extractor with output length m. Then\nIn this section, we consider those sources that can be approximated by a known stochastic process M. Here, we say a process M is known if its distribution is given, i.e., P M (x) can be easily calculated for any x ∈ {0, 1} ∗ . Note that this process M is not necessary to be stationary or ergodic. For instance, M can be an independent process z 1 z 2 ... ∈ {0, 1} ∗ such that\nOur goal is to extract randomness from an imperfect random source R. The problem is that we don\u2019t know the exact dis- tribution of R, but we know that it can be approximated by a known process M, speciﬁcally, we assume that β(R, M) ≤ β for a constant β. Then we can use the knowledge about M and β to construct a variable-length extractor. As a result, we have the following procedure to extract m almost-random bits.\nConstruction 7. Assume the real source is R, and there exists a known stochastic process M such that β(R, M) ≤ β for a constant β.\n1) Read input bits one by one from R until we get an input sequence x ∈ {0, 1} ∗ such that\nwhere k is an integer such that there exists a ﬁxed-length (k, ϵ) extractor that generates a random sequence of length m from a source with min-entropy k.\n2) Let n be the maximum length of all the possible input sequences, then\nIf |x| < n, we can extend the length of x to n by adding n − |x| trivial zeros at the end. Since x is randomly generated, from the above procedure, we get a random sequence Z of length n.\n3) Applying a (k, ϵ) extractor to Z yields a binary sequence of m that is ϵ-close to the uniform distribution U m . 2\nThe following example is provided for comparing this construction with ﬁxed-length constructions.\nExample 8. Let M be a biased coin with probability 0.8 (of being 1). If k 1 −β = 0, then we can get the input set\nIn this case, the expected input length is strictly smaller than 7. For ﬁxed-length constructions, to get a random sequence with min-entropy at least 2, we have to read 7 input bits independent of the content. It is less efﬁcient than the former method. \t 2\nTheorem 9. Construction 7 generates a random sequence of length m that is ϵ-close to the uniform distribution U m .\nTheorem 10. Given a real source R such that there exists a known stochastic process M with β(R, M) ≤ β, then the efﬁciency of Construction 7 is 1 − β ≤ η ≤ 1.\nProof: We only need to show that η ≥ 1−β. According to Lemma 3, as m → ∞, to make ϵ → 0, we have lim m →∞ k m = 1.\nNow, let\u2019s consider the number of elements in S p , namely, |S p |. To calculate |S p |, we let\nwhere x[1 : |x| − 1] is the preﬁx of x with length |x| − 1, then for all y ∈ S \u2032 p , log 2 1 P\nWe see that the gap β on efﬁciency in the above theorem is introduced by the difference between the source R and the known stochastic process M. In some sense, it reﬂects the model uncertainty of the real source R.\nIn this section, we use a general ideal model such as a biased coin or a Markov chain to approximate the real source R. Here, we don\u2019t care about the speciﬁc parameters of the ideal model. The reason is, in some cases, the source R is very close to an ideal source but we cannot (or don\u2019t want to) estimate the parameters accurately. As a result, we introduce a construction by exploring the characters of biased coins or Markov chains. For simplicity, we only discuss the case that the ideal model is a biased coin, and the same idea can be generalized when the ideal model is a Markov chain. Let G b.c. denote the set consisting of all the models of biased coins with different probabilities, then the following procedure is provided to extract m almost=random bits.\nConstruction 11. Assume the real source is R such that min M ∈G b.c. β( R, M) ≤ β for a constant β.\n1) Read input bits one by one from R until we get an input sequence x ∈ {0, 1} ∗ such that\nwhere k 0 is the number of zeros in x and k 1 is the number of ones in x. k is an integer such that there exists a ﬁxed-length (k, ϵ) extractor that generates a random sequence of length m from a source with min-entropy k.\n2) Since the input sequence x can be very long, we map it into a sequence z of ﬁxed length n such that\n3) Applying a (k, ϵ) extractor to Z yields a random se- quence of m that is ϵ-close to the uniform distribution U m . \t 2\nLet 1 a denote the all-one vector of length a, then we get the following result.\nTheorem 12. Construction 11 generates a random sequence of length m that is ϵ-close to the uniform distribution U m if P R (1 a ), P R (0 a ) ≤ 2 −k for a = 2 ⌊ k 1 −β ⌋ .\nTheorem 13. Given a real source R such that min M ∈G b.c. β( R, M) ≤ β. If there exists a model M ∈ G b.c. with probability p ≤ 1 2 of being 1 or 0 such that\nIn this section, we consider imperfect sources that are approximately stationary and ergodic. In [14], Visweswari- ah, Kulkarni and Verd´u showed that optimal variable-length source codes asymptotically achieve optimal variable-length random bits generation in the sense of normalized divergence. Although their work only focuses on ideal stationary ergodic processes and generates \u2018weaker\u2019 random bits, it motivates us to combine universal compression with ﬁxed-length extractors for efﬁciently extracting random bits from noisy stochastic processes. In this section, we will ﬁrst introduce Lempel-Ziv code and then present its application in constructing variable- length extractors.\nLempel-Ziv code is a universal data compression scheme introduced by Ziv and Lempel [18], which is simple to implement and can achieve the asymptotically optimal rate for stationary ergodic sources. The idea of Lempel-Ziv code is to parse the source sequence into strings that haven\u2019t appeared so far, as demonstrated by the following example.\nExample 14. Assume the input is 010111001110000..., then we parse it as strings 0, 1, 01, 11, 00, 111, 000, ..., where each string is the shortest string that never appear before. That means all its preﬁxes have occurred earlier.\nLet c(n) be the number of strings obtained by parsing a sequence of length n. For each string, we describe its location with log c(n) bits. Given a string of length l, it can described by (1) the location of its preﬁx of length l − 1, and (2) its last bit. Hence, the code for the above sequence is\nTypically, Lempel-Ziv code is applied to an input sequence of ﬁxed-length. Here, we are interested in Lempel-Ziv code with ﬁxed output-length and variable input-length. As a result, we can apply a single ﬁxed-length extractor to the output of Lempel-Ziv code for extracting randomness. In our algorithm, we read raw bits one by one from an imperfect source until the length of the output of Lempel-Ziv code reaches a predetermined length. In another word, the number of strings after parsing is a predetermined number c. For example, if the source is 1011010100010... and c = 4, then after reading 6 bits, we can parse them into 1, 0, 11, 01. Now, we get an output sequence (000, 1), (000, 0), (001, 1), (010, 1), which can be used as the input of a ﬁxed-length extractor. We call this Lempel-Ziv code as variable-length Lempel-Ziv code, based on which we have the following procedure to extract m almost-random bits.\nConstruction 15. Assume the real source is R and there exists a stationary ergodic process M such that β( R, M) ≤ β.\n1) Read input bits one by one based on the variable-length Lempel-Ziv code such that we get an output sequence Z whose length reaches n = k 1 −β (1 + ε), where k is an integer such that there exists a ﬁxed-length (k, ϵ) extractor that generates a random sequence of length\nm from a source with min-entropy k, and ε → 0 as k → 0.\n2) Applying a (k, ϵ) extractor to Z yields a random se- quence of length m that is ϵ-close to the uniform distribution U m . \t 2\nIt can be proved that the min-entropy of Z approaches k as k → ∞ and ε → 0, so that we can continue to apply a ﬁxed-length extractor to \u2018purify\u2019 the sequence.\nTheorem 16. When k → ∞ and ε → 0, Construction 15 generates a random sequence of length m that is ϵ-close to the uniform distribution U m .\nTheorem 17. Given a real source R such that there exists a stationary ergodic process M with β( R, M) ≤ β, then the efﬁciency of Construction 15 is"},"refs":[{"authors":[{"name":"M. Blum"}],"title":{"text":"Independent unbiased coin ﬂips from a correlated biased source: - a ﬁnite state Markov chain"}},{"authors":[{"name":"Z. Dvir"}],"title":{"text":"Kakeya sets, new mergers and older extractors"}},{"authors":[{"name":"P. Elias"}],"title":{"text":"The efﬁcient construction of an unbiased random sequence"}},{"authors":[{"name":"V. Guruswami"},{"name":"C. Umans"}],"title":{"text":"Unbalanced expanders and randomness extractors from parvaresh-vardy codes"}},{"authors":[{"name":"T. S. Han"},{"name":"M. Hoshi"}],"title":{"text":"Interval algorithm for random number generation"}},{"authors":[{"name":"J. Kamp"},{"name":"D. Zuckerman"}],"title":{"text":"Deterministic extractors for bit-ﬁxing sources and exposure-resilient cryptography"}},{"authors":[{"name":"J. Kamp"},{"name":"A. Rao"},{"name":"S. Vadhan"},{"name":"D. Zuckerman"}],"title":{"text":"Deterministic extrac- tors for small-space sources"}},{"authors":[{"name":"R. Motwan"},{"name":"P. Rgahavan"}],"title":{"text":"Randomized Algorithms, Combridge University press, 1995"}},{"authors":[{"name":"N. Nisan"}],"title":{"text":"Extracting randomness: how and why: a survey"}},{"authors":[{"name":"Y. Peres"}],"title":{"text":"Iterating von Neumann\u2019s procedure for extracting random bits"}},{"authors":[{"name":"A. Rao"}],"title":{"text":"Randomness extractors for independent sources and applica- tions"}},{"authors":[{"name":"J. Radhakrishnan"}],"title":{"text":"Bounds for dispersers, extractors, and depth-two supperconcentrators"}},{"authors":[{"name":"R. Shaltiel"}],"title":{"text":"Recent developments in explicit constructions of ex- tractors"}},{"authors":[{"name":"K. Visweswariah"},{"name":"R. Kulkarni"},{"name":"S. Verd´u"}],"title":{"text":"Source codes as random number generators"}},{"authors":[{"name":"J. von Neumann"}],"title":{"text":"Various techniques used in connection with random digits"}},{"authors":[{"name":"H. Zhou"},{"name":"J. Bruck"}],"title":{"text":"Efﬁciently generating random bits from ﬁnite state markov chains"}},{"authors":[{"name":"H. Zhou"},{"name":"J. Bruck"}],"title":{"text":"Streaming Algorithms for Optimal Generation of Random Bits"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Compression of individual squences by varaible rate coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565393.pdf"},"links":[{"id":"1569565383","weight":7},{"id":"1569565223","weight":3},{"id":"1569566385","weight":3},{"id":"1569567049","weight":3},{"id":"1569564635","weight":7},{"id":"1569565067","weight":3},{"id":"1569559617","weight":3},{"id":"1569566981","weight":3},{"id":"1569566683","weight":11},{"id":"1569566227","weight":3},{"id":"1569559259","weight":3},{"id":"1569566943","weight":3},{"id":"1569552245","weight":3},{"id":"1569559889","weight":3},{"id":"1569564481","weight":7},{"id":"1569560833","weight":3},{"id":"1569566469","weight":3},{"id":"1569565355","weight":3},{"id":"1569565547","weight":3},{"id":"1569565461","weight":3},{"id":"1569564245","weight":3},{"id":"1569564227","weight":11},{"id":"1569564233","weight":3},{"id":"1569560427","weight":7},{"id":"1569565317","weight":3},{"id":"1569565123","weight":3},{"id":"1569565771","weight":3},{"id":"1569566999","weight":3},{"id":"1569565809","weight":3},{"id":"1569558483","weight":3},{"id":"1569566963","weight":3},{"id":"1569564989","weight":7},{"id":"1569566787","weight":3},{"id":"1569566015","weight":3},{"id":"1569566523","weight":3},{"id":"1569565897","weight":3},{"id":"1569566895","weight":3},{"id":"1569566269","weight":3},{"id":"1569566095","weight":3},{"id":"1569558681","weight":11},{"id":"1569564611","weight":3},{"id":"1569565667","weight":3},{"id":"1569561795","weight":7},{"id":"1569566423","weight":7},{"id":"1569566437","weight":3},{"id":"1569558901","weight":3},{"id":"1569565427","weight":3},{"id":"1569552251","weight":3},{"id":"1569554971","weight":3},{"id":"1569566209","weight":3},{"id":"1569562821","weight":3},{"id":"1569565655","weight":3},{"id":"1569564857","weight":3},{"id":"1569566629","weight":3},{"id":"1569565033","weight":3},{"id":"1569565847","weight":3},{"id":"1569565633","weight":3},{"id":"1569555879","weight":3},{"id":"1569564973","weight":3},{"id":"1569566593","weight":3},{"id":"1569566043","weight":3},{"id":"1569566505","weight":7},{"id":"1569562207","weight":3},{"id":"1569567033","weight":3},{"id":"1569561123","weight":3},{"id":"1569566233","weight":3},{"id":"1569566407","weight":3},{"id":"1569566275","weight":3},{"id":"1569565463","weight":7},{"id":"1569562551","weight":7},{"id":"1569563395","weight":3},{"id":"1569551347","weight":3},{"id":"1569555367","weight":3},{"id":"1569565571","weight":7},{"id":"1569566983","weight":3},{"id":"1569565397","weight":3},{"id":"1569566873","weight":3},{"id":"1569565385","weight":15},{"id":"1569566267","weight":7},{"id":"1569564131","weight":3},{"id":"1569564919","weight":3},{"id":"1569564595","weight":3},{"id":"1569564291","weight":3},{"id":"1569566823","weight":3},{"id":"1569565013","weight":3},{"id":"1569565293","weight":3},{"id":"1569566641","weight":3},{"id":"1569551905","weight":7},{"id":"1569564861","weight":3},{"id":"1569565529","weight":3},{"id":"1569566619","weight":7},{"id":"1569565669","weight":3},{"id":"1569567483","weight":3},{"id":"1569561713","weight":3},{"id":"1569563919","weight":3},{"id":"1569557851","weight":3},{"id":"1569559597","weight":3},{"id":"1569559251","weight":3},{"id":"1569565337","weight":3},{"id":"1569566273","weight":3},{"id":"1569566341","weight":3},{"id":"1569565889","weight":3},{"id":"1569563725","weight":3},{"id":"1569565165","weight":3},{"id":"1569566797","weight":7},{"id":"1569566375","weight":7},{"id":"1569564257","weight":3},{"id":"1569565373","weight":7},{"id":"1569564141","weight":3},{"id":"1569566987","weight":3},{"id":"1569551541","weight":3},{"id":"1569564419","weight":3},{"id":"1569566067","weight":7},{"id":"1569566825","weight":3},{"id":"1569566443","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T1.5","endtime":"16:20","authors":"Hongchao Zhou, Jehoshua Bruck","date":"1341331200000","papertitle":"Variable-Length Extractors","starttime":"16:00","session":"S7.T1: Lossless and Universal Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569565393"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
