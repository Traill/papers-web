{"id":"1569563395","paper":{"title":{"text":"Joint Source-Channel Coding for the Multiple-Access Relay Channel"},"authors":[{"name":"Yonathan Murin"},{"name":"Ron Dabora"},{"name":"Deniz G¨und¨uz"}],"abstr":{"text":"Abstract\u2014Reliable transmission of arbitrarily correlated sources over multiple-access relay channels (MARCs) and multiple-access broadcast relay channels (MABRCs) is con- sidered. In MARCs, only the destination is interested in a reconstruction of the sources, while in MABRCs, both the relay and the destination want to reconstruct the sources. We allow an arbitrary correlation among the sources at the transmitters, and let both the relay and the destination have side information that are correlated with the sources. Two joint source-channel coding schemes are presented and the corresponding sets of sufﬁcient conditions for reliable communication are derived. The proposed schemes use a combination of the correlation preserving mapping (CPM) technique with Slepian-Wolf (SW) source coding: the ﬁrst scheme uses CPM for encoding information to the relay and SW source coding for encoding information to the destination; while the second scheme uses SW source coding for encoding information to the relay and CPM for encoding information to the destination."},"body":{"text":"The multiple-access relay channel (MARC) models a net- work in which several users communicate with a single destination, with the help of a relay [1]. Examples of such a network include sensor and ad-hoc networks in which an inter- mediate relay node is introduced to assist the communication from the source terminals to the destination. The MARC is a fundamental multi-terminal channel model that generalizes both the multiple-access channel (MAC) and the relay channel models, and has received a lot of attention in the recent years. If the relay terminal is also required to decode the source messages, the model is called the multiple-access broadcast relay channel (MABRC).\nWhile in [1],[2] MARCs with independent sources at the terminals are considered, in [3], [4] we allow arbitrary corre- lation among the sources to be transmitted to the destination in a lossless fashion. We also let the relay and the destination have side information that are correlated with the two sources. In [3] we address the problem of determining whether a pair of sources can be losslessly transmitted to the destination with a given number of channel uses per source sample, using statistically independent source code and channel code.\nIn [5] Shannon showed that a source can be reliably transmitted over a memoryless point-to-point (PtP) channel, if and only if its entropy is less than the channel capacity. Hence,\na simple comparison of the rates of the optimal source code and the optimal channel code for the respective source and channel, sufﬁces to conclude whether reliable communication is feasible. This is called the separation theorem. However, the optimality of separation does not generalize to multiuser networks [6], [7], and, in general the source and channel codes must be jointly designed for every particular combination of source and channel, for optimal performance.\nIn this paper we study source-channel coding for the trans- mission of correlated sources over MARCs . We note that while the capacity region of the MAC (which is a special case of the MARC) is known for independent messages, the optimal joint source-channel code for the case of correlated sources is not known in general [6]. Single-letter sufﬁcient conditions for communicating discrete, arbitrarily correlated sources over a MAC are derived in [6]. These conditions were later shown by Dueck in [8], to be sufﬁcient but not necessary. This gives an indication on the complexity of the problem studied in the present work.\nThe main technique used in [6] is the correlation preserving mapping (CPM) in which the channel codewords are correlated with the source sequences. Since the source sequences are correlated with each other, CPM leads to correlation between the channel codewords. The CPM technique of [6] is extended to source coding with side information for the MAC in [9] and to broadcast channels with correlated sources in [10]. Transmission of arbitrarily correlated sources over interference channels (ICs) is studied in [11], in which Liu and Chen apply the CPM technique to ICs. Lossless transmission over a relay channel with correlated side information is studied in [12] and [13]. In [12] a decode-and-forward (DF) based achievability scheme is proposed and it is shown that separation is optimal for physically degraded relay channels with degraded side information, as well as for cooperative relay-broadcast chan- nels with arbitrary side information. Necessary and sufﬁcient conditions for reliable transmission of a source over a relay channel, when side information is available at the receiver or at the relay, are established in [13].\nIn this paper we ﬁrst demonstrate the suboptimality of separate source and channel encoding for the MARC by considering the transmission of correlated sources over a discrete memoryless (DM) semi-orthogonal MARC in which the relay-destination link is orthogonal to the channel from the sources to the relay and the destination.\nNext, we propose two DF-based joint source-channel achievability schemes for MARCs and MABRCs. Both pro- posed schemes use a combination of SW source coding and the CPM technique. While in the ﬁrst scheme CPM is used for encoding information to the relay and SW source coding is used for encoding information to the destination; in the second scheme SW source coding is used for encoding information to the relay and CPM is used for encoding information to the des- tination. A comparison of the conditions of the two schemes reveals a tradeoff: while the relay feasibility conditions of the former are looser, the destination feasibility conditions of the latter are looser. These are the ﬁrst joint source-channel achievability schemes, proposed for a multiuser network with a relay, which take advantage of the CPM technique.\nThe rest of this paper is organized as follows: in Section II we introduce the system model and notations. In Section III we demonstrate the suboptimality of separate encoding for the MARC. In Section IV we present two achievability schemes for DM MARCs and MABRCs with correlated sources and side information, and derive their corresponding sets of fea- sibility conditions. We discuss the results in Section V, and conclude the paper in Section VI.\nIn the following we denote random variables with upper case letters, e.g. X, and their realizations with lower case letters, e.g. x. A discrete random variable X takes values in a set X . We use p X (x) ≡ p(x) to denote the probability mass function (p.m.f.) of a discrete RV X on X . We denote vectors with boldface letters, e.g. x; the i\u2019th element of a vector x is denoted by x i , and we use x j i where i < j to denote (x i , x i+1 , ..., x j−1 , x j ); x j is a short form notation for x j 1 . We use A ∗(n) (X) to denote the set of -strongly typical sequences w.r.t. the p.m.f p X (x) on X , as deﬁned in [14, Ch. 13.6]. When referring to a typical set we may omit the random variables from the notation, when these variables are clear from the context. The empty set is denoted by φ.\nThe MARC consists of two transmitters (sources), a receiver (destination) and a relay. Transmitter i observes to the source sequence S n i , for i = 1, 2. The receiver is interested in the lossless reconstruction of both source sequences observed by the two transmitters. The objective of the relay is to help the receiver decode these sequences. Let W n 3 and W n , denote the side information at the relay and at the receiver respectively. The side information sequences are correlated with the source sequences. For the MABRC both the receiver and the relay are interested in a lossless reconstruction of both source sequences. Figure 1 depicts the MABRC with side information setup.\nThe sources and the side information sequences, {S 1,k , S 2,k , W k , W 3,k } n k=1 , are arbitrarily correlated according to a joint distribution p(s 1 , s 2 , w, w 3 ) over a ﬁnite alphabet S 1 × S 2 × W × W 3 , and independent across different sample indices k. All nodes know this joint distribution.\nFor transmission, a discrete memoryless MARC with inputs X 1 , X 2 , X 3 over ﬁnite input alphabets X 1 , X 2 , X 3 , and outputs\nY, Y 3 over ﬁnite output alphabets Y, Y 3 , is available. The MARC is memoryless in the sense\np(y k , y 3,k |y k−1 , y k−1 3 , x k 1 , x k 2 , x k 3 ) = p(y k , y 3,k |x 1,k , x 2,k , x 3,k ). A source-channel code for MABRCs with correlated side\ninformation consists of two encoding functions at the trans- mitters: f (n) i : S n i → X n i , i = 1, 2, a decoding function at the destination, g (n) : Y n × W n → S n 1 × S n 2 , and a decoding function at the relay, g (n) 3 : Y n 3 × W n 3 → S n 1 × S n 2 . Finally, there is a causal encoding function at the relay, x 3,k = f (n) 3,k (y k−1 3,1 , w n 3,1 ), 1 ≤ k ≤ n. Note that in the MARC scenario the decoding function g (n) 3 does not exist. Let ˆ S n i\nand ˆ S n i,3 denote the reconstruction of S n i , i = 1, 2, at the receiver and at the relay respectively. The average probability of error of a source-channel code for the MABRC is deﬁned as P (n) e \t Pr ( ˆ S n 1 , ˆ S ) 2 = (S n 1 , S n 2 ) or ( ˆ S n 1,3 , ˆ S n 2,3 ) = (S n 1 , S n 2 ) . For the MARC the deﬁnition is similar except that the decod- ing error event at the relay is omitted. The sources (S 1 , S 2 ) can be reliably transmitted over the MABRC with side information if there exists a sequence of source-channel codes such that P (n) e → 0 as n → ∞. The same deﬁnition applies to MARCs.\nBefore presenting the new joint source-channel coding schemes, we ﬁrst motivate this work by demonstrating the suboptimality of separate encoding for the MARC.\nConsider the transmission of arbitrarily correlated sources S 1 and S 2 over a DM semi-orthogonal MARC (SOMARC), in which the relay-destination link is orthogonal to the channel from the sources to the relay and to the destina- tion. The SOMARC is characterized by the joint distribution p(y R , y S , y 3 |x 1 , x 2 , x 3 ) = p(y R |x 3 )p(y S , y 3 |x 1 , x 2 ), where Y R and Y S are the channel outpus at the destination. The SOMARC is depicted in Figure 2. In the following we present a scenario (sources and a channel) in which joint source- channel coding strictly outperforms separate source-channel coding.\nWe begin with an outer bound on the sum-capacity of the SOMARC. This is characterized in Proposition 1.\nProposition 1. The sum-capacity of the SOMARC, R 1 + R 2 , is upper bounded by\nProof: Detailed proof is provided in [4, Subsection VI-A]. Next, consider a SOMARC deﬁned by\nAdditionally, consider the sources (S 1 , S 2 ) ∈ {0, 1}×{0, 1} with the joint distribution p(s 1 , s 2 ) = 1 3 for (s 1 , s 2 ) ∈ {(0, 0), (0, 1), (1, 1)}, and zero otherwise. Then, H(S 1 , S 2 ) = log 2 3 = 1.58 bits/sample. For the channel deﬁned in (2) the mutual information expression I(X 1 , X 2 ; Y 3 , Y S ) re- duces to I(X 1 , X 2 ; Y S ). This is because I(X 1 , X 2 ; Y 3 , Y S ) = I(X 1 , X 2 ; Y S ) + I(X 1 , X 2 ; Y 3 |Y S ), and Y 3 is a deterministic function of Y S . Therefore,\n= 1.5 bits per channel use. \t (3) Hence, we have H(S 1 , S 2 ) > I(X 1 , X 2 ; Y S ), for any p(x 1 )p(x 2 ). We conclude that it is not possible to send the sources reliably to the destination by using a separation-based source and channel codes. However, by choosing X 1 = S 1 and X 2 = S 2 a zero error probability is achieved. This example shows that separate source and channel coding is, in general, suboptimal for sending arbitrarily correlated sources over MARCs .\nIn this section we present two sets of sufﬁcient condi- tions for reliable transmission of correlated sources over DM MARCs and MABRCs with side information. Both achiev- ability schemes use a combination of SW source coding, the CPM technique, and the DF scheme with successive decoding at the relay and backward decoding at the destination [1]. We shall refer to SW source coding as separate source and channel coding (SSCC). The achievability schemes differ in the way the source codes are combined. In the ﬁrst scheme (Thm. 1) SSCC is used for encoding information to the destination while CPM is used for encoding information to the relay. In the second scheme (Thm. 2), CPM is used for encoding information to the destination while SSCC is used for encoding information to the relay.\nTheorem 1. A source pair (S n 1 , S n 2 ) can be reliably transmitted over a DM MARC with relay and receiver side information as deﬁned in Section II if,\nH(S 1 |S 2 , W 3 ) < I(X 1 ; Y 3 |S 2 , X 2 , V 1 , X 3 , W 3 ) (4a) H(S 2 |S 1 , W 3 ) < I(X 2 ; Y 3 |S 1 , X 1 , V 2 , X 3 , W 3 ) (4b) H(S 1 , S 2 |W 3 ) < I(X 1 , X 2 ; Y 3 |V 1 , V 2 , X 3 , W 3 ) (4c)\nH(S 1 |S 2 , W ) < I(X 1 , X 3 ; Y |S 1 , X 2 , V 2 ) \t (4d) H(S 2 |S 1 , W ) < I(X 2 , X 3 ; Y |S 2 , X 1 , V 1 ) \t (4e) H(S 1 , S 2 |W ) < I(X 1 , X 2 , X 3 ; Y |S 1 , S 2 ), \t (4f)\nfor a joint distribution that factors as p(s 1 , s 2 , w 3 , w)p(v 1 )p(x 1 |s 1 , v 1 )×\np(v 2 )p(x 2 |s 2 , v 2 )p(x 3 |v 1 , v 2 )p(y 3 , y|x 1 , x 2 , x 3 ). (5) Proof: See [4, Subsection VI.D, Appendix C].\nTheorem 2. A source pair (S n 1 , S n 2 ) can be reliably transmitted over a DM MARC with relay and receiver side information as deﬁned in Section II if,\nH(S 1 |S 2 , W 3 ) < I(X 1 ; Y 3 |S 1 , X 2 , X 3 ) \t (6a) H(S 2 |S 1 , W 3 ) < I(X 2 ; Y 3 |S 2 , X 1 , X 3 ) \t (6b) H(S 1 , S 2 |W 3 ) < I(X 1 , X 2 ; Y 3 |S 1 , S 2 , X 3 ) \t (6c)\nH(S 1 |S 2 , W ) < I(X 1 , X 3 ; Y |S 2 , X 2 , W ) \t (6d) H(S 2 |S 1 , W ) < I(X 2 , X 3 ; Y |S 1 , X 1 , W ) \t (6e) H(S 1 , S 2 |W ) < I(X 1 , X 2 , X 3 ; Y |W ), \t (6f)\np(x 3 |s 1 , s 2 )p(y 3 , y|x 1 , x 2 , x 3 ). \t (7) Proof:\n1) Codebook construction: For i = 1, 2, assign every s i ∈ S n i to one of 2 nR i bins independently according to a uniform distribution on U i {1, 2, . . . , 2 nR i }. Denote this assignment by f i , i = 1, 2.\nFor i = 1, 2, for each pair (u i , s i ), u i ∈ U i , s i ∈ S n i , gener- ate one n-length codeword x i (u i , s i ), by choosing the letters x i,k (u i , s i ) independently with distribution p X i |S i (x i,k |s i,k ) for all 1 ≤ k ≤ n. Finally, generate one length-n re- lay codeword x 3 (s 1 , s 2 ) for each pair (s 1 , s 2 ) ∈ S n 1 × S n 2 by choosing x 3,k (s 1 , s 2 ) independently with distribution p X 3 |S 1 ,S 2 (x 3,k |s 1,k , s 2,k ) for all 1 ≤ k ≤ n.\n2) Encoding: Consider a source sequences of length Bn s Bn i \t ∈ S Bn i , i = 1, 2. Partition each sequence into B length-n subsequences, s i,b , b = 1, . . . , B. Similarly, for b = 1, 2, . . . , B, partition the side information sequences w Bn 3\nand w Bn into B length-n subsequences w 3,b , w b , respectively. We transmit a total of Bn source samples over B + 1 blocks of n channel uses each.\nAt block 1, source terminal i, i = 1, 2, observes s i,1 and ﬁnds its corresponding bin index u i,1 ∈ U i . It then transmits the channel codeword x i (u i,1 , a i ) where a i ∈ S n i is a ﬁxed sequence. At block b, b = 2, . . . , B, source terminal i, i = 1, 2, transmits the channel codeword x i (u i,b , s i,b−1 ) where u i,b ∈ U i is the bin index of source vector s i,b . At block B + 1, source terminal i, i = 1, 2, transmits x i (1, s i,B ).\nAt block b = 1, the relay transmits x 3 (a 1 , a 2 ). Assume that at block b, b = 2, . . . , B, B + 1, the relay obtained the estimates (˜ s 1,b−1 , ˜ s 2,b−1 ) of (s 1,b−1 , s 2,b−1 ). It then transmits the channel codeword x 3 (˜ s 1,b−1 , ˜ s 2,b−1 ).\n3) Decoding: The relay decodes the source sequences sequentially trying to reconstruct source block s i,b , i = 1, 2, at the end of channel block b as follows: let ˜ s i,b−1 , i = 1, 2, be the estimate of s i,b−1 , at the relay at the end of block b − 1. Using this information, and its received signal y 3,b , the relay channel decoder at time b decodes (u 1,b , u 2,b ), i.e., the bin indices corresponding to s i,b , i = 1, 2, by looking for a unique pair (˜ u 1 , ˜ u 2 ) such that:\nx 3 (˜ s 1,b−1 , ˜ s 2,b−1 ), y 3,b ∈ A ∗(n) . \t (8) The decoded bin indices, denoted (˜ u 1,b , ˜ u 2,b ), are then\ngiven to the relay source decoder. Using (˜ u 1,b , ˜ u 2,b ) and the side information w 3,b , the relay source decoder esti- mates (s 1,b , s 2,b ) by looking for a unique pair of sequences (˜ s 1 , ˜ s 2 ) that satisﬁes f 1 (˜ s 1 ) = ˜ u 1,b , f 2 (˜ s 2 ) = ˜ u 2,b and (˜ s 1 , ˜ s 2 , w 3,b ) ∈ A ∗(n) (S 1 , S 2 , W 3 ). Let (˜ s 1,b , ˜ s 2,b ) denote the decoded sequences.\nDecoding at the destination is done using backward de- coding. The destination node waits until the end of channel block B + 1. It ﬁrst decodes s i,B , i = 1, 2, using the received signal at channel block B + 1 and its side information w B . Going backwards from the last channel block to the ﬁrst, we assume that the destination has estimates (ˆ s 1,b+1 , ˆ s 2,b+1 ) of (s 1,b+1 , s 2,b+1 ) and consider decoding of (s 1,b , s 2,b ). From (ˆ s 1,b+1 , ˆ s 2,b+1 ) the destination ﬁnds the corresponding bin indices (ˆ u 1,b+1 , ˆ u 2,b+1 ). Using this information, its received signal y b+1 and the side information w b , the destination decodes (s 1,b , s 2,b ) by looking for a unique pair (ˆ s 1 , ˆ s 2 ) such that:\nx 3 (ˆ s 1 , ˆ s 2 ), w b , y b+1 ∈ A ∗(n) . \t (9) 4) Error probability analysis: The error probability analy-\nRemark 1. Constraints (4a)\u2013(4c) in Thm. 1 and (6a)\u2013(6c) in Thm. 2, are due to decoding at the relay, while constraints (4d)\u2013(4f) in Thm. 1 and (6d)\u2013(6f) in Thm. 2, are due to decoding at the destination.\nRemark 2. In Thm. 1 V 1 and V 2 represent the binning information for S 1 and S 2 , respectively. Observe that the left-hand side (LHS) of condition (4a) is the entropy of S 1 when (S 2 , W 3 ) are known. On the right-hand side (RHS) of (4a), as V 1 , S 2 , X 2 , X 3 and W 3 are given, the mutual in- formation expression I(X 1 ; Y 3 |S 2 , X 2 , V 1 , X 3 , W 3 ) represents the available rate that can be used for sending to the relay information on the source sequence S n 1 , in excess of the bin index represented by V 1 . The LHS of condition (4d) is the entropy of S 1 when (S 2 , W ) are known. The RHS of condition (4d) expresses the rate at which binning information can be transmitted reliably and cooperatively from transmitter 1 and the relay to the destination. This follows as the mutual information expression on the RHS of (4d) can be written as I(X 1 , X 3 ; Y |S 1 , X 2 , V 2 ) = I(X 1 , X 3 ; Y |S 1 , S 2 , V 2 , X 2 , W ),\nwhich, as S 1 , S 2 and W are given, represents the rate for sending the bin index of source sequence S n 1 to the destination (see [4, Subsection VI-D]). This is in contrast to the decoding constraint at the relay, c.f. (4a). Therefore, each mutual infor- mation expression in (4a) and (4d) represents a different type of information sent by the source: either the source-channel codeword to the relay in (4a), or bin index to the destination in (4d). This is because SSCC is used for sending information to the destination and CPM is used for sending information to the relay.\nIn Thm. 2 the LHS of condition (6a) is the entropy of S 1 when (S 2 , W 3 ) are known. In the RHS of (6a) the mutual information expression I(X 1 ; Y 3 |S 1 , X 2 , X 3 ) = I(X 1 ; Y 3 |S 1 , S 2 , X 2 , X 3 , W 3 ) represents the rate for sending the bin index of the source sequence S n 1 to the relay (see [4, Subsection VI-E]). This is because S 1 , S 2 and W 3 are given. The LHS of condition (6d) is the entropy of S 1 when (S 2 , W ) are known. In the RHS of condition (6d), as S 2 , X 2 and W are given, the mutual information expression I(X 1 , X 3 ; Y |S 2 , X 2 , W ) represents the available rate that can be used for sending information on the source sequence S n 1 to the destination.\nRemark 3. For an input distribution p(s 1 , s 2 , w 3 , w, v 1 , v 2 , x 1 , x 2 , x 3 ) =\np(s 1 , s 2 , w 3 , w)p(v 1 )p(x 1 |v 1 )p(v 2 )p(x 2 |v 2 )p(x 3 |v 1 , v 2 ), the conditions in (4) specialize to [3, Equation (2)], and the transmission scheme specializes to a separation-based achievability scheme.\nRemark 4. In both Thm. 1 and Thm. 2 the conditions stem- ming from the CPM technique can be specialized to the MAC source-channel conditions of [6, Equations (12)]. In Thm. 1 letting V 1 = V 2 = X 3 = W 3 = φ, reduces the relay conditions in (4a)\u2013(4c) to the ones in [6, Equations (12)] with Y 3 as the destination. In Thm. 2 letting X 3 = W = φ, reduces the destination conditions in (4d)\u2013(4f) to the ones in [6, Equations (12)] with Y as the destination.\nRemark 5. Thm. 1 and Thm. 2 establish different achievabil- ity conditions. As stated in Section III, SSCC is generally suboptimal for sending correlated sources over DM MARCs and MABRCs. In Thm. 1 the CPM technique is used for sending information to the relay, while in Thm. 2 SSCC is used for sending information to the relay. This observation implies that the relay decoding constraints of Thm. 1 are looser compared to the relay decoding constraints of Thm. 2. Using similar reasoning we conclude that the destination decoding constraints of Thm. 2 are looser compared to the destination decoding constraints of Thm. 1 (as long as coordination is possible, see Remark 6). Considering the distribution chains in (5) and (7) we conclude that these two theorems represent different sets of sufﬁcient conditions, and neither theorem is a special case of the other.\nRemark 6. Figure 3 depicts the cooperative relay broadcast channel (CRBC) model with correlated relay and destination side information, which is a special case of the MABRC with X 2 = S 2 = φ. For this model the optimal source-channel rate was obtained in [12, Theorem 3.1]:\nProposition ([12, Theorem 3.1]). A source S n 1 can be reliably transmitted over a DM CRBC with relay and receiver side information if\nH(S 1 |W 3 ) < I(X 1 ; Y 3 |X 3 ) \t (10a) H(S 1 |W ) < I(X 1 , X 3 ; Y ), \t (10b)\nfor some input distribution p(s 1 , w 3 , w)p(x 1 , x 3 ). Conversely, if a source S n 1 can be reliably transmitted then the conditions in (10a) and (10b) are satisﬁed with < replaced by ≤ for some input distribution p(x 1 , x 3 ).\nThe conditions in (10) can also be obtained from Thm. 1 by letting V 1 = X 3 , S 2 = X 2 = V 2 = φ, and considering an input distribution independent of the sources. However, when we consider Thm. 2 with S 2 = X 2 = φ, we obtain the following achievability conditions:\nH(S 1 |W 3 ) < I(X 1 ; Y 3 |X 3 , S 1 ) \t (11a) H(S 1 |W ) < I(X 1 , X 3 ; Y |W ), \t (11b)\nNote that the RHS of the inequalities in (11a) and (11b) are not greater than the RHS of the inequalities in (10a) and (10b), respectively. Moreover, not all joint input distributions p(x 1 , x 3 ) can be achieved via p(x 1 |s 1 )p(x 3 |s 1 ). Hence, the conditions obtained from Thm. 2 for the CRBC setup are stricter than those obtained from Thm. 1, illustrating the fact that the two sets of conditions are not equivalent. We conclude that the downside of using CPM to the destination as applied in this work is that it puts constraints on the distribution chain, thereby constraining the achievable coordination between the sources and the relay. For this reason, when there is only a single source, the joint distributions of the source and the relay (X 1 and X 3 ) achieved by the scheme of Thm. 2, do not exhaust the entire space of joint distributions, resulting in generally stricter source-channel constraints than those obtained from Thm. 1. However, recall that for SOMARC in Section III the optimal scheme uses CPM to the destination. Therefore, for the MARC it is not possible to determine whether either of the schemes is universally better than the other.\nRemark 7. In both Thm. 1 and Thm. 2 we use a combina- tion of SSCC and CPM. Since CPM can generally support sources with higher entropies, a natural question that arises is whether it is possible to design a scheme based only on CPM; namely, encode both cooperation (relay) information and the new information, using a superposition CPM scheme. This approach cannot be used directly in the framework of the current paper. Here, we use joint typicality decoder, which does not apply to different blocks generated independently with the same distribution. For example, we cannot test the joint typicality of s n 1,1 and s 2n 1,n+1 , as they belong to different time blocks. Using a CPM-only scheme would require such a test. We conclude that applying the CPM technique for sending information to both the relay and the destination cannot be done while using joint typicality decoder as considered in this paper. It is, of course, possible to construct schemes that use a different decoder, or apply CPM through intermediate RVs, which overcome this difﬁculty. Investigation of such coding schemes is left for future research.\nIn this paper we considered joint source-channel coding for DM MARCs and MABRCs. We ﬁrst showed via an explicit example that joint source-channel coding generally enlarges the set of possible sources that can be reliably transmitted compared to separation-based coding. We then derived two new joint source-channel achievability schemes. Both schemes use a combination of SSCC and CPM techniques. While in the ﬁrst scheme CPM is used for encoding information to the relay and SSCC is used for encoding information to the destination, in the second scheme SSCC is used for encoding information to the relay and CPM is used for encoding information to the destination. The different combinations of binning and source mapping enable ﬂexibility in the system design by choosing one of the two schemes according to the quality of the side information and received signals at the relay and at the destination. In particular, the ﬁrst scheme has looser decoding constraints at the relay and is therefore better when the channels from the sources to the relay are the bottleneck; while the second scheme has looser decoding constraints at the destination, and is more suitable for scenarios in which the channels to the destination are more noisy (at the cost of more constrained source-relay coordination)."},"refs":[{"authors":[{"name":"G. Kramer"},{"name":"M. Gastpar"},{"name":"P. Gupta"}],"title":{"text":"Cooperative strategies and capacity theorems for relay networks"}},{"authors":[{"name":"L. Sankaranarayanan"},{"name":"G. Kramer"},{"name":"N. B. Mandayam"}],"title":{"text":"Offset encoding for multiaccess relay channels"}},{"authors":[{"name":"Y. Murin"},{"name":"R. Dabora"},{"name":"D. G¨und¨uz"}],"title":{"text":"Source-channel coding for the multiple-access relay channel"}},{"authors":[{"name":"Y. Murin"},{"name":"R. Dabora"},{"name":"D. G¨und¨uz"}],"title":{"text":"Source-channel cod- ing theorems for the multiple-access relay channel"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"T. M. Cover"},{"name":"A. El Gamal"},{"name":"M. Salehi"}],"title":{"text":"Multiple access channels with arbitrarily correlated sources"}},{"authors":[{"name":"D. G¨und¨uz"},{"name":"E. Erkip"},{"name":"A. Goldsmith"},{"name":"H. V. Poor"}],"title":{"text":"Source and channel coding for correlated sources over multiuser channels"}},{"authors":[{"name":"G. Dueck"}],"title":{"text":"A note on the multiple access channel with correlated sources"}},{"authors":[{"name":"R. Ahlswede"},{"name":"T. S. Han"}],"title":{"text":"On source coding with side information via a multiple access channel and related problems in multiuser information theory"}},{"authors":[{"name":"T. Han"},{"name":"M. H. M. Costa"}],"title":{"text":"Broadcast channels with arbitrarily correlated sources"}},{"authors":[{"name":"W. Liu"},{"name":"B. Chen"}],"title":{"text":"Interference channels with arbitrarily correlated sources"}},{"authors":[{"name":"D. G¨und¨uz"},{"name":"E. Erkip"}],"title":{"text":"Reliable cooperative source transmission with side information"}},{"authors":[{"name":"R. Kwak"},{"name":"W. Lee"},{"name":"A. El Gamal"},{"name":"J. Ciofﬁ"}],"title":{"text":"Relay with side information"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569563395.pdf"},"links":[{"id":"1569566381","weight":30},{"id":"1569566485","weight":13},{"id":"1569565383","weight":3},{"id":"1569566725","weight":10},{"id":"1569565663","weight":10},{"id":"1569565377","weight":10},{"id":"1569567049","weight":6},{"id":"1569564635","weight":6},{"id":"1569565867","weight":3},{"id":"1569565067","weight":6},{"id":"1569564669","weight":3},{"id":"1569565691","weight":3},{"id":"1569559617","weight":6},{"id":"1569566981","weight":6},{"id":"1569566683","weight":3},{"id":"1569559259","weight":6},{"id":"1569566697","weight":3},{"id":"1569566597","weight":10},{"id":"1569565551","weight":3},{"id":"1569566943","weight":3},{"id":"1569566591","weight":6},{"id":"1569556029","weight":3},{"id":"1569552245","weight":6},{"id":"1569564481","weight":10},{"id":"1569566415","weight":6},{"id":"1569566469","weight":6},{"id":"1569565355","weight":10},{"id":"1569564469","weight":3},{"id":"1569565931","weight":6},{"id":"1569566373","weight":3},{"id":"1569566765","weight":10},{"id":"1569565461","weight":6},{"id":"1569564245","weight":3},{"id":"1569564731","weight":3},{"id":"1569564227","weight":10},{"id":"1569558325","weight":3},{"id":"1569565837","weight":6},{"id":"1569566303","weight":6},{"id":"1569566119","weight":3},{"id":"1569564233","weight":3},{"id":"1569563411","weight":6},{"id":"1569559541","weight":10},{"id":"1569566941","weight":6},{"id":"1569564203","weight":10},{"id":"1569556713","weight":6},{"id":"1569566467","weight":6},{"id":"1569560613","weight":6},{"id":"1569566999","weight":10},{"id":"1569565859","weight":3},{"id":"1569566843","weight":10},{"id":"1569566579","weight":3},{"id":"1569558483","weight":10},{"id":"1569565455","weight":16},{"id":"1569566963","weight":6},{"id":"1569561679","weight":3},{"id":"1569566709","weight":13},{"id":"1569564989","weight":6},{"id":"1569566015","weight":3},{"id":"1569566523","weight":3},{"id":"1569551763","weight":3},{"id":"1569565953","weight":6},{"id":"1569565321","weight":3},{"id":"1569566095","weight":6},{"id":"1569565907","weight":6},{"id":"1569563981","weight":3},{"id":"1569559565","weight":3},{"id":"1569566905","weight":16},{"id":"1569566753","weight":3},{"id":"1569566311","weight":3},{"id":"1569566063","weight":6},{"id":"1569555999","weight":3},{"id":"1569565213","weight":6},{"id":"1569566643","weight":3},{"id":"1569566511","weight":3},{"id":"1569565841","weight":6},{"id":"1569566531","weight":6},{"id":"1569561143","weight":6},{"id":"1569565833","weight":10},{"id":"1569564611","weight":6},{"id":"1569561795","weight":3},{"id":"1569566325","weight":6},{"id":"1569566437","weight":6},{"id":"1569566811","weight":3},{"id":"1569553909","weight":6},{"id":"1569566687","weight":6},{"id":"1569566939","weight":3},{"id":"1569553537","weight":3},{"id":"1569565915","weight":3},{"id":"1569552251","weight":3},{"id":"1569553519","weight":6},{"id":"1569566885","weight":3},{"id":"1569566231","weight":13},{"id":"1569554881","weight":10},{"id":"1569566445","weight":3},{"id":"1569566209","weight":3},{"id":"1569566649","weight":3},{"id":"1569565655","weight":6},{"id":"1569558985","weight":10},{"id":"1569566473","weight":3},{"id":"1569564333","weight":10},{"id":"1569566629","weight":10},{"id":"1569566257","weight":3},{"id":"1569565033","weight":13},{"id":"1569566447","weight":3},{"id":"1569566357","weight":3},{"id":"1569563897","weight":3},{"id":"1569565887","weight":6},{"id":"1569566721","weight":10},{"id":"1569565055","weight":3},{"id":"1569555879","weight":23},{"id":"1569565219","weight":13},{"id":"1569558509","weight":3},{"id":"1569564851","weight":3},{"id":"1569556671","weight":6},{"id":"1569566037","weight":3},{"id":"1569566223","weight":6},{"id":"1569566553","weight":10},{"id":"1569564969","weight":6},{"id":"1569566593","weight":10},{"id":"1569566043","weight":16},{"id":"1569565029","weight":6},{"id":"1569566505","weight":3},{"id":"1569565393","weight":3},{"id":"1569566191","weight":10},{"id":"1569567033","weight":3},{"id":"1569565527","weight":3},{"id":"1569566603","weight":3},{"id":"1569565363","weight":6},{"id":"1569566695","weight":6},{"id":"1569566051","weight":6},{"id":"1569566673","weight":10},{"id":"1569565441","weight":6},{"id":"1569565311","weight":3},{"id":"1569566233","weight":6},{"id":"1569566667","weight":3},{"id":"1569564097","weight":3},{"id":"1569560997","weight":3},{"id":"1569560349","weight":3},{"id":"1569566501","weight":3},{"id":"1569560503","weight":3},{"id":"1569565463","weight":3},{"id":"1569565439","weight":3},{"id":"1569566229","weight":3},{"id":"1569566133","weight":6},{"id":"1569562551","weight":6},{"id":"1569566901","weight":3},{"id":"1569551347","weight":10},{"id":"1569555367","weight":6},{"id":"1569566383","weight":3},{"id":"1569565571","weight":3},{"id":"1569565885","weight":3},{"id":"1569564411","weight":3},{"id":"1569565665","weight":3},{"id":"1569565549","weight":6},{"id":"1569565611","weight":13},{"id":"1569566479","weight":3},{"id":"1569565397","weight":13},{"id":"1569566873","weight":3},{"id":"1569565765","weight":6},{"id":"1569565435","weight":10},{"id":"1569557275","weight":3},{"id":"1569566129","weight":16},{"id":"1569565385","weight":3},{"id":"1569565919","weight":3},{"id":"1569565181","weight":6},{"id":"1569566711","weight":3},{"id":"1569565661","weight":10},{"id":"1569566267","weight":10},{"id":"1569564131","weight":10},{"id":"1569564919","weight":3},{"id":"1569561221","weight":3},{"id":"1569566917","weight":3},{"id":"1569566253","weight":10},{"id":"1569566691","weight":3},{"id":"1569566651","weight":6},{"id":"1569566823","weight":20},{"id":"1569566137","weight":13},{"id":"1569566237","weight":10},{"id":"1569566283","weight":3},{"id":"1569565375","weight":26},{"id":"1569566755","weight":3},{"id":"1569566813","weight":6},{"id":"1569565293","weight":16},{"id":"1569566771","weight":6},{"id":"1569559035","weight":3},{"id":"1569564247","weight":6},{"id":"1569563975","weight":6},{"id":"1569551905","weight":10},{"id":"1569565457","weight":3},{"id":"1569566487","weight":3},{"id":"1569556759","weight":13},{"id":"1569566619","weight":3},{"id":"1569565271","weight":3},{"id":"1569561185","weight":6},{"id":"1569566397","weight":3},{"id":"1569565669","weight":13},{"id":"1569565233","weight":3},{"id":"1569563721","weight":6},{"id":"1569566817","weight":3},{"id":"1569566911","weight":6},{"id":"1569564923","weight":10},{"id":"1569566299","weight":10},{"id":"1569564769","weight":13},{"id":"1569565769","weight":3},{"id":"1569566933","weight":10},{"id":"1569563919","weight":3},{"id":"1569557851","weight":30},{"id":"1569565389","weight":10},{"id":"1569565537","weight":3},{"id":"1569565561","weight":3},{"id":"1569566847","weight":3},{"id":"1569559251","weight":6},{"id":"1569567013","weight":3},{"id":"1569566583","weight":3},{"id":"1569560459","weight":13},{"id":"1569566807","weight":3},{"id":"1569565853","weight":13},{"id":"1569550425","weight":6},{"id":"1569565889","weight":3},{"id":"1569564505","weight":6},{"id":"1569565165","weight":13},{"id":"1569565635","weight":3},{"id":"1569566413","weight":3},{"id":"1569565113","weight":3},{"id":"1569566375","weight":6},{"id":"1569564257","weight":3},{"id":"1569566555","weight":6},{"id":"1569564931","weight":3},{"id":"1569564141","weight":10},{"id":"1569566973","weight":3},{"id":"1569566987","weight":3},{"id":"1569564509","weight":3},{"id":"1569551751","weight":6},{"id":"1569565139","weight":3},{"id":"1569564419","weight":16},{"id":"1569566067","weight":10},{"id":"1569563007","weight":3},{"id":"1569566443","weight":3},{"id":"1569565315","weight":3},{"id":"1569560581","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T4.1","endtime":"10:10","authors":"Yonathan Murin, Ron Dabora, Deniz Gündüz","date":"1341481800000","papertitle":"Joint Source-Channel Coding for the Multiple-Access Relay Channel","starttime":"09:50","session":"S11.T4: Joint Source-Channel Coding in Networks","room":"Stratton 20 Chimneys (306)","paperid":"1569563395"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
