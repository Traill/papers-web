{"id":"1569566133","paper":{"title":{"text":"On Energy/Information Cross-Layer Architectures"},"authors":[{"name":"Lav R. Varshney ∗\u2020"}],"abstr":{"text":"Abstract\u2014The importance of architectural principles in the design of engineering systems is well-recognized. This paper argues that the traditional separation between energy deliv- ery and information delivery leads to suboptimal systems. To demonstrate this for wireline systems that may use DC powerline communication, a capacity-power-wiring cost function is deﬁned. Signaling strategies that optimize this function deliver patterned energy: a commodity measured in bits and joules such that energy and information are intermixed. Cross-layer design leads to improved performance."},"body":{"text":"The traditional view in designing engineering systems is that energy and information should be handled with separate subsystems that have very little interaction. This modular architecture describes nearly any prominent engineering sys- tem. Energy and information, along with matter, 1 are perhaps the most basic elements of nature and there is no reason to believe that they have a common measure. Despite this physical principle, is there really no engineering beneﬁt to energy/information cross-layer design? Building on previous work [3], this paper argues that there is a beneﬁt and that separation leads to suboptimal performance.\nTo make this argument concrete, focus is placed on simul- taneously supplying a system with energy and information. Neither processing nor other transformations of energy or information [4] are addressed here.\nThe central argument of this paper is that the commodity that should be transmitted is patterned energy. A patterned energy signal carries both energy and information; its content is measured in both bits and joules.\nIt is assumed that patterned energy signals are carried over material wires. Since wires are costly, see [5] and references therein, their use should be limited in the design of systems. This leads to the development of an energy-information- wiring cost function that operationally describes a system that simultaneously meets two goals:\n1) large received energy per unit time (large received power), and\n2) large information per unit time (large information rate), and satisﬁes one constraint:\nAn informational deﬁnition of the optimal energy-information- wiring cost function is developed herein.\nA prime motivation for this work is direct current (DC) pow- erline communication. DC powerline communication delivers energy and information over a small number of wires and has seen recent interest in emerging applications that operate under the tightest of constraints. These include in-vehicle networks [6], body area networks [7], [8], and data centers [9].\nFor provision of energy and information within vehicles, it has been noted that due to \u201clarge deployment of advanced circuits in automobiles. . . interconnecting wires in some situ- ations weigh 100 kilograms\u201d and it is desirable to \u201cﬁnd an efﬁcient way for reducing the total weight\u201d [6]. DC powerline communication has been described as a good solution adopted in DaimlerChrysler and Ford cars [6].\nIn wearable systems for health monitoring, \u201cone of the important technical requirements is to eliminate entanglement of cable wires around the body\u201d and so \u201ctechnology must be developed to reduce the wires to just a few\u201d [10]. Again, DC powerline communication is proposed as a good solution implemented in clinical trials of wearable body area networks for elderly patients [7].\nIn data centers, DC powerlines have been proposed as a way to improve energy efﬁciency [9]. Moreover, the complex interplay between the energy requirements and information requirements within these structures has been duly noted [11].\nFor these and other related applications, it is clear that if only a single wire is available for provisioning energy and information, DC powerline communication techniques should be used. But what if there are two wires? Should there be separation such that one is used for power and one for information? This paper shows that no matter the number of wires, it is most efﬁcient to use them all to transmit energy and information simultaneously, using patterned energy signals.\nBefore closing the introductory section, it should be noted that some of the results also have bearing on systems that use wireless transmission of energy and information [12]. Two examples are retinal prostheses that receive power and data from light entering the eye [13] and implanted brain- machine interfaces that receive conﬁguration signals and en- ergy through inductive coupling [14].\nThis section brieﬂy discusses the fundamental tradeoff be- tween the rates at which energy and reliable information can\nbe transmitted over a single noisy line [3]. The basic schematic is shown in Fig. 1.\nBefore proceeding, one might wonder if a receiver can exploit all of the energy in a patterned energy signal by converting it into useful form, while still extracting the in- formation. If this is not possible, then any results presented herein would not provide engineering insight.\nPerhaps counterintuitively, all of the energy and all of the information in a patterned energy signal can be converted into useful form. When performing error control decoding, one might think that the energy associated with the noise must be dissipated. Results in the thermodynamics of computing demonstrate, however, that energy need not be dissipated in the decoding process, since energy is not required to perform mathematical work [15, Ch. 5]. In particular, decoders that are reversible computational devices would not dissipate any energy [16], [17] and electronic circuits that are almost thermodynamically reversible have been built [18].\nConsider a noisy wire modeled as a memoryless channel with input alphabet X , output alphabet Y, and transition probability assignment Q Y |X (y|x). Further, each output letter y ∈ Y has an energy b(y), a nonnegative real number. The average received energy for an input of length n is:\nwhere p(y n 1 ) is the output distribution and b(y n 1 ) is the n-letter extension of b(y). The capacity-power function of a wire is deﬁned as:\nNotice that the constraint is a minimum power constraint. A coding theorem [3, Thm. 1] showed that this is the supremum rate of reliable information transmission while meeting power delivery requirements.\nOne simple approach to transmitting energy and informa- tion simultaneously is to interleave the energy signal and the information-bearing signal. The transmitter and receiver\nsimultaneously commutate between the information source\u2013 destination pair and the energy source\u2013destination pair in Fig. 1. When in information mode, the energy in the signal is not exploited and when in energy mode, the information in the signal is ignored.\nIf the system is in information mode for τ fraction of the time, it follows directly from the noisy channel coding theorem [19] that the best a communication scheme could do is to achieve information rate of τ C and received power (1−τ )B max , where B max is the most received energy possible through the channel. In the case of discrete alphabet channels, B max is the maximum element of the vector b T Q Y |X com- puted from the vector-matrix product of the column vector of the b(y) denoted b and the channel transition matrix Q Y |X .\nWhen the interleaving schedule cannot be coordinated be- tween the transmitter and receiver, the receiver might randomly switch between the information destination and the energy destination. This would cause random puncturing of the code. Randomly puncturing a capacity-achieving random code just yields a shorter capacity-achieving random code. Thus, under these conditions, an information rate of τ C and received power of (1 − τ )B p ∗ Y is achievable, where\nRather than random puncturing, one might also use con- trolled puncturing [20, Ch. 2]. This would involve exploiting the information in the transmitted signal from the beginning of time until the codeword is decoded to the desired error probability, and then switching to harvesting energy afterward. Analysis of such sequential decoding procedures is beyond the scope of this paper.\nD. Optimal Approaches: Concavity of the Capacity-Power Function\nRather than time-sharing, the optimal approach is to use patterned energy signals where the energy and the information are not separated in time, but are intermixed. The capacity- power function C(B) describes the performance of optimal schemes.\nIt is clear that C(B) is non-increasing, since the feasible set in the optimization becomes smaller as B increases. Furthermore, it can be shown that C(B) is a concave ∩ function of B for 0 ≤ B ≤ B max [3, Thm. 2 and Thm. 3].\nE. A Wire with Gaussian Noise and Rail Voltage Constraints Consider the following example of optimal tradeoffs. Due\nto space restrictions and the fact that speciﬁc capacity-power examples are not the focus of this paper, many details that can be found in [1], [3] are omitted here.\nConsider the capacity-power function for a discrete-time, memoryless channel with bounded input alphabet X = [−A, A] ⊂ R, Y = R, Q Y |X governed by additive white Gaus- sian noise (AWGN) N (0, σ 2 ), and energy function b(y) = y 2 . In the engineering literature, this noise model and bounded\ninput alphabet have been asserted as valid for the physical channels encountered in wearable body area networks [10] and within vehicles [6].\nThe capacity-power achieving input distribution is supported on a ﬁnite set of mass points [1, Thm. 7.9] and so a ﬁnite numerical optimization algorithm yields the capacity-power function [21], [22]. Since the optimal signaling alphabet is discrete, practical signaling constellations may be used without shaping loss. A particle-based numerical optimization proce- dure [22] is used to determine the capacity-power function for A = 5 and σ = 1, shown in Fig. 2. As seen, as the power delivery requirement becomes more stringent, probability mass shifts to the edges of the input alphabet, culminating in antipodal signaling for maximum power delivery.\nThe prior developments in this section lead to the ﬁrst architectural design principle.\nTheorem 1. In general, the performance of a time-sharing approach is not optimal. Unless C(B) is constant for all B, 0 ≤ B ≤ B max , a time-sharing approach is strictly suboptimal.\nProof: Follows from the non-increasing and concave ∩ nature of the capacity-power function.\nThis suboptimality can be rather signiﬁcant. An example for a binary symmetric channel (BSC) is shown in Fig. 3, depicting both the coordinated and uncoordinated time-sharing approaches from Sec. II-C and the capacity-power function.\nDesign Principle 1. For systems that transmit energy and information and have one wire, it is best to use patterned energy signals rather than time-division.\nHaving studied a single wire, consider the possibility of adding more wires, albeit with some cost. If there are two wires, the traditional approach would be to use one wire to deliver power and the other to deliver information, but is this really the best way to maximize the delivery of energy and information over such a parallel channel?\nConsider a large number K of independent memoryless channels over whom information rate and received power are measured collectively and are to be maximized. Besides the usual channel input symbols x k ∈ X k for the kth subchannel, there is a dummy symbol that corresponds to not actively using the channel. 2 A unit wire cost is incurred if a channel is ever used actively, and there is a wiring constraint W that must be met. Let w(x K 1 ) indicate the number of channels that are ever used actively.\n(2) Proof of a coding theorem is omitted, but operational inter- pretation as the supremum rate of reliable information trans- mission while meeting power delivery requirements and wiring cost constraints is clear. Several properties of the optimal input distribution follow directly.\nLemma 1. The input distribution that achieves capacity- power-wiring cost C(B, W ) saturates the wiring cost bound, so that w(X K 1 ) = W .\nProof: Suppose the contrary. Then another wire can be brought into active use without violating the wiring constraint.\nUse of this additional wire can only increase the received power and information rate.\nThe best approach is to choose subchannel inputs that are independent of each other.\nLemma 2. The input distribution that achieves capacity- power-wiring cost C(B, W ) has independent inputs (X 1 , X 2 , . . . , X K ).\nProof: Follows since noise is independent across subchan- nels, cf. [19, Sec. 10.4].\nAny active subchannel should be used in a capacity-power achieving way.\nLemma 3. The portion of the input distribution that achieves capacity-power-wiring cost C(B, W ) corresponding to an active subchannel achieves the capacity-power C(B 0 ) of that subchannel for some B 0 .\nProof: Suppose the contrary for a given active subchan- nel. By deﬁnition of capacity-power, the input distribution can be modiﬁed either to increase the received power while maintaining the same information rate or to increase the information rate while maintaining the same received power. Such a modiﬁcation can only increase the collective received power or collective information rate.\nWith these lemmas, the optimization problem becomes easier. Before proceeding with general statements, an example is presented.\nExample 1. Consider several BSC wires with identical crossover probabilities ω = 1/10, and energy function b(0) = 0, b(1) = 1. The wiring cost constraint is W = 2 and the power delivery requirement is B = 6/5.\nDue to Lem. 1, two wires are to be used. Due to Lem. 2 and 3, independent capacity-power achieving random coding schemes, described by π 1 and π 2 , can be used on the wires.\nwhere h 2 (·) is the binary entropy function. The total power delivered is\nThe total power delivery constraint requires that π 1 + π 2 ≥ (B − 2ω)/(1 − 2ω), which in this case is π 1 + π 2 ≥ 5/4. To maximize the information rate, π 1 and π 2 should be chosen to be as small as possible, due to the concavity of the binary entropy function. Hence the best choice is to have π 1 = 5/8 and π 2 = 5/8.\nThe wires should be used to deliver the identical kind of patterned energy.\nThe basic idea of the example goes through for any set of identical wires.\nTheorem 2. For several identical wires with maximum power delivery constant B max , assuming B ≤ min(K, W )B max ,\noptimal usage in the sense of C(B, W ) is to use identically generated random codes that achieve C(B/ min(K, W )) on min(K, W ) wires; any remaining wires should be inactive.\nProof: That min(K, W ) wires should be actively used follows from Lem. 1. That independent codes that achieve capacity-power should be used follows from Lem. 2 and 3, so all active wires are to achieve C(B k ) for some values of B k , k = 1, 2, . . . , min(K, W ): it remains to choose these.\nDue to the non-increasing nature of the capacity-power function, the power delivery constraint should be met with equality:\nFinally, the power delivery requirement should be partitioned equally B k = B/ min(K, W ) for k = 1, 2, . . . , min(K, W ). This is because\nfor admissible Γ and δ due to concavity of capacity-power functions.\nDesign Principle 2. For systems that transmit energy and information and have the possibility of several identical wires, as many as feasible should be used to deliver patterned energy signals of the same kind. Wires should not be partitioned between those for energy and those for information.\nSuppose there are different kinds of wires\u2014with different noise properties\u2014that may be brought into use. What system design principles arise in this setting?\nLet C 1 (·), C 2 (·), . . . , C K (·) denote the capacity-power functions of the available wires sorted into order such that B (1) max ≥ B (2) max ≥ · · · ≥ B (K) max . The following is clear from previous arguments.\nTheorem 3. For several wires with maximum power delivery constants B (k) max , assuming B ≤ min(K,W ) k=1 \t B (k) max , optimal usage in the sense of C(B, W ) is to use random codes that achieve C(B k ) on min(K, W ) wires; any remaining wires should be inactive. Moreover B = active ks B k .\nThe allocation of how the power delivery requirement is to be met is governed by the following optimization problem.\nwhere the choice \t indicates inactivity, contributing zero information rate and zero power, and 1 ⊕ (·) is an indicator function of activity.\nLemma 4. Suppose there are two wires with capacity-power functions C 1 (·) and C 2 (·). If B (1) max ≥ B (2) max and C 1 (B) > C 2 (B) for all 0 ≤ B ≤ B (2) max , then the ﬁrst wire is active whenever the second wire is active.\nProof: Suppose the contrary that the second wire is active when the ﬁrst wire is not. Then by the ordering property, the wires can be switched to achieve either greater collective power or greater collective information rate.\nThere is no closed form solution to (3) for general sets of wires, but it can be solved computationally. In certain cases, it makes sense to relax the combinatorial sparse wire usage con- straint using convex relaxation. In average power-constrained AWGN settings, the solution is similar to modiﬁcations of waterﬁlling [12]. Nevertheless, the same design principle as in the identical wire case holds.\nDesign Principle 3. For systems that transmit energy and information and have the possibility of several wires, as many as feasible should be used to deliver patterned energy signals.\nWe have assumed that although different wires may have different noise properties, their wiring costs are equal. In fact, an inexpensive wire may be noisier [23]. Hence it is of interest to consider a more general view of wiring cost in future work.\nArchitectural principles are of utmost importance when en- gineering optimal systems. Principles of modular architecture such as the separation of estimation and control; the separation of source coding and channel coding; and the Von Neumann architecture have come to the fore for purely informational sys- tems [24], [25]. These principles, however, implicitly assume a separation between energy and information. In fact, power engineering and communication engineering have developed essentially independently as academic ﬁelds [26]. As shown in this paper, however, this separation leads to a loss of performance and so cross-layer design should be used.\nEnergy and information should ﬂow together as patterned energy signals, measured in both joules and bits.\nThese architectural principles have direct bearing on body area networks, data centers, and in-vehicle communication systems that use DC powerline communication, but they hold much more broadly. In future work, it would be interesting to consider more complicated network topologies where informa- tion and energy are to be transported together over physical wires. This whether considering extant applications in body area networks [27] or in general engineering systems [4].\nI thank J. Dauwels for help with Fig. 2, as well as S. K. Mitter and V. K. Goyal for wisdom that informed this paper."},"refs":[{"authors":[{"name":"L. R. Varshney"}],"title":{"text":"Unreliable and resource-constrained decoding"}},{"authors":[{"name":"A. Einstein"}],"title":{"text":"Ist die tr¨agheit eines k¨orpers von seinem energieinhalt abh¨angig?"}},{"authors":[{"name":"L. R. Varshney"}],"title":{"text":"Transporting information and energy simultaneously"}},{"authors":[{"name":"O. L. de Wec"},{"name":"D. Roo"},{"name":"C. L. Mage"}],"title":{"text":"Engineering Systems: Meeting Human Need in a Complex Technological World "}},{"authors":[{"name":"L. R. Varshney"}],"title":{"text":"Distributed inference networks with costly wires"}},{"authors":[{"name":"S. A. Mirtaheri"},{"name":"S. Salimpoor"}],"title":{"text":"HEV (hybrid electric vehicles) and the wiring reduction methods"}},{"authors":[{"name":"E. Wade"},{"name":"H. Asada"}],"title":{"text":"Conductive-fabric garment for a cable-free body area network"}},{"authors":[{"name":"I. Berganza Valmala"},{"name":"G. Bumiller"},{"name":"H. A. Latchman"},{"name":"M. V. Ribeiro"},{"name":"A. Sendin Escalona"},{"name":"E. R. Wade"},{"name":"L. W. Yonge"}],"title":{"text":"Systems and imple- mentations"}},{"authors":[{"name":"M. Ton"},{"name":"B. Fortenbery"},{"name":"W. Tschudi"}],"title":{"text":"DC power for improved data center efﬁciency"}},{"authors":[{"name":"E. Wade"},{"name":"H. H. Asada"}],"title":{"text":"Wearable DC powerline communication network using conductive fabrics"}},{"authors":[{"name":"P. Bodik"},{"name":"M. P. Armbrust"},{"name":"K. Canini"},{"name":"A. Fox"},{"name":"M. Jordan"},{"name":"D. A. Pat- terson"}],"title":{"text":"A case for adaptive datacenters to conserve energy and improve reliability"}},{"authors":[{"name":"P. Grover"},{"name":"A. Sahai"}],"title":{"text":"Shannon meets Tesla: Wireless information and power transfer"}},{"authors":[{"name":"R. Dinyari"},{"name":"J. D. Loudin"},{"name":"P. Huie"},{"name":"D. Palanker"},{"name":"P. Peumans"}],"title":{"text":"A curvable silicon retinal implant"}},{"authors":[{"name":"R. R. Harrison"},{"name":"P. T. Watkins"},{"name":"R. J. Kier"},{"name":"R. O. Lovejoy"},{"name":"D. J. Black"},{"name":"B. Greger"},{"name":"F. Solzbacher"}],"title":{"text":"A low-power integrated circuit for a wireless 100-electrode neural recording system"}},{"authors":[{"name":"R. P. Feynma"}],"title":{"text":"Feynman Lectures on Computation"}},{"authors":[{"name":"R. Landauer"}],"title":{"text":"Computation, measurement, communication and energy dissipation"}},{"authors":[{"name":"C. H. Bennett"}],"title":{"text":"Notes on Landauer\u2019s principle, reversible computation, and Maxwell\u2019s Demon"}},{"authors":[{"name":"M. P. Frank"}],"title":{"text":"Reversibility for efﬁcient computing"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"M. Bhardwaj"}],"title":{"text":"Communications in the observation limited regime"}},{"authors":[{"name":"J. G. Smith"}],"title":{"text":"The information capacity of amplitude- and variance- constrained scalar Gaussian channels"}},{"authors":[{"name":"J. Dauwels"}],"title":{"text":"Numerical computation of the capacity of continuous memoryless channels"}},{"authors":[{"name":"R. Ho"},{"name":"K. W. Mai"},{"name":"M. A. Horowitz"}],"title":{"text":"The future of wires"}},{"authors":[{"name":"V. Kawadia"},{"name":"P. R. Kumar"}],"title":{"text":"A cautionary perspective on cross-layer design"}},{"authors":[{"name":"S. Graham"},{"name":"G. Baliga"},{"name":"P. R. Kumar"}],"title":{"text":"Abstractions, architecture, mechanisms, and a middleware for networked control"}},{"authors":[{"name":"C. M. Jansky"}],"title":{"text":"Collegiate training for the radio engineering ﬁeld"}},{"authors":[{"name":"T. Zhu"},{"name":"Y. Gu"},{"name":"T. He"},{"name":"Z.-L. Zhang"}],"title":{"text":"eShare: A capacitor-driven energy storage and sharing network for long-term operation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566133.pdf"},"links":[{"id":"1569566381","weight":3},{"id":"1569565663","weight":3},{"id":"1569565377","weight":3},{"id":"1569556029","weight":4},{"id":"1569565355","weight":3},{"id":"1569566765","weight":3},{"id":"1569565461","weight":3},{"id":"1569564227","weight":3},{"id":"1569566303","weight":3},{"id":"1569560613","weight":3},{"id":"1569566999","weight":4},{"id":"1569558483","weight":3},{"id":"1569566089","weight":3},{"id":"1569566963","weight":3},{"id":"1569566709","weight":3},{"id":"1569566905","weight":3},{"id":"1569561143","weight":3},{"id":"1569566687","weight":3},{"id":"1569558985","weight":3},{"id":"1569566721","weight":3},{"id":"1569555879","weight":3},{"id":"1569565219","weight":3},{"id":"1569556671","weight":3},{"id":"1569566223","weight":3},{"id":"1569566191","weight":3},{"id":"1569565441","weight":3},{"id":"1569563395","weight":3},{"id":"1569551347","weight":3},{"id":"1569565549","weight":3},{"id":"1569565611","weight":3},{"id":"1569565397","weight":3},{"id":"1569565765","weight":3},{"id":"1569565435","weight":3},{"id":"1569566253","weight":3},{"id":"1569566237","weight":3},{"id":"1569566771","weight":3},{"id":"1569566641","weight":3},{"id":"1569551905","weight":3},{"id":"1569556759","weight":3},{"id":"1569565669","weight":3},{"id":"1569563721","weight":3},{"id":"1569564923","weight":3},{"id":"1569564769","weight":3},{"id":"1569566933","weight":3},{"id":"1569565389","weight":3},{"id":"1569560459","weight":6},{"id":"1569566807","weight":3},{"id":"1569565853","weight":3},{"id":"1569564505","weight":3},{"id":"1569565165","weight":3},{"id":"1569564141","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T3.4","endtime":"18:00","authors":"Lav R. Varshney","date":"1341337200000","papertitle":"On Energy/Information Cross-Layer Architectures","starttime":"17:40","session":"S8.T3: Energy Issues in Communication Systems","room":"Stratton S. de P. Rico (202)","paperid":"1569566133"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
