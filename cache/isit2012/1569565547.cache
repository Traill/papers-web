{"id":"1569565547","paper":{"title":{"text":"Estimating Multiple Concurrent Processes"},"authors":[{"name":"Jayadev Acharya"},{"name":"Hirakendu Das"},{"name":"Ashkan Jafarpour"},{"name":"Alon Orlitsky"},{"name":"Shengjun Pan"}],"abstr":{"text":"Abstract\u2014We consider two related problems of estimating properties of a collection of point processes: estimating the multiset of parameters of continuous-time Poisson processes based on their activities over a period of time t, and estimating the multiset of activity probabilities of discrete-time Bernoulli processes based on their activities over n time instants.\nFor both problems, it is sufﬁcient to consider the observations\u2019 proﬁle\u2014the multiset of activity counts, regardless of their process identities. We consider the proﬁle maximum likelihood (PML) esti- mator that ﬁnds the parameter multiset maximizing the proﬁle\u2019s likelihood, and establish some of its competitive performance guarantees. For Poisson processes, if any estimator approximates the parameter multiset to within distance ǫ with error probability δ, then PML approximates the multiset to within distance 2ǫ with error probability at most δ · e 4 √ t · S , where S is the sum of the Poisson parameters, and the same result holds for Bernoulli processes.\nIn particular, for the L 1 distance metric, we relate the problems to the long-studied distribution-estimation problem and apply recent results to show that the PML estimator has error probability e −( t · S ) 0.9 for Poisson processes whenever the number of processes is k = O(tS log(tS)), and show a similar result for Bernoulli processes. We also show experimental results where the EM algorithm is used to compute the PML."},"body":{"text":"Service providers like websites or phone companies often want to estimate the usage patterns of their subscribers as a whole. For example, they may want to know how many users are active on an average at any point of time, or the number of heavy users or other aggregate usage statistics, based on observations over a period of time. We consider two natural and arguably the simplest models for this problem. In the Poisson model, the activities of different users are independent Poisson processes and in the Bernoulli model, they are independent Bernoulli processes. We want to estimate the multiset of Poisson parameters or success probabilities of the processes in the respective models, given samples from each of them.\nThe Poisson multiset estimation problem is mathematically deﬁned as follows. Let Λ def = (λ 1 , λ 2 , . . . , λ k ) be the param- eters of k Poisson processes. Since we are interested in estimating only the multiset of Poisson parameters, without loss of generality we assume that λ 1 ≥ λ 2 ≥ · · · ≥ λ k ≥ 0. Each of these processes is observed for time t and thus, the sample or multiplicity µ(i) produced by process i is distributed according to poi(λ i · t) for i ∈ [k]. Here and throughout this paper, poi(λ) denotes the Poisson distribution with mean λ and [k] def = {1, 2, . . . , k} for any positive integer k. We observe\nwhere µ 1 ≥ µ 2 ≥ · · · ≥ µ m > 0, namely, the multiset of positive multiplicities, or number of times each process that was active, ﬁred till time t. Note that we observe only the m ≤ k processes that were active at least once during time t, we are not given any information about k or the k − m that were not active. An estimator Q takes µ as input and outputs a multiset of nonnegative reals Q µ def = (q 1 , q 2 , . . . , q k \u2032 ) as an estimate of the unknown Λ. To measure the quality of estimation, one may use a suitable distance measure D(Λ, Q). One such distance measure is the (sorted) L 1 distance |Λ−Q| def = ∞ i=1 |λ i −q i |, where λ i = 0 for i > k and q i = 0 for i > k \u2032 . The following is an example of Poisson multiset estimation.\nExample 1. For Λ = (λ 1 , λ 2 , λ 3 , λ 4 ) = (3.5, 3, 1.2, 0.1) and t = 2, we have µ(1) ∼ poi(7), µ(2) ∼ poi(6), µ(3) ∼ poi(2.4) and µ(4) ∼ poi(0.2). Let the samples produced be µ(1) = 6, µ(2) = 8, µ(3) = 3, µ(4) = 0, namely, µ = (8, 6, 3). If Z ∼ poi(λ), then the maximum likelihood estimate of λ is Z. Hence, Q µ = µ/t = (4, 3, 1.5) is a reasonable estimate of Λ and |Λ − Q µ | = 0.5 + 0 + 0.3 + 0.1 = 0.9.\nThe Bernoulli multiset estimation problem is deﬁned simi- larly. Let B def = (θ 1 , θ 2 , . . . , θ k ), be the success probabilities of k Bernoulli 0-1 distributions, where θ 1 ≥ θ 2 ≥ · · · ≥ θ k ≥ 0. For each i ∈ [k], let X(i) def = X(i, 1), X(i, 2), . . . , X(i, n) be n samples drawn independently according to Bernoulli(θ i ). The samples X(i, j) take values 1 or 0 depending on whether user i is active or inactive at time instant j ∈ [n]. Let\nwhere m ≤ k, be the multiset of observed sequences, i.e., that have at least one activity. Let µ i be the number of ones in X i for i ∈ [m] and let µ = (µ 1 , µ 2 , . . . , µ m ) be the multiset of counts of ones in the sequences in X. Without loss of generality, µ 1 ≥ µ 2 ≥ · · · ≥ µ m > 0. It is sufﬁcient to consider only estimators that depend on X only through µ. This is because we are interested in estimating only the multiset B as a whole and not the success probabilities of speciﬁc processes, and furthermore, the sequences are generated i.i.d.. See [3, Section 3.1.3] for a simple proof of this fact. Hence, a Bernoulli multiset estimator Q takes X and outputs Q X def = Q µ def = (q 1 , q 2 , . . . , q k \u2032 ) as an estimate of B. As earlier, any suitable distance D(B, Q), e.g., the L 1 distance |B − Q| def = ∞ i=1 |θ i − q i |, can be used to measure the quality of estimation. The following example illustrates Bernoulli multiset estimation.\nExample 2. Let B = (θ(1), . . . , θ(5)) = (1, 1 2 , 1 8 , 1 8 , 1 8 ). Let n = 3 and the sample sequences obtained be X(1) = (1, 1, 1), X(2) = (1, 0, 1), X(3) = (0, 0, 0),\nX(4) = (1, 0, 0), X(5) = (0, 0, 0). We only observe the sequences X = X(1), X(2), X(4). The empirical estimator outputs Q = µ/n = (1, 2 3 , 1 3 ). If in addition, we are given that k = 5 and that each of the θ(i) have a uniform prior over [0, 1], then one obtains the Laplace or add-one estimate for each of the processes as ( 3+1 3+2 , 2+1 3+2 , 1+1 3+2 , 0+1 3+2 , 0+1 3+2 ) and outputs Q \u2032 = ( 4 5 , 3 5 , 2 5 , 1 5 , 1 5 ). The multisets Q \u2032\u2032 = (1, 1 2 ), Q \u2032\u2032\u2032 = (1, 1, 1 3 ). are also allowed estimates, although it is clear that they cannot generate the given sample sequences. Yet, |B − Q \u2032\u2032 | = 3 8 ≤ |B − Q| = 5 8 .\nFor both Poisson and Bernoulli multiset estimation, it is natural to consider the empirical estimator Q emp that out-\nfor the respective problems. The probability that a collection of Poisson processes Λ produce samples (µ(1), . . . , µ(k)) is\nHence Q emp,p µ \t = max Λ Λ(µ(1), . . . , µ(k)), i.e., the em- pirical estimator maximizes the likelihood of observing (µ(1), . . . , µ(k)) such that µ(i) = µ i for i ∈ [m] and µ(i) = 0 for i > m.\nSimilarly, the probability that a collection of Bernoulli processes B produce sequences (x(1), . . . , x(k)), with (µ(1), . . . , µ(k)) as the respective counts of ones is\nTherefore Q emp,b µ \t = max B B(x(1), . . . , x(k)) is the maxi- mum likelihood estimator of (x(1), . . . , x(k)) such that µ(i) = µ i for i ∈ [m] and µ(i) = 0 for i > m.\nFor any Λ, let S Λ def = k i=1 λ i and for any B, let S B def = k i=1 θ i . It can be shown that Q emp,p is a good estimate of Λ in terms of L 1 distance when k is small compared to t · S Λ since µ(i) concentrates around t · λ(i) for i ∈ [k] using Poisson tail bounds of Fact 4. Similarly, Q emp,b is a good estimate of B in terms of L 1 distance when k is small compared to n · S B since µ(i) concentrates around n · θ(i) for i ∈ [k] using Chernoff bounds of Fact 7. However, it can also be shown similar to the large alphabet examples in [11], [6] that the empirical estimators may not be good estimates of Λ or B when the number of processes is large and k = Ω(tS Λ ) or k = Ω(nS B ) respectively. In this paper, we show estimators that have good estimation guarantees even for large alphabets.\nThe empirical Poisson multiset estimator considers the like- lihood of observing a speciﬁc (µ(1), . . . , µ(k)) such that the multiset of nonzero multiplicities is µ. However, for estimating the multiset Λ, it is natural to consider the overall likelihood of µ i.e., the probability of observing any (µ(1), . . . , µ(k)) whose multiplicity multiset is µ. The information in µ is equivalently conveyed by the proﬁle ϕ def = ϕ(µ) def = (ϕ 1 , ϕ 2 , . . .) where ϕ µ def = |{µ i : µ i = µ, i ∈ [m]}|, called the prevalence of µ,\nis the number of multiplicities in µ that are equal to µ. We henceforth use µ and its proﬁle ϕ with the same meaning. The likelihood of a µ or its proﬁle ϕ under a collection Λ is\nΛ(ϕ) def = Λ(µ) def = Pr (µ(1), . . . , µ(k)) ∈ S ϕ =\nwhere S ϕ def = S µ is the collection of all (µ(1), . . . , µ(k)) whose multiplicity multiset is µ. Similarly, in the Bernoulli multiset estimation problem, the probability of a µ and its proﬁle ϕ under a collection B is\nB(ϕ) def = B(µ) def = Pr (X(1), . . . , X(k)) ∈ S ϕ =\nwhere S ϕ is the collection of all (x(1), . . . , x(k)) whose multiplicity multiset is µ.\nFor both problems we consider the proﬁle maximum likelihood (PML) estimator that maximizes the likeli- hood of observing the proﬁle of the given observa- tions. For Poisson multiset estimation, the PML distribu- tion is Q PML ,p ϕ \t def = ˆ Λ ϕ def = arg max Λ Λ(ϕ) and the PML is ˆ Λ(ϕ) def = max Λ Λ(ϕ) = ˆ Λ ϕ (ϕ). When the maximization is limited to a class of Poisson multisets L, we use the notations Q PML ,p L,ϕ and ˆ Λ L,ϕ . Likewise, for Bernoulli multiset estimation, we have Q PML ,b ϕ \t def = ˆ B ϕ def = arg max B B(ϕ) and the PML is\nˆ B(ϕ) def = max B B(ϕ) = ˆ B ϕ (ϕ). When the maximization is limited to a class of Bernoulli multisets B, we use the notations Q PML ,b B,ϕ and ˆ B B,ϕ .\nIn general, Q PML is different from Q emp . The PML tech- nique, also known as pattern maximum likelihood, has been used earlier in [10], [11], [9] for estimating the probability multiset of distributions in the context of universal compres- sion of large alphabet data sources. As we observe later, computation of PML Poisson multiset is almost identical to computing PML distribution, but PML Bernoulli multiset has a much different structure. We also develop useful connections between distribution estimation and Poisson and Bernoulli multiset estimation. For other recent works that efﬁciently exploit proﬁles for problems related to distribution multiset estimation and property testing, see [13] and references therein. D. Summary of main results\nWe show competitive estimation guarantees for the PML estimators that can be described informally as follows. In the Poisson model, if there is an estimator Q that estimates any Λ to within ǫ in some distance D with high probability δ, i.e., Pr(D(Λ, Q ϕ ) ≥ ǫ) ≤ δ, then the PML estimator provides a similar guarantee that Pr D(Λ, ˆ Λ ϕ ) ≥ 2ǫ ≤ δ·e 4 √ t·S Λ +e −t·S Λ /3 . In the Bernoulli model, if an estimator Q is a good estimate of any B in some distance measure D such that Pr(D(B, Q ϕ ) ≥ ǫ) ≤ δ, then the PML estimator is such that Pr D(B, ˆ B ϕ ) ≥ 2ǫ ≤ δ·e 4 √ n·S B +e −n·S B /3 . The results are shown using a similar competitive property of maximum likelihood estimators in general. Similar arguments have been used previously in [9] to show consistency properties of PML for distribution estimation.\nFor these results to be useful, we need to show estimators whose error probability δ is smaller than e −4 √ t·S Λ in the\nPoisson model or e −4 √ n·S B in the Bernoulli model. We show the existence of such estimators when L 1 is used as distance by relating these problems to that of distribution multiset estimation. It additionally shows that distribution multiset estimators can be easily adapted for Poisson and Bernoulli multiset estimation. In the distribution estimation problem, one wants to estimate the probability multiset {p 1 , p 2 , . . . , p k } of an unknown distribution P , i.e., k i=1 p i = 1, given ℓ samples generated i.i.d. according to P . This problem has been studied for a long time, e.g., see [11], [9], [5], [14], [13] and references therein for an overview of past and current developments. In particular, estimators are shown in [13] that can approximate distributions to within an L 1 distance of ǫ with high probability, whenever their support size is k = O(ǫ 2.1 ℓ log(ℓ)). We show that in the Poisson model, estimating a Λ to within an L 1 distance of ǫS Λ is equivalent to estimating the distribution P = Λ S Λ to within an L 1 distance of ǫ using ℓ = O(t · S Λ ) samples. Furthermore, an error probability of δ(ℓ) for distribution estimation translates to δ(t · S Λ ) in the Poisson model. Likewise, for a Bernoulli multiset B, a distribution estimator Q such that Pr(| B S B − Q ϕ | ≤ ǫ) ≤ δ(ℓ) implies a Bernoulli multiset estimator Q such that Pr(|B − Q ϕ | ≤ ǫ) ≤ δ(n · S B ). where ℓ = O(n · S B ). We use the results for distribution estimation in [13] along with the competitive estimation guarantees for the PML estimator to show that for any Λ such that k = O(ǫ 2.1 tS Λ log(tS Λ )), Pr(|Λ−Q PML ϕ | ≥ ǫ) ≤ e −(tS Λ ) 0.9 and for any B such that k = O(ǫ 2.1 nS B log(nS B )), Pr(|B − Q PML ϕ | ≥ ǫ) ≤ e −(nS B ) 0.9 .\nComputational aspects of ﬁnding the PML multiset by leveraging existing techniques for computing PML distribution are discussed towards the end of the paper.\nWe ﬁrst show a general competitive estimation guarantee for maximum likelihood (ML) estimators. Let Z be a discrete alphabet of size |Z| and P be a collection of probability distributions on Z. Given a sample Z generated according to an unknown distribution P ∈ P, we want to estimate P . An estimator Q : Z → P, outputs a distribution Q z ∈ P when given input z ∈ Z. The ML estimator outputs a distribution\nˆ P z def = arg max P ∈P P (z). Let D(·, ·) be a distance measure deﬁned on distributions in P. The next lemma shows that ˆ P Z is almost as good as any other estimator.\nLemma 3. For some ǫ ≥ 0 and δ ∈ [0, 1], let Q be an estimator such that for all P ∈ P, when Z ∼ P , Pr D(P, Q Z ) ≥ ǫ ≤ δ. Then, Pr D(P, ˆ P Z ) ≥ 2ǫ ≤ δ ·|Z|.\nProof Sketch. If Z = z is such that P (z) > δ, then D(P, ˆ P z ) ≤ 2ǫ. To see this, note that D(P, Q z ) ≤ ǫ, otherwise if D(P, Q z ) ≥ ǫ, then\ncontradicting that Q has error probability at most δ. By a similar reasoning, D( ˆ P z , Q z ) ≤ ǫ, since Q is a good estimator of ˆ P z ∈ P as well and ˆ P z (z) ≥ P (z) > δ. Hence,\nD(P, ˆ P z ) ≤ D(P, Q z ) + D(Q z , ˆ P z ) ≤ 2ǫ and Pr D(P, ˆ P Z ) ≥ 2ǫ\n+ Pr (D(P, ˆ P Z ) ≥ 2ǫ) ∧ (P (Z) ≤ δ) ≤ 0 + Pr(P (Z) ≤ δ) ≤ δ · |Z|.\nFor showing such an estimation guarantee for PML in the Poisson model, we use two more facts. In the multiset estimation problem, Z is the set of all proﬁles or all µ, which is inﬁnite. However, we show that the proﬁles generated by any Λ concentrate over a much smaller subset. For any ϕ and its corresponding µ let S ϕ = S µ = m i=1 µ i be the sum of multiplicities. If ϕ ∼ Λ, then S ϕ = k i=1 µ(i) is distributed according to poi(tS Λ ) by the well known property of sum of Poisson random variables. Hence, S ϕ concentrates around S Λ by the Poisson tail bounds given below.\nFact 4. (Also [12, Corollary 32].) For all ǫ ∈ (0, 1) and sufﬁciently large λ, if X \t ∼ poi(λ), then Pr |X − λ| ≥ ǫλ ≤ 2 exp(−ǫ 2 λ/3). For α ≥ 2, Pr X ≥ αλ ≤ exp(−αλ/6).\nµ i = S} is in 1-1 correspondence with the integer partitions of S [10] and can therefore be bounded by this well known fact about partition number [4].\nUsing the general lemma and these facts, we have the following results.\nLemma 6. Let L be a class of Poisson multisets Λ such that S Λ ≥ 2 and D(·, ·) be a distance measure on L. Suppose an estimator Q is such that for some ǫ, δ > 0, when ϕ ∼ Λ ∈ L, Pr D(Λ, Q ϕ ) ≥ ǫ ≤ δ. Then, Pr D(Λ, ˆ Λ L,ϕ ) ≥ 2ǫ ≤\nIn the last inequality, the bound on the ﬁrst term follows from S ϕ ∼ poi(tS Λ ) and Fact 4. For the second term, we use Lemma 3, along with Fact 5 which implies that |Z| = |{ϕ : S ϕ ≤ 2S Λ }| ≤ 2tS Λ · |Φ 2tS Λ | ≤ e 4 √ tS Λ .\nA similar estimation guarantee can be shown for PML Bernoulli multiset estimator. This time, we use the fact that if ϕ ∼ B, then S ϕ = k i=1 n j=1 X(i, j) is a sum of independent 0-1 random variables and concentrates around its mean nS B using the Chernoff bounds below.\nFact 7. (Chernoff bounds.) Let X = n i=1 Y i be a sum of independent 0-1 random variables Y 1 , . . . , Y n such that Pr(Y i = 1) = p i . Let µ = E[X] = i p i . For ǫ ∈ [0, 1], Pr(|X − µ| ≥ ǫµ) ≤ 2e −µǫ 2 /3 . For ǫ ≥ 1, Pr(X ≥ (1 + ǫ)µ) ≤ e −µǫ/3 .\nLemma 8. Let B be a class of Bernoulli multisets B and D(·, ·) be a distance measure on B. For large n,\nlet Q be an estimator such that for some ǫ, δ > 0 when X ∼ B ∈ B, Pr D(B, Q ϕ(X) ) ≥ ǫ ≤ δ. Then, Pr D(B, ˆ B B,ϕ(X) ) ≥ 2ǫ ≤ δe 4 √ nS B + e −nS B /3 .\nTo make use of these results, we show Poisson and Bernoulli multiset estimators whose error probability is less than e −5 √ tS Λ and e −5 √ nS B respectively by relating these problems to distribution multiset estimation.\nA distribution multiset estimator Q takes as input a se- quence Y = Y 1 , . . . , Y ℓ of ℓ samples drawn i.i.d. according to an unknown distribution P = (p 1 , . . . , p k ) and outputs Q Y = (q 1 , . . . , q k \u2032 ) as an estimate of P . We assume the probabilities in P and Q are arranged in decreasing order. We use µ(i) def = µ Y (i) to denote the number of appearances of symbol i (whose probability is p i ) in Y . As earlier, µ def = (µ 1 , . . . , µ m ) def = {µ(i); µ(i) > 0, i ∈ [k]} is the multi- set of nonzero multiplicities and ϕ def = ϕ(µ) def = ϕ(Y ) denotes the corresponding proﬁle. Without loss of generality, we as- sume Q depends on Y only through its proﬁle, i.e., Q Y = Q ϕ .\nTo relate the various estimation problems, it is useful to consider the well known useful technique of Poissonization which is summarized below.\nFact 9. Let Y \u2032 be a sequence of ℓ \u2032 ∼ poi(ℓ) samples drawn i.i.d. ∼ P . Then, for all i ∈ [k], µ Y \u2032 (i) ∼ poi(ℓp i ) and is independent of µ(i \u2032 ) for all other i \u2032 ∈ [k].\nIn the next deﬁnition and lemma, we show that good distribution multiset estimators can be used to construct good Poisson multiset estimators, both under L 1 distance guarantees. Deﬁnition 10. Let L be a class of Poisson multisets and let P def = { Λ S Λ = ( λ 1 S Λ , . . . , λ k S Λ ) : Λ ∈ L} be the corresponding class of normalized distributions. Let Q be a distribution multiset estimator for P. Then, the corresponding Poisson multiset estimator Q poi outputs Q poi ϕ def = S ϕ t · Q ϕ .\nLemma 11. For ǫ ∈ (0, 1), let L be a class of Poisson multisets such that S Λ is sufﬁciently large for all Λ ∈ L. Let P def = {Λ/S Λ : Λ ∈ L}. Let Q be a distribution estimator such that when ℓ ≥ min Λ∈L S Λ 2 and given Y ∼ P ℓ , Pr |P − Q ϕ(Y ) | > ǫ ≤ δ(ℓ), where δ decreases monotonically in ℓ. Then the estimator Q poi corresponding to Q is such that when ϕ ∼ Λ ∈ L,\nHere, the ﬁrst term is due to the Poisson tail bounds of Fact 4. For the second term, we use Fact 9, which implies that ϕ(µ) has the same distribution as ϕ(Y \u2032 ), where Y \u2032 is an i.i.d. sequence of length S µ ∼ poi(tS Λ ) drawn according to the\n. Since the number of samples in the input Y \u2032 to Q is S µ ≥ S Λ 2 and δ is monotonically decreasing, the error probability is at most δ tS Λ 2 ).\nCombining the above observations, by union bound, Pr |Λ − Q poi ϕ | > 2ǫS Λ ) ≤ Pr Λ S\nWe use similar arguments for converting good distribution multiset estimators into good Bernoulli multiset estimators under L 1 distance guarantees.\nDeﬁnition 12. Let B be a class of Bernoulli multisets and P def = { B S B = ( θ 1 S B , . . . , θ k S B ) : B ∈ B} be the corresponding class of normalized distributions. Let Q be a distribution multiset estimator for P. We deﬁne as follows a corresponding Bernoulli multiset estimator Q bern that takes input X ∼ B ∈ B. For i = 1, . . . , m, generate independent n i ∼ poi(n/2). If some n i > n, terminate the estimation process and output error. Otherwise, for each of i = 1, . . . , m, let Y i consist of ﬁrst n i samples of X i , i.e., Y i = X i,1 , X i,2 , . . . , X i,n i .\nLet µ \u2032 i def = µ(Y i ) be the number of 1\u2019s in Y i for i ∈ [m]. And let ϕ \u2032 = (ϕ \u2032 1 , ϕ \u2032 2 , . . .) be the proﬁle corresponding to µ \u2032 def = {µ \u2032 i : µ \u2032 i > 0, i ∈ [m]}. The output of Q bern is\nLemma 13. Let B be a class of distribution multisets and let P def = {B/S B : B ∈ B}. Let Q be a distribution estimator such that for large n and ℓ ≥ n · min B∈B S B 4 , when Y ∼ P ℓ , Pr |P − Q ϕ(Y ) | > ǫ ≤ δ(ℓ), where δ decreases monoton- ically in ℓ. Then, the corresponding Q bern is such that when X ∼ B ∈ B,\nProof. Let X ∼ B ∈ B and ϕ = ϕ(X). We analyze the error probability in each of the intermediate steps of Deﬁnition 12. Using the Poisson tail bounds in Fact 4 and union bound, probability that n i > n for some i ∈ {1, . . . , m} is at most\nIf all n i < n, by Fact 9 on Poissonization, all µ \u2032 i ∼ poi(nθ i /2) = poi (nS B /2) · (θ i /S B ) . Again by Fact 9, ϕ \u2032 has the same distribution as the proﬁle of a sequence Y \u2032 consisting of n \u2032 ∼ poi(nS B /2) samples drawn i.i.d. from the distribution B S B . Hence ϕ \u2032 has length S ϕ \u2032 = nS B 4 with probability ≥ 1 − e −nS B /12 by Poisson tail bounds. In that\n−Q ϕ \u2032 | ≥ ǫ ≤ δ(S ϕ \u2032 ) ≤ δ nS B 4 .\nX(i, j) is a sum of independent 0-1 random vari- ables and has mean E[S ϕ ] = nS B , we have Pr |S ϕ −nS B | ≥ ǫnS B ≤ 2e −ǫ 2 nS B /3 . Similar to the proof of Lemma 11, if | B S B − Q ϕ \u2032 | ≤ ǫ and |S ϕ − nS B | ≤ ǫnS B , then\nPutting these observations together, and using union bound for bounding the overall error probability,\n≤ Pr n i > n for some i ∈ [m] + Pr S ϕ \u2032 < nS B 4\n− Q ϕ \u2032 ≥ ǫ ∧ (S ϕ \u2032 ≥ nS B 4 ) + Pr |S ϕ − nS B | ≥ ǫnS B\nWe note that in both Lemma 11 and Lemma 13, an L 1 distance guarantee of 2ǫS Λ and 2ǫS B is reasonable since the maximum L 1 distance between any two multisets, each of whose sum of parameters is S, is at most 2S. As an application of these lemmas, we state and use the main result in [12], [13] that shows an estimator which can approximate distributions to within a small relative earthmover distance, and hence small L 1 distance, even when the support size k is superlinear in the number of samples ℓ. While the error probability shown in [12] is e −ℓ 0.03 , it can be improved to arbitrarily close to exponential, say e −ℓ 0.9 , by minor modiﬁcations to the various constant parameters of their estimator.\nTheorem 14. (Also [12, Theorem 3].) For ǫ > 0 and sufﬁciently large ℓ, there is an estimator Q such that for all P whose support size is k = O(ǫ 2.1 ℓ log(ℓ)), when Y ∼ P ℓ , Pr P − Q ϕ(Y ) > ǫ ≤ e −ℓ 0.9 .\nCorollary 15. For ǫ > 0, and for all t and Λ such that t·S Λ is sufﬁciently large and k = O(ǫ 2.1 tS Λ log(tS Λ )), when ϕ ∼ Λ, Pr |Λ − ˆ Λ ϕ | ≥ 2ǫS Λ ≤ e −(tS Λ ) 0.8 .\nFor all n and B such that n · S B is sufﬁciently large and k = O(ǫ 2.1 nS B log(nS B )), when ϕ ∼ B, Pr |B − ˆ Λ ϕ | ≥ 2ǫS B ≤ e −(nS B ) 0.8 . In both cases, the maximum likelihood is calculated over multisets of the respective support size bounds.\nWe consider some of the computational aspects of PML in this brief ﬁnal section. Due to space constraints, with- out going into details, we state that for any ϕ, due to the similarity between the expressions for likelihoods Λ(ϕ) and P (ϕ), ˆ Λ ϕ = S ϕ t · ˆ P ϕ . Thus, computation of PML Poisson multiset is equivalent to computing the PML distribution multiset of a given proﬁle. However, computation of PML Bernoulli multiset ˆ B ϕ is somewhat different from that of ˆ P ϕ . Nonetheless, the computation of both ˆ P ϕ and ˆ B ϕ seem to be difﬁcult in general, e.g., see [11] and subsequent works [8],\n[2], [1] along these lines. We conclude with an example of an experimental result for Bernoulli multiset estimation, as shown in Figure 1. The underlying multiset B is taken to be θ i = 0.05 for i ∈ {1, 2, . . . , 500}, i.e., k = 500. The empirical estimate, referred to as SML or sequence maximum likelihood in the ﬁgure is clearly not a good estimate of B, both in terms of support size and shape. Note that we do not get to even observe 154 out of the 500 processes. However, the PML multiset is seen to be a very good estimate of B. It is computed approximately using an EM-MCMC algorithm similar to that used in [11], [7] for distribution estimation."},"refs":[{"authors":[{"name":"J. Achary"},{"name":"H. Da"},{"name":"A. Orlitsk"},{"name":"S. Pan"}],"title":{"text":"Algebraic computation of pattern maximum likelihood"}},{"authors":[{"name":"J. Achary"},{"name":"A. Orlitsk"},{"name":"S. Pan"}],"title":{"text":"The maximum likelihood probability of unique-singleton, ternary, and length-7 patterns"}},{"authors":[],"title":{"text":"Tugkan Batu"}},{"authors":[{"name":"H. Hard"},{"name":"S. Ramanujan"}],"title":{"text":"G"}},{"authors":[{"name":"M. Jedyna"}],"title":{"text":"Bruno  Sanjeev Khudanpur"}},{"authors":[{"name":"B. Kell"},{"name":"T. Tulara"},{"name":"A. B. Wagne"},{"name":"P. Viswanath"}],"title":{"text":"Universal hypothesis testing in the learning-limited regime"}},{"authors":[{"name":"A. Orlitsk"},{"name":"S. Pa"},{"name":"P. Santhana"},{"name":"K. Viswanathan"}],"title":{"text":"Sajama, N"}},{"authors":[{"name":"A. Orlitsk"}],"title":{"text":"Shengjun Pan"}},{"authors":[{"name":"A. Orlitsk"},{"name":"P. Santhana"},{"name":"K. Viswanatha"},{"name":"J. Zhang"}],"title":{"text":"N"}},{"authors":[{"name":"A. Orlitsk"},{"name":"P. Santhana"},{"name":"J. Zhang"}],"title":{"text":"N"}},{"authors":[{"name":"P. Santhana"}],"title":{"text":"Alon Orlitsky, Narayana  Krishnamurthy Viswanathan, and Junan Zhang"}},{"authors":[],"title":{"text":"Gregory Valiant and Paul Valiant"}},{"authors":[],"title":{"text":"Gregory Valiant and Paul Valiant"}},{"authors":[{"name":"B. Wagne"},{"name":"R. Kulkarni"}],"title":{"text":"Aaron  Pramod Viswanath, and Sanjeev  Prob- ability estimation in the rare-events regime"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565547.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T8.3","endtime":"10:50","authors":"Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon  Orlitsky, Shengjun Pan","date":"1341397800000","papertitle":"Estimating Multiset of Bernoulli Processes","starttime":"10:30","session":"S9.T8: Portfolios and Estimation","room":"Stratton (491)","paperid":"1569565547"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
